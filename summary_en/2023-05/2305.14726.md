#### Background
- **Background**
The article introduces that large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples.

- **Existing Work**
The article points out that current methods exhibit large variance in performance when selecting the best in-context demonstrations, and the selected examples might introduce unfavorable prior biases on the output label space. Existing methods include simple methods such as selecting the nearest neighbors by embedding distance and retrieval-based methods that require training a retriever model. However, these methods cannot be applied to training data of any size and rely on training only small Parameter-Efficient Fine-Tuning (PEFT) models.

#### Core Contributions
- **Proposed a Cross-Entropy Difference (CED) method for in-context demonstration selection**
  - **Challenge 1: How to select the best in-context examples**
    The paper proposes a Cross-Entropy Difference (CED) method for selecting in-context examples, based on the observation that the effectiveness of examples negatively correlates with the test sample's perplexity. They employ parameter-efficient fine-tuning to train small models on training data used for computing the cross-entropy difference between a test example and every candidate in-context demonstration. This metric is used to independently rank and select in-context demonstrations for each test input.

  - **Challenge 2: How to theoretically explain the effectiveness of CED**
    The article provides a theoretical explanation for the effectiveness of CED, which approximates the gradient alignment between training and test examples. Drawing from previous findings that in-context examples may act as "meta-gradients" on the frozen LLM, it shows that demonstrations with gradients similar to those of test inputs can lead to improved performance in downstream tasks.

#### Implementation and Deployment
CED-ICD selection method was evaluated on a mixed-domain data set composed of 8 datasets across 4 text generation tasks: binary classification, multiple choice, extractive question answering, and abstractive question answering. The results show that downstream model performance using CED-ICD outperforms nearest neighbor baselines and is transferable across models of different sizes, enabling selection on smaller models but evaluating test examples on much larger models including GPT-3.5.

#### Summary
The paper presents a novel Cross-Entropy Difference (CED) method for in-context demonstration selection, provides a theoretical rationale, and achieves performance improvements on large language models of different sizes and types.