#### Background
- **Background**
Large language models (LLMs) have made significant advancements but suffer from high-memory consumption, becoming a major obstacle for large-scale training. Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) reduce memory usage but cannot consistently match the performance of full parameter training.
- **Existing Work**
Although LoRA is a popular approach due to the ability to merge adaptor weights back into the base model, it still struggles to surpass full parameter fine-tuning in all settings, particularly on large-scale datasets during continual pre-training, raising doubts about its effectiveness. The limited number of trainable parameters in LoRA restrains its representational power.

#### Core Contributions
  - **Introduced Layerwise Importance Sampled AdamW (LISA)**
    - **Challenge 1: Memory Efficiency vs. Performance in PEFT**
      The challenge is boosting the performance of LLMs in fine-tuning tasks while maintaining memory efficiency. By analyzing LoRA's training statistics across layers, the paper uncovers a skewed weight norm distribution, indicating varying layer importance. The LISA approach, inspired by importance sampling, selectively updates critical LLM layers while leaving others unchanged, addressing the trade-off between performance and memory efficiency.

#### Implementation and Deployment
LISA outperforms LoRA and full parameter tuning, with similar or lower GPU memory consumption, consistently surpassing LoRA by 11%-37% in MT-Bench scores. It achieves comparable or superior performance on large models like LLaMA-2-70B across various domains like MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness.

#### Summary
The LISA strategy proposed in the paper uses layer-wise weight importance sampling to enhance the fine-tuning efficiency and performance of large language models, while maintaining memory efficiency comparable to LoRA.