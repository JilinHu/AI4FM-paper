#### Background
- **Background**
The paper discusses the challenge of automating the conversion of visual designs into functional code in front-end development. This task requires understanding visual elements and their layouts and translating these into structured code. Despite rapid advancements in code generation from natural language in recent years, generating code from user interface (UI) design has not been given adequate attention due to the diversity of visual and text signals on the UI and the vast search space in the resulting code. There have been attempts in the past, but they were usually limited to simple or synthetic examples.

- **Existing Work**
Multimodal LLMs have developed recently to handle both visual and text input and generate text output for various visually grounded tasks. However, current multimodal LLMs still struggle with this task because they haven't been systematically evaluated and optimized for their performance in it.

#### Core Contributions
  - **Proposed a Design2Code task for front-end generation**
    - **Challenge 1: Diversity and Code Search Space**
      Introduced a benchmark with 484 diverse real-world webpages as test cases and developed a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate code implementations directly rendering the given reference webpages from input screenshots. Combined automatic metrics with comprehensive human evaluations to tackle the challenge of diversity in visual and text signals and the vast search space in code generation.

    - **Challenge 2: Enhancing Generation Quality**
      Developed multimodal prompting methods and demonstrated their effectiveness on GPT-4V and Gemini Pro Vision, further finetuned an open-source Design2Code-18B model achieving comparable performance with Gemini Pro Vision. These methods significantly improved the quality of generated webpages in terms of visual appearance and content, and in some measures, were considered better than the original reference pages.

#### Implementation and Deployment
The evaluation shows that GPT-4V outperforms other models in this task, with both human evaluation and automatic metrics indicating that GPT-4V-generated webpages can replace the original reference webpages in 49% of cases in terms of visual appearance and content. Surprisingly, in 64% of cases, evaluators considered GPT-4V-generated webpages to be better than the original ones. The paper also introduces automatic metrics, including high-level visual similarity and low-level element matching, which evaluate different aspects of the generated quality, representing a comprehensive assessment of multimodal LLMs' generative capabilities.

#### Summary
The paper formalizes and benchmarks the Design2Code task to assess the capability of current multimodal LLMs in converting visual designs into code, finding that GPT-4V performs best, offering a new paradigm for automating front-end development.