#### Background
- **Background**
This paper analyzes the challenges faced by large language models (LLMs) concerning training, fine-tuning, and serving aspects, especially considering data quality, diversity, and model architecture and optimization.

- **Existing Work**
Although there has been progress in improving the performance and efficiency of LLMs, existing works often lack meticulous data processing or innovative modifications and optimizations in model architecture, limiting further improvements in model performance.

#### Core Contributions
  - **Introduced the Yi-34B model**
    - **Challenge 1: Data Quality and Diversity**
      The paper proposes a multi-tiered data cleaning strategy aimed at enhancing the dataset's quality and diversity. By employing techniques such as heuristic filtering, learned filtering, and cluster-based filtering techniques, the Yi-34B can extract high-quality and diverse data from common web crawls.

    - **Challenge 2: Model Architecture and Optimization**
      The Yi-34B model uses an improved transformer architecture that is 'decoder-only' and incorporates Grouped-Query Attention (GQA) and SwiGLU activation functions to reduce training and inference costs without any observed decrease in performance.

#### Implementation and Deployment
Yi-34B achieves performance on par with GPT-3.5 across various standard benchmarks. Model parameters and cache quantization have made it possible to control inference costs, allowing the community to deploy the model on cost-effective devices. Detailed performance comparison shows that Yi-34B is comparable to other major LLMs in areas such as commonsense reasoning, college exams, math, coding, reading comprehension, etc. Yi-34B contributes to the community by providing cost-effective models with GPT-3.5 quality, supporting developers in building AI-native applications, and enabling users with locally runnable chatbots.

#### Summary
The paper successfully introduces the Yi-34B model, performing comparably to GPT-3.5 in both performance and efficiency, and provides detailed descriptions of innovative approaches to pre-training large language models and their instruction fine-tuning.