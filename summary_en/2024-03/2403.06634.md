#### Background
- **Background**
This paper addresses the ability of an adversary to learn about a production language model by querying its API, as investigated by the field of model stealing—a process where adversaries extract model weights via API queries.

- **Existing Work**
Prior work primarily focused on reconstructing a model from the bottom-up, starting from the input layer, and lacked an efficient approach to directly extract the last layer of the model.

#### Core Contributions
- **Introduced an attack method**
  - **Challenge 1: How to recover the complete embedding projection layer of a transformer language model that is a black-box?**
        Unlike previous bottom-up model reconstruction, this paper introduces a top-down attack that extracts the model's last layer directly. By making targeted queries, it's possible to extract the embedding dimension or the final weight matrix.

  - **Challenge 2: What is the utility of the stolen layer?**
        Stealing this layer provides several benefits: it discloses the transformer model's width, often correlated with its total parameter count; it reduces the extent to which the model is a black box; the sheer possibility of stealing any model parameters is surprising and it hints at concerns that further extensions of this attack could reveal even more information; lastly, recovering the model’s last layer may give insight into more global information about the model, such as the relative size differences between different models.

#### Implementation and Deployment
The assessment showed that the attack introduced in this paper could extract the entire projection matrix of OpenAI's ada and babbage language models for under $20 USD, thereby confirming their hidden dimensions of 1024 and 2048, respectively. It also recovered the exact hidden dimension size of the gpt-3.5-turbo model and estimated that it would cost less than $2000 in queries to extract the full projection matrix. After responsible disclosure, the authors shared their findings with affected services, including working with OpenAI to verify the effectiveness of their method. Consequently, companies like OpenAI and Google modified their APIs to introduce mitigations and defenses, such as those suggested in this paper, making it harder and more expensive for adversaries to perform such attacks.

#### Summary
The paper proposes a novel attack for model stealing from production language models, capable of effectively extracting the final layer of a Transformer model. It sheds light on the utility of such an attack for decrypting details, parameters, and dimensions of black-box models, and outlines the necessity of modifying the APIs to prevent such attacks in the future.