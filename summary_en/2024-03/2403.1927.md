#### Background
- **Background**
The paper presents the state of large language models (LLMs) in revolutionizing the field of natural language processing (NLP), emphasizing the importance of a series of training processes, including alignment tuning, to ensure the safety and usefulness of the model. It points out that complex reinforcement learning techniques, such as Proximal Policy Optimization (PPO), play a critical role in the alignment phase.
- **Existing Work**
The current popularity of Direct Preference Optimization (DPO) aims to address the complexity of reinforcement learning in LLM training. It uses human or higher-level AI judgment to select preference datasets and then trains LLMs by comparing log probabilities of selected and rejected responses. However, obtaining these probabilities can be challenging since proprietary models like GPT-4 do not provide log probabilities for inputs.

#### Core Contributions
  - **Introduced stepwise DPO (sDPO)**
      - **Challenge 1: Choice of reference models**
        The existing approaches typically use the basic SFT model as reference, which may be a weaker option with potentially misaligned preferences. The sDPO method proposes using the aligned model from the previous step as the reference model for the current step, thus employing a more aligned reference model (a better lower bound) in training.

      - **Challenge 2: Effective use of preference datasets**
        sDPO suggests using the available preference datasets in a stepwise manner rather than all at once. This approach allows the model to accumulate alignment over past steps and to train more effectively, ensuring that the final model is trained with the same amount of data as a model trained with DPO.
  
#### Implementation and Deployment
The study conducted preliminary experiments on Mistral-7B-OpenOrca and OpenHermes-2.5-Mistral-7B models and compared them with different reference models to measure the importance of using a well-aligned reference model in DPO. The experimental results indicated that using an already aligned reference model (Intel-7B-DPO) yielded the best performance, surpassing models trained with more data and larger size (SOLAR-0-70B). This validates the significant impact that a pre-aligned reference model has on the performance of the final aligned model. The sDPO method, due to its utilization of more aligned reference models, was able to train a final aligned model that outperformed other popular LLMs with more parameters.

#### Summary
The paper proposes a novel stepwise DPO (sDPO) method that effectively improves the performance and alignment of the final model by using preference datasets in a stepwise manner, and the aligned model from previous steps as the reference model for the current step.