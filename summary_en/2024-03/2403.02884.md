#### Background
- **Background**
The paper discusses the LLMs' proficiency in problem-solving, especially noting that their capability to solve mathematical problems is still found insufficient, possibly due to the complex multi-step reasoning required in solving mathematical challenges.

- **Existing Work**
Instruction Tuning has been demonstrated as an effective approach to enhancing the capabilities of LLMs. However, this method is limited by the size of currently available mathematical reasoning datasets, which only have about 7.5 thousand training examples each, restricting further improvements.

#### Core Contributions
- **Introduced MathScale**
    - **Challenge 1: Insufficient size and quality of datasets**
        MathScale, a simple and scalable method, addresses this challenge by generating high-quality mathematical reasoning data using frontier LLMs like GPT-3.5. It begins by extracting topics and knowledge points from seed math questions and builds a concept graph, which is then utilized to generate new math questions.

    - **Challenge 2: Lack of a comprehensive evaluation benchmark**
        The proposed MWPBENCH is a benchmark for Math Word Problems comprising ten datasets covering a range of math problems from K-12 to college and competitive levels. Furthermore, MWPBENCH offers a unified evaluation protocol to ensure consistent and fair model comparisons.

#### Implementation and Deployment
MathScale effectively scaled the size of the math dataset it generated, creating a dataset named MathScaleQA with two million math question-answer pairs. When MathScaleQA was used to fine-tune open-source LLMs, such as LLaMA-2 and Mistral, it significantly improved their capabilities in mathematical reasoning. In evaluations conducted on MWPBENCH, the MathScale-7B model achieved the state-of-the-art performance across all datasets, surpassing its peers of equivalent size by 42.9% in micro-average accuracy and 43.7% in macro-average accuracy.

#### Summary
MathScale proposes a scalable approach to creating high-quality mathematical reasoning data and introduces a new comprehensive benchmark, MWPBENCH, to fully evaluate the mathematical reasoning capabilities of LLMs, thereby significantly enhancing the models' performance in solving mathematical problems.