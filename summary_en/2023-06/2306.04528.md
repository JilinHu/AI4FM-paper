#### Background
- **Background**
The study addresses the robustness of large language models (LLMs) to adversarial prompts, which is a key concern in the fields of human-computer interaction and security. Adversarial prompts consist of potential perturbations such as typos, synonym replacements, and style differences, which could lead to skewed outcomes from the LLMs. The paper proposes a new open benchmark, PromptRobust, to evaluate the robustness of LLMs against such adversarial prompts.
- **Existing Work**
Current research mostly focuses on evaluating adversarial robustness on static datasets and fails to consider the LLMs' capacity to handle diverse inputs in practical settings. Moreover, the existing benchmarks are insufficient for dynamic evaluations or for use as actionable benchmark suites.

#### Core Contributions
  - **Introduced PromptRobust**
      - **Challenge 1: Evaluating LLMs' robustness under adversarial prompts**
        Existing evaluations are often limited to static datasets. PromptRobust enables researchers to assess how models handle possible input variations such as mistyped words, synonym replacements, or minor stylistic differences. This benchmark provides an open-source, dynamic platform to simulate potential real-world errors and assess the robustness of LLMs.
      - **Challenge 2: Feasibility of evaluating different LLMs and datasets**
        Computational resources limit evaluations on full datasets and inclusion of all LLMs. While PromptRobust addresses this issue, it also implies the need for future research to extend its application across various models and datasets.

#### Implementation and Deployment
The implementation of PromptRobust involved using adversarial attack techniques to simulate potential perturbations and conducting extensive experiments and analysis on various tasks and models, such as T5-large, UL2, and the latest model, ChatGPT. The findings show that current LLMs are not sufficiently robust to adversarial prompts. Furthermore, the study delves into the reasons behind this by analyzing attention visualization and frequently used words to guide experts and non-experts in developing better prompt engineering tools. Additionally, the paper proposes potential countermeasures, such as input data preprocessing, incorporating low-quality data during pre-training, and improved fine-tuning strategies. To facilitate foundational work on the robustness of LLMs, PromptRobust will be open-sourced.

#### Summary
PromptRobust is an innovative, open benchmark aimed at evaluating how LLMs handle input errors that are likely to occur naturally in the real world, such as typos and synonym replacements. The open-sourcing of this tool will aid future robustness research.