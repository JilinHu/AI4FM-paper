#### Background
- **Background**
The paper discussed how Large Language Models (LLMs) perform more accurately and interpretably when given "chain-of-thought" (CoT) prompts to reason out answers step by step for a variety of problems involving mathematical, logical, and common-sense reasoning.

- **Existing Work**
Although CoT methods have demonstrated significant success, there are still many instances where the generated rationales are incorrect. Previously, improvements involved fine-tuning models to generate better rationales. If gold-standard rationales could be obtained, for instance, through crowdsourcing or automatic methods, supervised learning could be applied, but such data are often hard to come by. An attractive alternative proposed is to begin with data sets that only contain questions and correct answers, more readily available, and bootstrap rationales during the learning process. A variant of this strategy was the self-taught reasoner (STaR), which generates proposed rationales from an LLM and then fine-tunes on the rationales leading to the correct answer.

#### Core Contributions
- **Presented a new approach named TRICE**
    - **Challenge 1: Sampling**
        The core challenge identified in the paper is sampling from the posterior over rationales conditioned on the correct answer. To address this issue, a learning algorithm called TRICE is introduced, utilizing a simple Markov-chain Monte Carlo (MCMC) expectation-maximization (EM) algorithm together with a novel control-variate technique that reduces the variance of gradient estimates as the model performance improves.

    - **Challenge 2: Dealing with Incorrect Rationales**
        A key advantage of the TRICE algorithm over STaR is its reduced tendency to overlook examples that become difficult due to failed rationalization processes. This not only stabilizes the convergence of the algorithm but also enhances performance by also allowing learning from incorrect rationales.

#### Implementation and Deployment
According to the evaluation results, TRICE significantly improves the model's accuracy on held-out samples, outperforming models fine-tuned with STaR, direct tuning with or without CoT, and even supervised fine-tuning with human-generated rationales.

#### Summary
This paper develops an MCMC-EM based fine-tuning strategy that, by averaging over rationales, helps LLMs generate the correct answers, holding potential for wide applicability.