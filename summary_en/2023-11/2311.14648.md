#### Background
- **Background**
Recent advancements in language models (LMs) have brought about concerns regarding their tendency to generate false but plausible-sounding text, referred to as "hallucinations." This presents issues for language-based AI systems and can be harmful to users relying on the output. The research highlights that the inception of hallucinations in LMs is statistically guaranteed for "arbitrary" facts that can't be verified against the training data, and is intrinsic to the statistical calibration condition achieved in language models.

- **Existing Work**
The existing work failed to address the issue of hallucinations because it did not recognize the inherent statistical reason behind it. Hallucinations generated by AI are not only associated with the architectural design of LMs or the quality of data but are also tied to the statistical calibration implemented within LMs. Despite optimizations to reduce the hallucination rate, they fall short of addressing the root cause because existing calibration methods can't differentiate between reliable and fabricated information. Furthermore, the hallucination issue is also rooted in the richness of information in the training data — some facts might only appear once, leading the model to hallucinate during generation.

#### Core Contributions
- **Introduced a statistical calibration condition**
  - **Challenge 1: How to build an excellent predictive language model that must generate hallucinations?**
      As good predictive performance implies that a model must be able to produce high-probability facts without explicit cues, hallucinations are inevitable. The article proposes that even under ideal training conditions — where training data are independently and identically distributed (i.i.d.) and devoid of factual errors — good predictive performance will still lead to the generation of hallucinations. By implementing calibration in the model, the article demonstrates the link between the statistical calibration condition and the generation of hallucinations.

  - **Challenge 2: How to quantify the extent of hallucinations that language models should produce?**
      The article provides a method to quantify the degree of hallucination, which is closely related to the proportion of facts that appear exactly once in the training data (a "Good-Turing" estimate), even under the assumption of ideal training data without errors. This indicates that even if the training data quality is extremely high, the model will naturally generate a certain proportion of hallucinations to maintain statistical calibration.

#### Implementation and Deployment
The paper validated their statistical lower-bound model through theoretical proof and empirical verification. Results show that hallucinations are an inevitable part of generative models, even under idealized assumptions (such as the absence of factual errors in the training data). Especially after models have been pretrained to precisely predict text, post-training is necessary to mitigate hallucinations on arbitrary facts that appear once in the training set. However, the analysis also indicates that there is no statistical reason for pretraining to induce hallucinations on facts that appear more than once in the training data (e.g., references to publications such as articles and books) or systematic facts (like arithmetic calculations), suggesting that other architectures or learning algorithms may reduce hallucinations of these types.

#### Summary
The paper outlines the statistical root cause of inevitable hallucinations under sufficient calibration in pretrained language models, elucidates the native mechanism of hallucination generation in models with good predictive performance and provides a lower bound estimate for the rate of hallucination. It discusses the likelihood of different types of facts hallucinating and points towards potential future directions for mitigating specific types of hallucinations.