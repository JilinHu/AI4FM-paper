#### Background
- **Background**
The paper discusses the importance and challenges of fact-checking, which is split into four parts: check-worthiness detection, evidence retrieval, fact verification, and explanation generation. A vast number of automatic fact-checking frameworks and models have been developed, leveraging pre-trained language models for feature extraction and deep learning methods for result prediction. Yet, they face difficulties due to the limited scale of training data and domain sensitivity, often requiring extensive annotated data, which is time-consuming and laborious.

- **Existing Work**
Existing studies began to explore the use of large language models (LLMs) like GPT-4 and LLaMa2 in natural language processing (NLP) tasks, showing promising performance. In fact-checking, while LLMs demonstrate potential in subtasks like fact verification, there hasn't been a systematic assessment of their performance across all subtasks of fact-checking. Also, research is lacking on whether LLMs could handle all subtasks of fact-checking simultaneously. This paper aims to systematically evaluate the feasibility of LLMs in fact-checking tasks and offer future research directions.

#### Core Contributions
  - **Proposed an overall assessment framework for fact-checking**
    - **Challenge 1: Whether LLMs perform well in fact-checking tasks in a 0-shot setting**
      Research indicates that LLMs struggle with fact-checking tasks in a 0-shot setting even with enhanced prompts. Besides, while LLMs can check facts and provide reasonable explanations, their performance deteriorates when handling only the fact-verification task.

    - **Challenge 2: Absence of an integrated model that can handle the entire fact-checking process, can LLMs resolve all subtasks simultaneously?**
      The study explores whether prompt tuning helps improve the capability of LLMs in fact-checking and if LLMs can use their knowledge as a base to provide evidence for claims that need verification. Results reveal that prompt enhancement aids performance improvement to some extent, yet LLMs as a knowledge base still have to overcome the challenge of hallucinations and misinformation.

#### Implementation and Deployment
Researchers conducted multiple experiments on three fact-checking datasets to elucidate the performance of LLMs on each subtask and the whole fact-checking pipeline. The results indicate that LLMs find it challenging to solve fact-checking tasks in a 0-shot setting even with different methods of prompt enhancement. While LLMs can fact-check and generate explanations simultaneously, they perform less effectively when tackling only the fact-verification task. Even though LLMs can flexibly use their knowledge to retrieve evidence and predict labels, they face challenges when functioning as a knowledge base. The findings suggest that additional efforts and research are required to enhance the performance of LLMs on fact-checking tasks and make them proficient fact-checkers.

#### Summary
The paper systematically evaluates the potential of LLMs in the entire fact-checking process, revealing that while they show promise in certain aspects, considerably more research and trials are needed to improve their performance in fact-checking tasks.