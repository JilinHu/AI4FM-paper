#### Background
- **Background**
  The article discusses the application and evaluation of Large Language Models (LLMs) in task automation. LLMs generally need to parse automation tasks, which includes task decomposition, tool invocation, and parameters. These parsed tasks are typically easier to collect and construct. Thus, the paper explores a question: is it possible to synthesize user instructions based on the expected parsed tasks?

- **Existing Work**
  Existing research fails to adequately address the complexity of data collection, with current methods insufficient for simulating more realistic and complex user instructions. There also lacks a comprehensive evaluation system to effectively assess an LLM's capabilities in task decomposition, tool invocation, and parameter prediction.

#### Core Contributions
  - **Challenge 1: Complexity of Data Collection**
      The paper introduces the concept of the Tool Graph (TG), which addresses the complexity of data collection by randomly sampling a sub-graph from the TG to represent the expected task list in the user instruction, and then using a back-instruct strategy to generate the final user instructions.
  
  - **Challenge 2: Effective and Quantitative Capability Evaluation of LLMs**
      Introduces an evaluation system called TASKEVAL, which includes a series of metrics to objectively measure the capability of LLMs in task decomposition, tool invocation, and predicting the parameters of tools. Moreover, human evaluation was conducted to confirm the positive correlation between TASKEVAL and human judgement.

#### Implementation and Deployment
During evaluation, the research utilized three different architectures for sampling to achieve better controllability: node, chain, and directed acyclic graph (DAG). A self-critique mechanism was added to enhance data quality. To ensure diversity in the evaluation, data generation covered three domains (e.g., Hugging Face, multimedia, and daily life), forming TaskBench to assess the capabilities of LLMs in task automation. Experimental results demonstrated that TaskBench effectively reflects the capabilities of LLMs across multiple dimensions, with a high correlation to human evaluation.

#### Summary
This paper presented TaskBench, a new benchmark test, and TASKEVAL, an evaluation system, which together effectively address the assessment challenges of LLMs in task automation through data generation and quantitative evaluation.