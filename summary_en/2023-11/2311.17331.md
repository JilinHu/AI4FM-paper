#### Background
- **Background** 
The article discusses that in the field of Visual Question Answering (VQA), current Vision Language Models (VLMs) often consider VQA tasks as perception tasks and employ black-box models that overlook explicit modeling of relationships between different questions within the same visual scene. Furthermore, existing VQA methods that rely on Knowledge Bases (KBs) may be biased due to limited data and face challenges in relevant information indexing. To overcome these constraints, the paper presents an explainable multi-agent collaboration framework that exploits knowledge embedded in Large Language Models (LLMs) trained on extensive corpora.

- **Existing Work**
Existing studies attempt to augment models with prior knowledge from external Knowledge Bases (KBs) to enhance the model's ability to handle complex images from the real world, but they usually fail to provide adaptive, unbiased prior information for a diverse set of visual tasks. Moreover, some methods that need to combine LLMs' knowledge often lack interpretability and robustness, and are different from the human reasoning process.

#### Core Contributions
- **Introduced a multi-agent collaboration framework named SIRI**
  - **Challenge 1: Bridging the large linguistic domain gap between LLMs and VLMs** 
      It's a challenge to explicitly benefit from the associations between individual entities related to a specific issue learned by LLMs. The paper innovatively combines three agents—Seeker agent for generating issues related to the question, Responder agent for handling simple VQA tasks, and Integrator agent for integrating information from the Seeker and Responder agents to produce the final answer—mimicking the top-down reasoning process in human cognition. This collaboration among agents simulates the top-down reasoning process in human cognition, overcoming the limitations in existing methods due to the linguistic domain differences between agents.

  - **Challenge 2: Enhancing the interpretability and generalizability of VQA models**
      The framework introduces a module called the Multi-view Knowledge Base, which uses the potential of LLMs by organizing each hypothesis generated by the Seeker agent as a node. Additionally, the Integrator agent uses this knowledge base and the probability of hypotheses assigned by LLMs to assess and finalize the answer for the given original question. This structure allows the model to provide sufficient interpretations while providing the VQA answer.

#### Implementation and Deployment
The SIRI framework utilizes three parallel agents for human-like reasoning: the Responder agent generates candidate answers for a question, the Seeker agent generates relevant issues based on the question and answer candidates, and obtains respective responses, and the Integrator agent combines the information from the former two to produce the final answer. Experiments show that evaluations across various VQA datasets and different VLMs demonstrate the effectiveness and broad applicability of the method.

#### Summary
This work innovatively combines three agents to simulate the top-down reasoning process in human cognition, and introduces the concept of a Multi-view Knowledge Base, significantly enhancing the expressiveness and interpretability of VQA models.