#### Background
- **Background**       
    The paper addresses the impracticality of standard full-data classifiers in NLP, which require thousands of labeled samples, in data-limited domains like finance. Few-shot methods such as contrastive learning have emerged to address this issue, being effective with approximately 20 examples per class. Large Language Models (LLMs) like GPT-4 also have shown potential with only 1-5 examples per class. However, there is an under-explored area regarding the trade-offs between performance and cost, which is crucial for organizations with limited budgets.

- **Existing Work**
    To tackle the problem mentioned above, this study utilizes the Banking77 dataset for financial intent detection to conduct a comprehensive evaluation of state-of-the-art LLMs from OpenAI, Cohere, and Anthropic, covering various few-shot scenarios. Banking77, reflecting real-world data, consists of 77 labels with significant semantic overlap, making it an ideal dataset for investigating methodological approaches while addressing a business case.

#### Core Contributions
  - **Introduced a method for resource-limited text classification in banking**
      - **Challenge 1: Cost-effective LLM querying**
          There is a lack of exploration in existing literature regarding trade-offs between operational costs and performance for LLMs. The paper presents a method based on Retrieval-Augmented Generation (RAG), which can significantly reduce operational costs when compared to traditional few-shot approaches. This method only retrieves a minor but crucial fraction (2.2%) of examples and surpasses the most competitive method (GPT-4) by 1.5 percentage points in performance while saving $700 on the test set (âˆ¼3k examples). This is a substantial contribution for small organizations with limited funding.

      - **Challenge 2: Data generation in low-resource settings**
          The study also simulated a low-resource data scenario and performed data augmentation using GPT-4, demonstrating the potential to improve performance in data-limited scenarios. This approach enhances performance and analyses the threshold where data augmentation effectiveness diminishes, helping AI practitioners in their decision-making process.
      
#### Implementation and Deployment
The researchers first fine-tuned the MPNet with the complete dataset, then applied SetFit, a contrastive learning technique, showing similar results with only 20 examples. They also conducted in-context learning with a range of popular LLMs like GPT-3.5 and GPT-4, showing that expert-curated samples outperform randomly selected ones. The findings suggest that in-context learning with LLMs can outperform fine-tuned MLMs in financial few-shot text classification tasks with even fewer examples.

#### Summary
For the first time, this paper presents a comprehensive evaluation of methodologies in a resource-limited industrial context, including cost analysis, RAG method, and data augmentation using GPT-4, offering new avenues for the financial industry to address challenges related to data and budget constraints.