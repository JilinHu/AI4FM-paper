#### Background
- **Background**
The article examines the progress and performance of open-source Large Language Models (LLMs) one year after the release of ChatGPT. ChatGPT quickly gained public attention and business investments with its ability to provide helpful, safe, and detailed answers across various task domains. However, the technical details of ChatGPT are not open-sourced, making it challenging to assess its social risks and impacts and to achieve reproducible research results. Open-source LLMs present a promising direction that could resolve or avoid these issues, though they are generally believed to perform subpar compared to closed-source models like OpenAI's GPT-3.5 (ChatGPT) and GPT-4. Nonetheless, open-source LLMs are rapidly narrowing the gap and have surpassed GPT-3.5-turbo in some standard benchmarks.

- **Existing Work**
Most high-performance LLMs rely on large-scale self-supervised pre-training, followed by fine-tuning for downstream tasks. Although the community has been actively pushing for high-performing open-source LLMs, open-source models such as Llama-2 and Falcon are still deemed to be less performant than their closed-source counterparts. Assessing LLM capabilities is also an active research area due to the diversity of evaluations required. The article thoroughly reviews open-source LLMs that have caught up or surpassed ChatGPT's performance in various domains and discusses the potential pitfalls.

#### Core Contributions
- **Provided a systematic review of open-source LLMs**
    - **Challenge 1: Matching or surpassing ChatGPT across various task domains**
        The article lists and analyzes several open-source LLMs that have made progress in different task domains compared to ChatGPT, including long-input tasks, application-specific capabilities, and trustworthiness. For instance, Llama-2-long-chat-70B outperforms GPT-3.5-turbo-16k on the ZeroSCROLLS benchmark, showing the effectiveness of driving up the performance of open-source LLMs on long-input tasks through pre-training with extended context windows and additional data.

    - **Challenge 2: Best practices for training LLMs to reduce performance gaps with closed-source models**
        The paper also discusses the best practices in constructing high-performing LLMs, including data collection and processing, model design, and training process. For instance, using smaller but higher-quality fine-tuning data, employing the decoder-only transformer architecture, and emphasizing the model's alignment with human preferences and instruction adherence. The article also points out some loopholes and potential problems in the development of open-source LLMs, such as data contamination, close-sourced development of alignment, and difficulties in continuous improvements over fundamental abilities.

#### Implementation and Deployment
In evaluating the performance of open-source LLMs against ChatGPT, the paper uses a variety of benchmarks and evaluation metrics, and it shows that open-source LLMs are becoming competitive in many tasks. For example, Llama-2-long improved efficiency in long-input tasks, Lemur-70B-chat performed better in exploring the environment and following feedback in coding tasks, and Gorilla surpassed GPT-4 in writing API calls. Moreover, many open-source models showed capabilities surpassing GPT-3.5-turbo after task-specific fine-tuning, like in domains of mental health analysis and radiology report interpretation. These comparisons indicate that open-source LLMs are gradually catching up with ChatGPT in quality and performance through targeted improvements.

#### Summary
This survey paper provides an assessment of the performance of open-source LLMs across multiple task domains compared to ChatGPT, highlighting the strengths and potential problems of current open-source LLMs, and offers insights for future research and development. Furthermore, it summarizes numerous best practices and challenges, indicating that the open-source field could potentially close the gap with commercial models to some extent.