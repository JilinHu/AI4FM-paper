#### Background
- **Background**       
    The paper analyzes the in-context learning (ICL) performance of Vision-Language Models (VLMs), which becomes essential when the model needs to answer questions about new images based on a few provided examples (demonstrations) within the context. The findings show text information within demonstrations plays a significant role in influencing model answer generation, whereas random demonstration selections do not always offer the necessary context for various queries, and demonstrations retrieved based on visual similarity overlook textual context.

- **Existing Work**
    The existing work has failed to consider the need to take into account both image and text information when selecting demonstrations for ICL. A demonstration selection method solely based on visual similarity misses out on text content that is more informative for specific queries as questions related to visually similar images may not always be interconnected.

#### Core Contributions
  - **Introduced a method called Mixed Modality In-Context Example Selection (MMICES)**
    - **Challenge 1: How to effectively select informative demonstrations**
        The authors discuss the challenge that appropriate demonstration selection is crucial for the ICL performance of VL models, yet there are several challenges. For example, retrieval solely based on text information presents its own challenges, as general queries like "What is in this picture?" often do not provide sufficient information. To tackle this challenge, the paper proposes the MMICES method, combining initial demonstration selection based on image similarity with a re-ranking based on text similarity, aiming to pick out demonstrations that are more informative and relevant to specific queries.

    - **Challenge 2: How to improve selection quality despite minimal impact from demonstration images**
        The paper notes that although the impact of demonstration images has been found to be minimal, they play a role in preliminary demonstration retrieval. Choosing demonstrations visually similar to the query image is more likely to bring texts containing related information, which is the rationale for MMICES's mixed-modality approach. MMICES utilizes this approach to ensure the provision of high-quality context demonstrations, improving the models' ability to generate the correct response.

#### Implementation and Deployment
The paper investigates 7 different vision-language models, including OpenFlamingo and IDEFICS, covering their architecture, model sizes, pre-trained datasets, and instruction tuning. It employs four VL tasks and datasets for evaluation such as VQAv2, OK-VQA, GQA, and MSCOCO, and presents the experimental setup using the models and metrics. The performance section reveals that across different experimentations, MMICES consistently showed higher performance metrics over several evaluation rounds, highlighting its superiority.

#### Summary
This paper proposed a novel method, MMICES, for selecting demonstrations in in-context learning for vision-language models, demonstrating its effective performance across different models and datasets.