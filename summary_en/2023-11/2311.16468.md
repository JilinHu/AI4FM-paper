#### Background
- **Background**
    The article presents the context where Large Language Models (LLMs) have demonstrated exceptional capabilities to unify most NLP tasks. However, in the realm of human motion, researchers still develop isolated models for each task. Inspired by InstructGPT and the generalist concept of Gato, the article introduces AvatarGPT, an All-in-One framework for motion understanding, planning, generation, and synthesis tasks.

- **Existing Work**
    The existing works on text-to-human-motion generation show two main limitations from an end-to-end perspective. First, they are primarily effective for generating short motion sequences, and extending them for longer durations is challenging. Second, they rely on manually crafted detailed text inputs for each motion sequence, limiting their application in real-world scenarios.

#### Core Contributions
  - **An All-in-One framework, AvatarGPT, is introduced**
    - **Challenge 1: Enable the infinite possibility for long-term motion synthesis.**
        This is a challenge as existing technology mainly applies to short sequences. AvatarGPT implements a principled approach through iterative traversal of tasks within a closed-loop, surpassing any previous work's capabilities for long-duration motion synthesis.

    - **Challenge 2: Construct datasets from raw videos without manual intervention.**
        This has been difficult because datasets typically required human intervention to generate. AvatarGPT developed an innovative unsupervised pipeline to generate natural language descriptions of human action sequences from raw videos and curated a dataset for tuning high-level human action planning.

    - ...

#### Implementation and Deployment
The AvatarGPT framework treats continuous motion sequences as discrete tokens, treating them as an extension of the LLM vocabulary. It is structured around four high-level sub-tasks: task planning, decomposition, summarization, and scene estimation, plus three low-level sub-tasks: text-driven motion generation, motion understanding, and in-between motion generation. Using specialized prompts for each sub-task and an instruction-tuning strategy for model training, AvatarGPT sets new benchmarks in low-level tasks and shows promising results in high-level tasks, signifying its effectiveness.

#### Summary
The research introduces a novel, integrated AvatarGPT framework for handling high-level and low-level tasks related to understanding, planning, and generating human motions, showcasing the potential for extended duration motion synthesis and reduced manual intervention.