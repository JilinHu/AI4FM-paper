#### Background
- **Background**
This article addresses the efficiency of aligning the output of large language models (LLMs) with human expectations using feedback in natural language. Traditionally, LLMs use reinforcement learning from human feedback (RLHF) which leverages ranking feedback of human preference signals to perform alignment. However, human preference for LLM outputs may be expressed in richer forms, including natural language that provides detailed feedback about the strengths and weaknesses of a given response. The work explores data efficiency of modeling human feedback in natural language.

- **Existing Work**
RLHF is notably unstable during training and requires a significant amount of computational resources to support both policy and base models, posing great challenges to infrastructure, especially when the model size exceeds 50B parameters. Additionally, RLHF conveys human preference to the model through a single reward value which can lack detail and specificity. Teaching LLMs to learn from fine-grained human feedback is challenging as it involves detailed critiques in natural language compared to pairwise comparison feedback that simply selects a preferred response.

#### Core Contributions
  - **Introduced a Critique and Revise (CnR) method**
      - **Challenge 1: Insufficient human feedback data**
        Teaching the model human preferences with fewer data (like 1000 records or even fewer), by critiquing and revising responses in natural language. The CnR method allows an open-source LLM, such as Falcon-40B-Instruct, to revise responses of even strong LLMs like ChatGPT, showing notable data efficiency advantage over traditional preference comparisons.

      - **Challenge 2: High detail requirement for human feedback**
        The CnR method not only requires the LLM to generate a critique to outline the positive and negative aspects of the response but also to revise the initial response according to the critique. This method allows for more precise and detailed human feedback alignment, guiding the model to correct errors in the response and modify answers according to the provided critique.
  
#### Implementation and Deployment
In practice, the paper demonstrates that this model is capable of generating critiques that are meaningful and specific to a given response and can address errors in the response. When applied to ChatGPT response revisions, it achieved a win rate of 56.6% in one iteration, and this rate can be improved to 65.9% after five iterations. Additionally, ablation studies showed that revision quality could be enhanced by using larger base pre-trained models and higher-quality instruction-tuning data. The impact of the amount of CnR data on the performance of the CnR model was also investigated.

#### Summary
The paper presented an effective CnR method capable of efficiently aligning LLMs with human expectations through detailed feedback and response revision using natural language. With relatively less human feedback data, this method significantly improves the quality of responses from even top LLMs such as ChatGPT.