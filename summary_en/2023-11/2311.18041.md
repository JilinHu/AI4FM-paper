#### Background
- **Background**
This article introduces the powerful summarization abilities of Large Language Models (LLMs), however, their capabilities in conversational summarization remain under-explored. The work evaluates LLMs with approximately 10 billion parameters on conversational summarization, showcasing their performance with various prompts.

- **Existing Work**
LLMs have previously demonstrated impressive multitasking abilities, including summarization, with remarkable zero-shot performance. Despite being 'prompted' to perform tasks with relatively simple natural language phrases, the brittleness of LLMs in consistently processing these prompts poses a significant challenge. The generated summaries' quality and consistency are heavily influenced by the instructions, and improper prompt selection can result in a steep drop in ROUGE scores. The existing work, although actively advancing, still reveals the inherent fragility issues when applying LLMs to conversational summarization, especially evident in smaller open-sourced LLMs.

#### Core Contributions
  - **Addresses the evaluation of conversational summarization with ~11 billion parameter LLMs**
      - **Challenge 1: Dealing with the frailty of LLMs in processing prompts**
          The study further tuned LLMs on a technical council meetings dataset and evaluated their performance on a chit-chat dataset, discussing the capability of fine-tuned models to perform zero-shot summarization across different domains.

      - **Challenge 2: Selecting appropriate model parameters for running on edge devices within a reasonable compute budget**
          The paper chose models with approximately 11 billion parameters and investigated these models under constraints such as cost, hardware limitations, and a desire for lower carbon emission model development approaches. The study addressed how to optimize the use of these models effectively on PCs and mobile systems through compressing models and reducing memory interactions.

#### Implementation and Deployment
The paper used the MeetingBank dataset for instruction tuning and evaluated the models in a zero-shot manner on the Samsum and Dialogsum datasets. Regarding model choice, authors pointed out that larger models and more data increase inferencing needs. The article also discussed optimizations in hardware inference, such as through model compression and reduced memory interactions to accommodate running models on low-power devices. Finally, it highlighted the significant memory consumption of running inference instances on a PC and proposed methods to optimize models to fit these constraints.

#### Summary
The paper focuses on the application of Large Language Models in the conversational summarization task, deeply examining the impact of different instructions on model performance and researching optimization techniques for using compressed models under hardware limitations.