#### Background
- **Background**
    The paper discusses the capability of large language models (LLMs) to answer knowledge-intensive complex questions using chain-of-thought (CoT) reasoning. However, LLMs tend to generate factually incorrect reasoning steps when required knowledge is not present or out-of-date in their parameters. Recent works have turned to retrieve external knowledge to augment CoT reasoning, but despite their promise, these chain-based methods suffer from issues such as negative retrieval and limited sight, where errors proliferate due to the inability to foresee and review past steps.

- **Existing Work**
    Existing works on retrieval-augmented LLMs often adopt one-time retrieval, which fails to meet the requirements of complex questions that necessitate multi-hop inference. Another emerging line of research performs multiple retrievals during the generation process. Nevertheless, these methods still face the described issues. They disregard that unnecessary or incorrect external knowledge may mislead LLM reasoning and that local errors can easily propagate along the reasoning chain due to limited foresight.

#### Core Contributions
  - **A novel approach: Probabilistic Tree-of-thought Reasoning (ProbTree)**
    - **Challenge 1: Negative retrieval**
        Overcoming negative retrieval issues in leaf nodes, ProbTree uses both Closed-book QA employing parametric knowledge and Open-book QA utilizing retrieved external knowledge. Answers are chosen based on greater confidence as determined by self-evaluation to eliminate the impact of negative retrieval.

    - **Challenge 2: Limited sight**
        ProbTree's tree-like structure equips LLMs with broader insights to globally reason using information from child nodes, allowing recovery from local errors on non-leaf nodes, and alleviating error propagation issues.
  - ...

#### Implementation and Deployment
ProbTree was evaluated on three open-domain complex QA datasets. Empirical results using OpenAI's GPT-3 showed ProbTree significantly outperforms existing state-of-the-art (SOTA) models, improving F1 scores by 3.9%, 7.3%, and 5.2% on the datasets HotpotQA, MusiQue, and 2WikiMultihopQA, respectively.

#### Summary
The paper introduces Probabilistic Tree-of-thought Reasoning (ProbTree), a novel method that explores LLMs' capabilities to answer complex, knowledge-intensive questions and incorporates uncertainty into the reasoning process, integrating external and parametric knowledge within a unified framework.