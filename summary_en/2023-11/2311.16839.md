#### Background
- **Background**
This paper discusses the phenomenon of "hallucinations" in large vision-and-language models (LVLMs), referring to models generating inaccurate factual descriptions without supporting evidence. The problem is particularly prevalent in tasks involving detailed descriptions and dialogues, significantly impacting model performance.

- **Existing Work**
Existing methods fail to effectively address this issue because they usually do not consider the style consistency of the model-generated content and lack simple and efficient strategies to directly optimize model preferences.

#### Core Contributions
  - **Introduced a method called "Hallucination-Aware Direct Preference Optimization" (HA-DPO)**
    - **Challenge 1: Elimination of hallucinations**
      The challenge of eliminating hallucinations lies in building a training display that can make the model prefer non-hallucinatory outputs. Through the HA-DPO method, the authors effectively constrained the model's preferences, making it favor outputs without hallucinations.

    - **Challenge 2: Construction of style-consistent data**
      The challenge of optimizing the model through establishing pairs of positive and negative samples with consistent style is that these samples need to maintain style consistency for stable training while incorporating enough diversity. The paper ensures style consistency by rewriting samples with GPT-4 and stabilizing pre-training with the control of the Î² parameter.

    - **Challenge 3: New method for evaluating hallucinations**
      To better assess hallucinations in LVLMs and to address deficiencies in existing evaluation methods, the paper introduces a new metric, "Sentence-level Hallucination Ratio" (SHR).

#### Implementation and Deployment
The training data was constructed using the Visual Genome (VG) dataset that contained 2000 images and corresponding 6000 non-hallucinatory replies and 6000 hallucinatory replies. The HA-DPO training and evaluation showed marked progress in reducing hallucinations and improving model performance against the POPE dataset and the newly proposed SHR metric.

#### Summary
The article proposes an innovative strategy for optimizing LVLMs to reduce hallucinations and introduces a new evaluation method to more comprehensively measure hallucinations. The effectiveness of the proposed method is validated through experiments.