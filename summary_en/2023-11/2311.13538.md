#### Background
- **Background**
This paper explores the challenges of invoking Large Language Models (LLMs) to perform complex reasoning, such as mathematical reasoning and commonsense reasoning. Current LLMs are sensitive to prompt words and styles, with a noticeable gap between LLM understanding and human-written prompts.

- **Existing Work**
Existing research has made multiple attempts at how to select effective examples, proposing techniques such as Chain of Thought (CoT) prompting and developing various enhancements based on it. However, the effect of CoT’s text style, specifically LLMs’ familiarity and proficiency in language use during reasoning, on the performance of complex reasoning by LLMs is still under-researched.

#### Core Contributions
  - **Introduced the AlignedCoT technique**
    - **Challenge 1: Text style differences leading to unstable reasoning capabilities in LLMs**
      The technique provides consistent and correct step-wise prompts in zero-shot scenarios by progressively probing, refining, and formatting the LLM chain of thoughts, eliminating the need for handcrafted few-shot demonstrations while maintaining the quality of prompts.

    - **Challenge 2: Disparity between training and inference**
      By aligning the text style of conventional few-shot CoT with the "native style" of LLMs, this method reduces the disparity between training and inference, decreases the requirement for extensive model generalization capabilities, and results in performance improvements.

#### Implementation and Deployment
Experiments on mathematical and commonsense reasoning tasks demonstrate that LLMs equipped with AlignedCoT outperform those with human-crafted demonstrations. Furthermore, the authors applied AlignedCoT to rewrite the GSM8K training set, creating a GSM8K-Align dataset, and confirm that it can effectively improve the performance of retrieval-augmented methods.

#### Summary
The AlignedCoT technique presented in this paper aims to align the CoT text style with the "native style" of Large Language Models to improve their reasoning capabilities, and its effectiveness has been demonstrated through empirical evidence.