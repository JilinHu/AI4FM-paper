#### Background
- **Background**
    The paper discusses whether large language models (LLMs) show sociodemographic biases even when they refuse to "speak". The authors address this question by probing contextualized embeddings to explore whether these biases are encoded in the models' latent representations.

- **Existing Work**
    Previous attempts to detect embedding biases involve creating two opposite attribute word sets, such as negative and positive emotions, and measuring each test word's (e.g., nationalities) cosine similarity to both sets. If it is closer to one set, implicit association can be claimed. This method, known as WEAT, has been applied to gender, professions, and ethnicities, but it has drawbacks: cosine similarity does not directly optimize for discriminating between the two word sets, nor can it model attributes that cannot be split into two opposing sets, such as numbers.

#### Core Contributions
- **Introduced a new probe**
    - **Challenge 1: Building opposite attribute word sets and their limitations**
        The paper introduces a new probe based on the logistic Bradley–Terry model to discriminate between the hidden vectors of two opposite attribute word sets, using the LLM’s own outputs as potentially the labels for the sets. This approach captures the LLM’s bias more accurately, which is particularly valuable when extracting embeddings and labels from LLMs as it uses prompts eliciting preference for two attribute words. The trained probe is then transferred to controversial word pairs (e.g., comparing different countries), and if the probe favors one target group, implicit association can be claimed, similar to WEAT.

    - **Challenge 2: Detecting hidden biases and associative biases**
        The method proved to be more accurate, outperforming WEAT and max-margin classification by a relative 27–34% on three different datasets when comparing positive-negative pairs of actions, emotions, and numbers. The research also found that embeddings in the middle layers best represent word pair preferences. By applying the probe to the analysis of sociodemographic biases, substantial biases in nationality, politics, religion, and gender were found within the embeddings of the LLMs, even though the LLMs had explicit safety measures in place.

#### Implementation and Deployment
The paper validates the effectiveness of the new probe across three datasets and thirteen LLMs, showing its advancement in preference recognition. When applied to the analysis of sociodemographic biases, the probe found that English LLMs tend to favor Western over Eastern countries, Europe over Africa, left-wing over right-wing ideologies, libertarianism over authoritarianism, Christianity and Judaism over Islam, and females in professions over males.

#### Summary
The authors proposed a novel probe to detect implicit association biases in LLMs representations and demonstrated state-of-the-art performance in preference detection. The research additionally uncovered significant biases in multiple instruction-following and "classic" LLMs related to nationality, politics, religion, and gender, despite the explicit safety calibration of the LLMs.