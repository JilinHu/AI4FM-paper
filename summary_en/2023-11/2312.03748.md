#### Background
- **Background**
This study examines the application of large language models (LLMs), particularly GPT-3.5 and GPT-4, in conjunction with the Chain-of-Thought (CoT) method for the automatic scoring of student-written responses in science assessments. It aims to overcome the challenges of accessibility, technical complexity, and explainability which have previously restricted the use of automatic assessment tools by researchers and educators.

- **Existing Work**
Existing methods of automatic scoring rely heavily on advancements in machine learning and natural language processing (NLP). These systems are good at understanding the syntactical structure of student responses, yet struggle with complexities of scientific reasoning and interpreting students' thought processes. Developing such scoring models is time-intensive and laborious. Recent studies have proposed using prompt engineering to relieve researchers from the burden of labeling large numbers of training cases, but the accuracy of the reported scoring still requires significant improvement. This is often due to the limited ability of LLMs to understand the depth of content-specific knowledge and the rationale behind students' answers. Furthermore, despite the release of various LLMs, like the GPT family, there’s no clear answer on which models and hyperparameters are best for automatic scoring.

#### Core Contributions
- **Presented a new method for automatic scoring systems in science education**
    - **Challenge 1: Limitations of automatic scoring tools**
        The study introduces combining LLMs with CoT prompting methods to significantly boost the accuracy of automatic scoring systems and address issues of accessibility, technical complexity, and lack of explainability. The approach reduces human effort and captures student thought processes in constructing scientific explanations, potentially aligning more closely with human scoring outcomes.

    - **Challenge 2: Enhancing scoring accuracy**
        The research experimented with LLMs’ scoring accuracy under various conditions, specifically comparing prompting approach (zero-shot vs. few-shot learning), LLM reasoning strategy (CoT vs. Non-CoT), and providing contextual item information and scoring rubrics. The effect of different versions and hyperparameters of the GPT family on automatic scoring performance was also tested. Findings suggest that GPT-4 outperforms GPT-3.5 in various scoring tasks, and CoT significantly increases scoring accuracy when used with item stems and rubrics.

#### Implementation and Deployment
The experimental setup in the paper included six prompting engineering strategies, combining either zero-shot or few-shot learning with CoT. Results indicated that few-shot learning had a higher accuracy (67%) compared to zero-shot learning (60%), with an increase of 12.6%. CoT did not significantly influence scoring accuracy without item stems and scoring rubrics (accuracy 60%). However, when paired with contextual item stems and rubrics, CoT markedly improved scoring accuracy (13.44% increase for zero-shot; 3.7% increase for few-shot). Additionally, GPT-4 demonstrated superior performance over GPT-3.5 across various scoring tasks, with an 8.64% difference. GPT-4's single-call strategy, particularly using greedy sampling, outperformed other approaches, including ensemble voting strategies.

#### Summary
The study showcases the potential of LLMs in facilitating automatic scoring, highlighting that CoT significantly enhances scoring accuracy when used with item stems and scoring rubrics. The combined approach of LLMs with CoT can reduce complexity and manpower cost in building automatic scoring models and potentially offer a closer alignment with human scoring results.