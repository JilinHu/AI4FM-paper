#### Background
- **Background**
The paper points out that visual mathematical reasoning, as a fundamental visual reasoning ability, has garnered extensive attention from the Large Multimodal Models (LMMs) community. However, existing benchmarks mainly focus on result-oriented performance, neglecting the underlying principles of knowledge acquisition and generalization, hence limiting the assessment of LMMs in addressing complex problems.

- **Existing Work**
Existing work has attempted to emulate human reasoning patterns using methods such as Chain of Thought (COT) and Program of Thought (POT). Yet, these may not adequately evaluate the ability of models to decode visual information in images and reason based on text problems in more challenging visual mathematical reasoning scenarios.

#### Core Contributions
- **Introduced a benchmark named WE-MATH**
  - **Challenge 1: Decomposition of Composite Problems**
      The challenge lies in breaking down composite problems into sub-problems categorized according to required knowledge concepts. The paper addresses this by creating a benchmark designed to explore problem-solving principles through fine-grained problem categorization and collection.

  - **Challenge 2: Multidimensional Evaluation**
      Another challenge is proposing a new four-dimensional metric system, including Insufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery (CM), and Rote Memorization (RM), to hierarchically assess LMMs' inherent problems in their reasoning process.

#### Implementation and Deployment
The results indicate a phase transition in challenges from IK to IG for higher-level LMMs like GPT-4o in visual math reasoning tasks, suggesting that these models are moving towards the knowledge generalization stage. In contrast, other LMMs exhibit a marked inclination towards Rote Memorizationâ€”they solve composite problems involving multiple concepts correctly but fail in answering sub-problems. This shows that existing LMMs have not yet achieved true knowledge generalization and understanding. The evaluation with the WE-MATH benchmark confirmed that the IK issue of LMMs can be effectively mitigated through knowledge augmentation strategies. The study also revealed a negative correlation between the solving step and problem-specific performance.

#### Summary
This paper introduces a visual mathematical reasoning benchmark called WE-MATH, aimed at going beyond traditional end-to-end performance assessments to deeply explore and evaluate the problem-solving principles of LMMs, their ability to acquire and generalize knowledge. It reveals challenges in the inherent reasoning processes of multimodal models using a new multi-dimensional evaluation method and validates the effectiveness of knowledge augmentation strategies, advancing the progress of LMMs in the domain of visual mathematical reasoning.