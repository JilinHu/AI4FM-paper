#### Background
- **Background**
The paper addresses the issue of personal information disclosure during interactions with commercial chatbots based on large language models (LLMs). The research team conducted a detailed analysis of actual interactions between real users and GPT models, discovering the leakage of personally identifiable and sensitive information.

- **Existing Work**
Existing research has failed to fully detect personal information leaks in interactions between users and commercial chatbots, especially in unexpected contexts such as translation and coding tasks. Additionally, current PII detection systems cannot adequately capture sensitive topics that are often discussed in interactions with LMMs, such as detailed sexual preferences or specific drug usage.

#### Core Contributions
  - **Provided a fine-grained analysis of personal disclosures in user-chatbot interactions**
      - **Challenge 1: Identifying PII and sensitive topics**
          Researchers created a taxonomy of tasks and sensitive topics by analyzing real user chat datasets—“WildChat”, achieving automatic task and sensitive topic categorization for user queries.

      - **Challenge 2: Quantifying leak frequency and reliability of detections**
          Employed machine learning methods to automatically categorize 5K conversations selected from WildChat, validated with a subset of human annotations, and released annotations related to limitations in detecting PII and sensitive information.

#### Implementation and Deployment
The study utilized the “WildChat” dataset, a collection of one million user-GPT interactions, and initially performed one round of PII removal. Researchers found that over 70% of queries still contained some form of PII, and nearly 15% discussed sensitive subjects not considered PII. The study achieved results in revealing the risks inherent in interactions with chatbots, even though users may not be aware that their data is being collected and the risks it entails. The study has important implications for chatbot designers and LLM researchers, calling for the design of appropriate nudging mechanisms to help users moderate interactions and increased transparency from chatbot companies.

#### Summary
This research highlights the issue of personal information leakage in interactions with chatbots. It presents the types of sensitive information shared in these interactions and calls for measures in chatbot design to protect user privacy and maintain appropriate transparency of the content exchanged.