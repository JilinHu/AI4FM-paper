#### Background
- **Background**
This paper discusses the importance of understanding knowledge mechanisms in Large Language Models (LLMs) toward the advancement of trustworthy Artificial General Intelligence (AGI). Human knowledge evolution has been extensive, dedicating to using existing knowledge and exploring unknown spheres of knowledge for fostering more advanced intelligence. In comparison, the mechanisms for learning, storage, utilization, and evolution of knowledge in LLMs remain somewhat of a mystery.

- **Existing Work**
Despite many attempts to demystify various types of knowledge in LLMs through knowledge neurons and circuits, efforts remain scattered across various tasks without a comprehensive review and analysis.

#### Core Contributions
- **Introducing a novel taxonomy and comprehensive review of knowledge mechanisms in LLMs**

  - **Challenge 1: Knowledge Evolution Across Time**
    Presenting a new taxonomy including knowledge utilization at specific times and knowledge evolution across all periods. The paper offers a fresh perspective by analyzing the mechanisms of knowledge utilization at three levels: memorization, comprehension and application, and creation.

  - **Challenge 2: Fragility of Parametric Knowledge and "Dark Knowledge" Potential**
    Discussing knowledge evolution in individual and group LLMs, and analyzing the inherent conflicts and integrations in this process. It also conjectures that the prevalent transformer architecture may hinder creativity, and data distribution and quantity may contribute to the fragility of parametric knowledge, leading to hallucinations and knowledge conflict. Moreover, "dark knowledge" will likely exist for an extended period.

#### Implementation and Deployment
The paper explores and manipulates advanced knowledge in LLMs from a novel angle, examines current limitations through the lens of knowledge evolution history, and encourages more efficient and trustworthy architectures and learning strategies for the future. It hypothesizes mostly based on transformer-based LLMs and confirms the generalizability of these hypotheses across other architecture models. Additionally, it provides directions and tools for future research.

#### Summary
The paper suggests that a deep understanding of knowledge mechanisms in LLMs is crucial for developing powerful and reliable AI. It introduces a new framework for evaluating such systems, focusing on the utilization and evolution of knowledge, offering a vision and tools for future research directions.