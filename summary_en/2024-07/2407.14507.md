#### Background
- **Background**
The paper addresses the challenges large language models (LLMs) face, such as deficient reasoning abilities and the creation of hallucinatory content. Existing studies such as Self-Consistency, Self-Improve, and Self-Refine, all share a common theme of involving LLMs in evaluating and updating themselves to mitigate these issues. Yet, there's a lack of a unified perspective in summarizing these works and their motivations.

- **Existing Work**
Previous tools and methodologies fall short in summarizing and resolving issues related to deficient reasoning and hallucination content in LLMs from a unified perspective.

#### Core Contributions
  - **Introduced a theoretical framework**
      - **Challenge 1: Internal Consistency**
        The problem of deficient reasoning and hallucination presence is tackled by the theoretical framework "Internal Consistency," which offers a unified explanation for these phenomena by assessing the coherence among the latent, decoding, and response layers of LLMs based on sampling methodologies.
      - **Challenge 2: Self-Feedback**
        Building on the Internal Consistency framework, the paper introduces a streamlined and effective theoretical framework named "Self-Feedback," comprising two modules: Self-Evaluation and Self-Update. The Self-Evaluation module captures Internal Consistency indicators, while Self-Update uses these signals to enhance the model's responses or the model itself.

#### Implementation and Deployment
The paper systematically categorizes studies by tasks and line of work, summarizes relevant evaluation methods and benchmarks, and delves into the effectiveness of Self-Feedback. Key perspectives including the "Hourglass Evolution of Internal Consistency," "Consistency Is (Almost) Correctness" hypothesis, and "The Paradox of Latent and Explicit Reasoning" are proposed. It also outlines avenues for future research and has made experimental code, reference lists, and statistical data openly accessible.

#### Summary
This paper introduces the concepts of Internal Consistency and Self-Feedback to address consistency and hallucination issues in large language models, providing a new lens to understand and enhance these models and anticipates future research directions.