#### Background
- **Background**
The paper introduces ChatQA 2, a model based on Llama3 aiming to bridge the gap between open-access LLMs and leading proprietary models (e.g., GPT-4-Turbo) in terms of longer-context understanding and Retrieval-Augmented Generation (RAG) capabilities. These capabilities are crucial for LLMs to process large volumes of information that do not fit into a single prompt, each being complementary depending on the computational budget and the downstream tasks.

- **Existing Work**
The existing RAG pipeline has limitations that can undermine downstream task accuracy, such as top-k chunk-wise retrieval leading to context fragmentation and the introduction of too much irrelevant context by a large top-k value. However, state-of-the-art long-context retrievers can largely alleviate these issues.

#### Core Contributions
- **A new LLM model is introduced**
    - **Challenge 1: Extend context window and enhance instruction following capabilities**
        NVIDIA presents a detailed, continuous training recipe to extend the context window of Llama3-70B-base from 8K to 128K tokens, alongside a three-stage instruction tuning process to boost the model's instruction following, RAG performance, and long-context understanding abilities.

    - **Challenge 2: Improve performance on long-context understanding tasks and RAG benchmarks**
        By leveraging a long-context retriever to address the top-k context fragmentation issue in RAG, the Llama3-ChatQA-2-70B model achieves accuracy on par with GPT-4-Turbo on many long-context understanding tasks and even surpasses it on the RAG benchmarks.

#### Implementation and Deployment
The evaluation of Llama3-ChatQA-2-70B shows that while using RAG, the accuracy can be slightly worse than the direct long-context solution on benchmarks with maximum 32K tokens inputs. However, for tasks beyond 100k, the RAG-based result is still better than the long context evaluation. Moreover, the study demonstrates that for long-context situations (beyond 100k tokens), RAG is recommended for better accuracy and lower inference cost, despite RAG being slightly inferior to the direct long-context solution even when increasing top-k chunks to 30. Additionally, the model proved to achieve accuracy comparable to GPT-4-Turbo-2024-0409 on these benchmarks.

#### Summary
The paper presented a model named Llama3-ChatQA-2-70B, designed to bridge the gap between open-access LLMs and proprietary models, capable of handling up to 128K tokens context, and achieving comparable performance to GPT-4-Turbo on various benchmarks.