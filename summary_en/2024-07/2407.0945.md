#### Background
- **Background**
The paper discusses the limitation of large language models (LLMs) in processing extensive contexts, which hinders their ability to maintain coherence and accuracy in long sequences, in contrast to the human brain's proficiency in organizing and retrieving episodic experiences over a lifetime. To address this, EM-LLM, a novel approach that engrains vital aspects of human episodic memory and event cognition into LLMs, is introduced.

- **Existing Work**
Existing works have tackled this challenge through retrieval enhancement, key-value pair retrieval, segmentation and retrieval block techniques. However, there is still a noticeable performance gap between handling long-context and short-context tasks, even when using existing long-context architectures.

#### Core Contributions
  - **A new LLM based on event cognition and episodic memory**
    - **Challenge 1: Handling Long Context Information**
      Existing Transformer-based architectures and related techniques face limitations in reasoning ability and computational costs when dealing with lengthy text sequences. The EM-LLM approaches this by introducing an efficient memory formation process, initially determining the boundaries of episodic events dynamically based on the model's level of surprise during inference and then refining them via graph-theoretic measurements for coherence.
    - **Challenge 2: Efficient and Human-like Memory Retrieval**
      Memory retrieval is often time-consuming and requires the model to access relevant information from a vast amount of data. EM-LLM employs similarity-based retrieval and temporal contiguity mechanisms for efficient access to pertinent information, mimicking human temporal dynamics to improve the model's capability to handle complex tasks requiring nuanced temporal reasoning.

#### Implementation and Deployment
EM-LLM has demonstrated superior performance in handling long-context tasks, outperforming the state-of-the-art InfLLM model with an overall relative improvement of 4.3% across various tasks on the LongBench dataset, including a significant 33% improvement on the PassageRetrieval task. This performance also indicates strong correlations between EM-LLM event segmentation and human-perceived events, building a bridge between the artificial system and its biological counterpart. The approach advances LLM capabilities in dealing with extended contexts and provides a computational framework for exploring human memory mechanisms, establishing new pathways for interdisciplinary research in AI and cognitive science.

#### Summary
The paper proposes an innovative structure, EM-LLM, by integrating human episodic memory and event cognition into large language models, enabling them to manage practically infinite context lengths while remaining computationally efficient. This research enhances LLMs' capabilities to process expansive contexts and contributes to understanding human memory mechanisms.