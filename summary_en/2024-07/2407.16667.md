#### Background
- **Background**
Advanced Large Language Models like GPT-4 have been integrated into many real-world applications, expanding the attack surface of LLMs and exposing them to various threats, including “jailbreak attacks” that trigger toxic responses through crafted prompts.
- **Existing Work**
Existing red teaming methods do not consider LLMs' unique vulnerabilities across different scenarios, such as code-related tasks, making it difficult to adjust prompts for context-specific vulnerabilities and thus lacking efficiency. Additionally, they are limited in their approach by refining handcrafted templates through basic mutations like synonym replacements, lacking automation and scalability.

#### Core Contributions
- **Introduced a multi-agent LLM system named RedAgent**
    - **Challenge 1: Lack of high-quality jailbreak prompts**
        Previous efforts could rapidly generate thousands of prompts, albeit ineffective. By abstracting existing attacks into a concept of “jailbreak strategy,” RedAgent overcomes this by generating context-aware jailbreak prompts, thus addressing the issue of ineffective prompts.
    - **Challenge 2: Lack of automation and scalability**
        RedAgent continuously learns how to leverage jailbreak strategies more effectively by reflecting on contextual feedback and red teaming attempts within an additional memory buffer, enhancing automation and scalability.

#### Implementation and Deployment
Through extensive experimentation, RedAgent has demonstrated the ability to jailbreak most black-box LLMs with just five queries, doubling the efficiency of existing red teaming methods. It can also jailbreak customized LLM applications more efficiently. By creating context-aware prompts towards GPT's trending applications, RedAgent discovered 60 severe vulnerabilities in real-world applications, using only two queries per vulnerability. All issues have been reported, and communication has been established with OpenAI and Meta for bug fixes. Moreover, LLMs applications integrated with external data or tools appear more susceptible to jailbreak attacks than foundation models.

#### Summary
The RedAgent system effectively identifies and exploits the security vulnerabilities of large language models by simulating context-specific jailbreak strategies. It enhances the efficiency and automation of red teaming methods while providing a new perspective on understanding and strengthening the security of LLM applications.