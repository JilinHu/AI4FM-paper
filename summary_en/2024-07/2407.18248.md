#### Background
- **Background**
This paper discusses the valuable yet challenging objective of performing mathematical reasoning within large-scale language models. Previous studies have focused on enhancing the reasoning abilities of such models through various methods, but the enhancement of smaller sized LMs has been relatively unexplored.

- **Existing Work**
There is some evidence that smaller LMs can significantly boost their reasoning capabilities by learning from the outputs of larger, more advanced LMs such as GPT-4. However, this technique can be expensive and limited in terms of sustainability and scalability. Additionally, employing large LMs for data annotation to enhance the performance of smaller LMs presents a clear trade-off between economic costs and performance gains. Moreover, self-improvement methods that promote learning from self-generated data instead of larger models' outputs are heavily dependent on the inherent capabilities of the base models.

#### Core Contributions
- **Introduced a novel self-training architecture**
  - **Challenge 1: Enhancing the reasoning ability of small-scale LMs without relying on larger LMs**
      The authors present an innovation by integrating Direct Preference Optimization (DPO) into the conventional self-training framework, specifically targeting performance objectives within chain-of-thought reasoning, and in particular, with mathematical reasoning tasks. This integration aims to guide models towards more accurate and diverse chain-of-thought reasoning using preference data.

  - **Challenge 2: Reducing computational costs while improving performance**
      Empirical results indicate that self-training combined with DPO improves LMs’ reasoning performance while reducing computational costs, offering a more cost-effective and scalable solution compared to relying on large proprietary LMs.

#### Implementation and Deployment
The proposed method was evaluated using different base models on various mathematical reasoning tasks. The findings suggest that this approach not only improves LMs’ reasoning performance but also provides a cost-effective and scalable solution, outperforming reliance on large proprietary LMs. The paper also demonstrates how LMs can be effectively integrated with external tools to significantly boost downstream task performance without notably impacting inference speed.

#### Summary
The paper proposes an innovative self-training framework that incorporates Direct Preference Optimization (DPO) to enhance the reasoning capabilities of small-scale language models in mathematical reasoning tasks. This method reduces reliance on large-scale models, cuts computational costs, and shows positive results on multiple math reasoning tasks.