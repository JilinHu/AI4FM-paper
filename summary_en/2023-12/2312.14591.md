#### Background
- **Background**
The paper explores human interaction where natural language feedback helps in reflecting on actions, maintaining appropriate behavior, and correcting mistakes, probing the possibility of using language feedback to align Large Language Models (LLMs).

- **Existing Work**
Previous research mainly used rewards or preference data to align LLMs, but these methods did not fully utilize the feedback. Additionally, current approaches only use judgmental language feedback as a prompt, making the model generate an improved response for supervised training as a new demonstration, which cannot learn from mistakes effectively.

#### Core Contributions
- **Introduced a new framework named Contrastive Unlikelihood Training (CUT)**
  - **Challenge 1: Improving Utilization of Judgmental Feedback**
      CUT framework enables fine-grained inappropriate content detection and correction based on judgmental feedback. By contrasting generation probabilities associated with real judgments (which may contain negative opinions) and fabricated judgments (depicting perfection), it detects and penalizes inappropriate content within a response, allowing the model to learn directly from judgments and make corrections.

  - **Challenge 2: Offline and Online Model Alignment**
      The CUT method demonstrated its effectiveness in both offline and online alignment experiments. In the offline setting, even with only 1317 judgment data, CUT was able to surpass the 175B DaVinci003 model on the AlpacaEval benchmark and outperformed the best baseline by 52.34 points. In the online alignment experiments, CUT showed it could iteratively refine LLMs with model-specific judgment data, improving performance on AlpacaEval steadily from 81.09 to 91.36 points.

#### Implementation and Deployment
The effectiveness of the CUT framework has been demonstrated through various experiments. In offline alignment experiments, with 1317 available judgment data, CUT achieved a win rate of 62.56 on the AlpacaEval benchmark, surpassing the 175B DaVinci003 model and performing 52.34 points better than the best baseline. Online alignment experiments showed continuous performance improvement with four iterations of CUT, where the performance of LLaMA2-chat-13b rose from 81.09 to 91.36 points on AlpacaEval. The study also found that compared to rewards, judgment holds greater potential for aligning LLMs.

#### Summary
The paper presents a new framework for aligning LLMs through direct use of language feedback named Contrastive Unlikelihood Training (CUT) and demonstrates its effectiveness in various scenarios including offline and online alignment, as well as further optimizing both unaligned (cold-start) and already aligned (warm-start) models. Research indicates that judgmental feedback holds greater potential than rewards for aligning LLMs, meriting further investigation.