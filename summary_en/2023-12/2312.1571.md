#### Background
- **Background**
LLMs have exhibited the issue of generating responses with inaccuracies or fabrications, known as "hallucinations," which can restrict their real-world applications.

- **Existing Work**
Previous studies have introduced various methods to mitigate hallucinations in LLMs, including high-quality data selection, reinforcement learning from external feedback, retrieval-enhanced generation, and leveraging model uncertainty. However, they all face the difficult challenge of optimizing LLMs to produce fewer hallucinations.

#### Core Contributions
  - **Introduced a simple Induce-then-Contrast Decoding (ICD) strategy**
      - **Challenge 1: Reducing hallucinations effectively**
      By first constructing a factually weak LLM through inducing hallucinations from the original LLM and then penalizing these induced hallucinations during decoding, the paper enhances the factuality of the generated content. It amplifies predictions from the original model and downplays erroneous predictions through contrastive decoding.

  - **Challenge 2: Addressing the issue without altering the current training objectives**
      The paper points out that maximum likelihood-based next-token prediction may contribute to hallucinations in LLMs. The ICD method proposed does not require changing the training objectives but subtracts the hallucinatory knowledge from the output of the original model during the generation process.

#### Implementation and Deployment
The effectiveness of ICD was evaluated using discrimination-based and generation-based hallucination benchmarks like TruthfulQA and FACTSCORE. The proposed ICD method was shown to enhance the factuality of LLMs significantly, with ICD-equipped Llama2-7B-Chat and Mistral-7B-Instruct achieving comparable performance to ChatGPT and GPT4 on TruthfulQA. Moreover, it allowed Llama2-7B-Chat to surpass its 70B counterpart in factual precision when generating texts on FACTSCORE.

#### Summary
The paper offers a novel method to reduce hallucinations in LLMs by constructing a factually weaker model and subtracting its knowledge in the generation process, improving the generation of factual content.