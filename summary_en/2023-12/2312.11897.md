#### Background
- **Background**
Recent advancements in visual-language models (VLMs) have seen an exponential growth, with new models being pretrained on an increasingly larger scale in terms of both the number of parameters and the dataset size.

- **Existing Work**
Existing work in video sampling faces challenges due to the high redundancy of video data sources. These challenges have been approached through preprocessing modules, multi-modal attention-like mechanisms, and the process is often limited to a small number of video frames, e.g., up to 8 frames in some cases.

#### Core Contributions
  - **Introduced a Text-Conditioned Resampler (TCR)**
    - **Challenge 1: Processing long video sequences**
      TCR addresses the challenge of processing long video sequences by bridging pre-trained visual and language models. It is a transformer-based sampling architecture conditioned on a task, which selects the most relevant frame features to pass to an LLM.
      
    - **Challenge 2: Spatiotemporal understanding of long videos**
      With the emergence of large-scale long-form video datasets such as EGO4D, researchers have had to develop new models and benchmarks to address the challenge of spatiotemporal understanding of extended video lengths. An example is the EgoSchema dataset, which is a pioneering effort in long-video question answering because every question requires the viewing of a 3-minute video.

#### Implementation and Deployment
The TCR, with its lightweight design and cross-attention mechanism, is capable of processing over 100 frames at a time. This enables the model to work with much longer sections of video than prior works. Empirical validation on a variety of assessment tasks has yielded new state-of-the-art results on NextQA, EgoSchema, and the EGO4D-LTA challenge. Additionally, TCR highlights tasks that require extended video contexts, thus providing a means to further evaluate long-range video models.

#### Summary
This paper presents TCR, a novel architecture and pre-training method capable of processing long videos conditioned on textual prompts. It effectively bridges pre-trained visual encoders with LLMs, addressing the challenge of long-form video understanding and sets new best performance benchmarks across several evaluation tasks.