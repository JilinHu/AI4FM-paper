#### Background
- **Background**
This paper discusses the applicability of Large Language Models (LLMs) as research agents in scientific inquiries, specifically their capacity to answer questions by retrieving relevant literature. Current benchmarks tend to focus on LLMs' abilities to respond with widely-known or widely-available information in a given context. They are thus insufficient for assessing an agent's capability to answer questions based on retrieved information. Hence, this research introduces the LitQA dataset for the evaluation of the retrieval-augmented agent called PaperQA.

- **Existing Work**
Existing work has not adequately assessed the capabilities of agents as they often rely on information that models may have encountered during training rather than specially designed to test the model's ability to retrieve and generate responses based on this information.

#### Core Contributions
- **Proposal of PaperQA**
    - **Challenge 1: Dataset Design**
      Current language model training datasets often contain older or widely available information, likely already seen by the model during training. To address this challenge, the LitQA dataset was curated by experts and includes 50 multiple-choice questions, all from the biomedical field, taken from literature published after September 2021. The selected questions are challenging or impossible to answer without retrieving the pertinent paper.

    - **Challenge 2: Performance Evaluation**
      The need for a metric that can accurately answer questions based on retrieved information and manifest uncertainty when uncertain was a challenge. PaperQA was compared with existing commercial tools and human experts in performance on the LitQA dataset, showcasing its significance, and the ability to retrieve relevant papers and the rate of hallucinated citations were scrutinized.

#### Implementation and Deployment
All experiments on PaperQA were implemented within the LangChain's agent framework. PaperQA employed four different LLM instancesâ€”with the agent LLM using GPT-4, the summary LLM using GPT-3.5, the answer LLM using GPT-4, and the ask LLM also using GPT-4. As its search engine, it utilized Google Scholar to gather the top-5 accessible papers. Evaluation results indicate that PaperQA outclasses other LLMs, the AutoGPT agent, and commercial products using Retrieval-Augmented Generation (RAG) on the LitQA dataset and is on par with human experts. Additionally, PaperQA showed a lower rate of incorrect answers compared to all tools and matched human levels of expressing uncertainty when unsure, highlighting PaperQA's calibrated expression of uncertainty when it is uncertain.

#### Summary
The paper presents PaperQA, a retrieval-augmented generative agent for scientific research capable of answering questions based on up-to-date scientific literature with a performance comparable to human experts, and in some aspects even superior. The effectiveness of PaperQA is demonstrated, and its superiority is affirmed through comparative results with human experts and other commercial tools.