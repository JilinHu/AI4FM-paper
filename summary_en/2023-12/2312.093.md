#### Background
- **Background**
The paper discusses the importance of safely deploying large language models (LLMs) and highlights the limitations of existing likelihood-based metrics, such as perplexity, as reliable indicators of the quality of generated content.

- **Existing Work**
Existing research has shown that while LLMs demonstrate good calibration at the token level when choosing correct answers in multiple-choice questions or evaluating true/false statements, they are not as reliable at the sequence level probability estimates as quality indicators.

#### Core Contributions
  - **Introduced a selective generation method based on self-evaluation**
    - **Challenge 1: Obtaining a quality-calibrated confidence score for open-ended generation tasks**
      The researchers instructed an LLM to self-evaluate its answers, using either a multi-way comparison or a point-wise evaluation approach, including a “None of the above” option to explicitly express model uncertainty. Experiments have shown that these self-evaluation-based scores not only improve accuracy but also correlate better with the overall quality of generated content.

#### Implementation and Deployment
By evaluating with TRUTHFULQA and TL;DR, the paper demonstrated that scoring methods based on self-evaluation, like reformulating open-ended generation tasks into token-level prediction tasks and capitalizing on the LLM's strong calibration at that level, not only improve accuracy but also correlate more nicely with the overall quality of generated content. Experiments were conducted with PALM-2 and GPT-3, supporting the effectiveness of this approach.

#### Summary
The paper presents a new method where LLMs are guided to self-evaluate in order to improve the calibration of the quality of their generative output in selective generation scenarios. Experiments show that this method enhances the accuracy and overall quality of the generated content by LLMs.