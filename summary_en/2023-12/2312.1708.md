#### Background
- **Background**
Despite achieving high accuracy rates on math problem-solving benchmarks like GSM8K, existing large language models (LLMs) are criticized for their focus on end results rather than the reasoning process, leading to concerns about data contamination and overfitting to benchmarks.

- **Existing Work**
Existing benchmarks such as GSM8K emphasize result-oriented metrics, which have led to state-of-the-art LLMs showing high accuracy but potentially lacking deeper reasoning capabilities when faced with new tasks.

#### Core Contributions
- **Introduced a new evaluation paradigm, DiagGSM8K**
  - **Challenge 1: Assessing the reasoning process**
      The study highlights that while LLMs could generate accurate solutions for math problem-solving, they lack deep understanding of the reasoning process. The new assessment paradigm proposed in the paper complements the existing benchmarks and more holistically evaluates the model's cognitive and reasoning abilities.

  - **Challenge 2: Cultivating fundamental cognitive abilities**
      Current training paradigms predominantly use inductive learning, which fails to cultivate the understanding of basic rules and principles. The new paradigm requires LLMs to exhibit deeper cognitive understanding abilities, revealing a need for more reflective approaches in training and assessment methods.

#### Implementation and Deployment
Evaluation using DiagGSM8K showed that state-of-the-art LLMs only achieved single-digit accuracy on this more nuanced assessment, with GPT-4 performing up to ten times more accurately than GPT3-5. This indicates that the new assessment paradigm focuses on reasoning processes instead of purely computational outcomes and also exposes fundamental deficiencies in the current evaluation and training approaches.

#### Summary
This paper presents a new evaluation paradigm that challenges LLMs to engage in meta-reasoning, and it introduces the accompanying open-source benchmark DiagGSM8K, adding a new dimension to the evaluation of LLMs' cognitive abilities.