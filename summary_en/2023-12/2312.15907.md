#### Background
- **Background**
The paper acknowledges that Large Language Models (LLMs) exhibit transformative capabilities in various NLP tasks, whereas the potential risks and negative social impacts are increasing concerns, making the alignment of LLMs with human values and intentions challenging for existing techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF).
  
- **Existing Work**
Existing methods face issues such as a significant computational resource requirement for SFT, instability and hyperparameter sensitivity for RLHF, along with the tedious and expensive process of collecting high-quality human feedback. Additionally, adapting to variable human social norms across times and locations is problematic when trying to internalize them using existing alignment methods.

#### Core Contributions
  - **Introduced On-the-fly Preference Optimization (OPO) approach**
    - **Challenge 1: How to construct a rule module reflecting authoritative values for LLMs to adhere to?**
      The challenge is tackled by the rule creation module, which aims to collect and establish a text corpus containing certain human values, organizing laws and morality related rules as structured memories for LLMs to follow externally.
      
    - **Challenge 2: How to design flexible evaluation algorithms?**
      To prevent potential benchmark leakage and to expand testing of alignment rules, the paper proposes a scalable evaluation module that can automatically generate new legal and professional moral questions, based on collected rules.

#### Implementation and Deployment
The paper evaluates the effectiveness of the OPO method using both human-annotated and auto-generated text. New multiple-choice questions are auto-generated based on rules using GPT-4, with high-quality reviewed by well-educated annotators, producing valid legal and professional moral question datasets. Additionally, a comprehensive evaluation was performed on 15 LLMs, including both closed-source variants such as GPT-4 and open-source models of varying sizes across five evaluation datasets, consistently enhancing the alignment of most LLMs.

#### Summary
The research advances a dynamic OPO method that aligns LLMs with the complex and varying landscape of human values in real-time, using collected rules as external memory without further training. Despite limitations in inference efficiency and potential for retrieval model enhancements, extensive experiments across multiple evaluation datasets vouch for the method's effectiveness.