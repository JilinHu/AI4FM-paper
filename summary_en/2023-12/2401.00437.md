#### Background
- **Background**       
The article points out significant advancements in automatic text evaluation using large language models (LLMs) as evaluators. However, the existing sample-wise evaluation paradigm has issues including sensitivity to prompt design, poor noise resistance, and subpar ensemble performance with static references. Unlike current LLM evaluators, which rely only on criteria, human evaluators consider both the criteria definition and comparison among samples.

- **Existing Work**
Due to the labor-intensive and time-consuming nature of human evaluation, researchers have explored various automated methods as a supplement, including rule-based, embedding-based, and learning-based evaluation approaches. These methods achieved progress but still have a considerable gap in consistency with human judgment. The existing LLM evaluators tend to evaluate each sample individually without comparison, resulting in excessive reliance on prompt design and insufficient differentiation between scores.

#### Core Contributions
  - **Introduced a new paradigm named BATCHEVAL**
      - **Challenge 1: Enhance the robustness of prompt design and score differentiation**
          BATCHEVAL conducts batch-wise evaluations by splitting all samples into several batches, then compiling each batch into a prompt for the LLM. This approach introduces in-batch samples as an additional reference to the criteria, reducing dependency on prompt design and enhancing score differentiation through batch comparison, thereby improving robustness.

      - **Challenge 2: Improve diversity and performance in ensemble evaluations**
          Iterative updates of batch compositions in BATCHEVAL provide LLMs with diverse evaluation references, enhancing the diversity and ensemble performance of assessments. Experiments have demonstrated that this method not only increases accuracy but also reduces API costs.

#### Implementation and Deployment
Experiments were conducted across 4 text evaluation tasks, primarily using GPT-4: evaluating turn-level responses in dialogue, text summarization, and story generation. Results show that BATCHEVAL exceeds the best performing LLM evaluators by 10.5% in Pearson correlation with human evaluations, while the API costs are only 64% of those for other methods. Additionally, the generalization of BATCHEVAL across various LLMs, its robustness against prompt design and noise, and hyperparameter choices have been validated through further experiments. The working mechanism of BATCHEVAL has also been explored.

#### Summary
The paper introduces a novel LLM evaluation paradigm—BATCHEVAL—that addresses the issues of robustness and consistency with human judgment in automatic text evaluation. By implementing batch-wise evaluation and iterative processing, BATCHEVAL significantly surpasses existing methods in terms of accuracy and cost-efficiency.