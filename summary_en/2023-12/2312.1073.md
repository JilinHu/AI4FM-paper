#### Background
- **Background**
Despite the impressive achievements of large language models (LLMs) in various NLP tasks, deploying them encounters the challenge of high computational and memory requirements. Researchers typically enhance open-source smaller models through knowledge distillation from LLMs to reduce computational resource costs, which has shown promise. However, these smaller models often fall short in achieving LLM-level performance in tasks that demand advanced reasoning.

- **Existing Work**
The existing knowledge distillation methods have limited efficacy in enhancing the reasoning abilities of models, especially for tasks that rely on coherent and executable reasoning paths, such as mathematical reasoning tasks. These limitations prevent smaller models from performing well on such tasks.

#### Core Contributions
- **Introduced the Mixed Distillation Framework**
    - **Challenge 1: Existing distillation methods are not boosting small models' advanced reasoning capabilities**
        To address this challenge, the paper introduces the Mixed Distillation framework, which leverages Program-of-Thought (PoT) and Chain-of-Thought (CoT) reasoning capabilities from LLMs and distills them into smaller models. PoT is aimed at improving the performance of reasoning results generated by smaller models, while CoT helps to optimize these results simultaneously.

    - **Challenge 2: Limited single-path reasoning efficiency in existing small-model methods**
        To enhance the reasoning abilities of smaller models, the Mixed Distillation framework integrates both PoT and CoT for multi-path reasoning learning. This encourages smaller models to enhance performance by harnessing the combined reasoning strengths from LLMs.

#### Implementation and Deployment
Experiments showed that applying the Mixed Distillation framework to the SVAMP dataset with a 7-billion-parameter model, Llama2, and CodeLlama not only surpassed the improvements of single-path distillation methods but also outperformed LLM (GPT-3.5-turbo) in reasoning accuracy. Through a multi-path reasoning sampling approach, the models achieved impressive accuracy performances of 85% and 85.5%, respectively, marking progress beyond previous distillation approaches.

#### Summary
The Mixed Distillation framework significantly enhanced smaller models' advanced reasoning capabilities by integrating PoT and CoT abilities from LLMs, specifically showing improved performance in mathematical reasoning tasks.