#### Background
- **Background**
The paper discusses the recent advances in large language models (LLMs) that have demonstrated human-like abilities in language understanding and generation for real-world tasks and are even considered a potential pathway to artificial general intelligence (AGI). However, the open-source LLMs currently available are primarily designed for English contexts and perform poorly in Chinese scenarios. To address this, the paper introduces the YAYI 2 model, which is trained for multilingual contexts.

- **Existing Work**
Other open-source LLMs such as Llama and Falcon, though they may perform comparably to proprietary models in English environments, lack optimization for Chinese contexts, resulting in subpar performance on Chinese tasks. To bridge this gap, several Chinese-based LLMs have been proposed, but there is a continued need for expansion and optimization to cater to a broader range of multilingual scenarios. 

#### Core Contributions
  - **Introduced a multilingual large language model named YAYI 2**
      - **Challenge 1: Multilingual recognition and understanding**
      The YAYI 2 model, with 30 billion parameters, was pre-trained from scratch on a multilingual corpus containing 2.65 trillion tokens. The model has been aligned with human values through supervised fine-tuning and reinforcement learning from human feedback.
      
      - **Challenge 2: Data processing and model training**
      A self-developed data processing pipeline was created to enhance data quality. The model is trained to accumulate various knowledge and acquire professional skills such as math, coding, and logical reasoning, enhancing its responsiveness to multilingual scenarios and diverse data formats.

#### Implementation and Deployment
The paper outlines the training process of the YAYI 2 model, which includes acquiring pre-training data, building a multilingual tokenizer, detailing the model architecture, and introducing computing cluster configurations and training strategies. Furthermore, the paper demonstrates the effectiveness of the proposed base model through extensive experiments on multiple benchmarks. According to these tests, YAYI 2 performs better than other open-source models of similar size and even outperforms LLMs with larger parameters in some benchmarks.

#### Summary
The paper presents YAYI 2, a large language model optimized for multilingual scenarios, which significantly improves performance on various tasks, especially in Chinese-related tasks, by pre-training on a large corpus and aligning with human values through multiple approaches.