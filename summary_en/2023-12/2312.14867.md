#### Background
- **Background**
This paper discusses the challenges in effectively evaluating the performance and capabilities of models in the rapidly growing field of conditional image generation, particularly limited explainability.

- **Existing Work**
The existing evaluation techniques like LPIPS, CLIP-Score, and DreamSim have their limitations, such as being insensitive to the end task, potentially failing to measure the desired aspects of the generated images and providing opaque scores with limited explainability. Additionally, some research relied on human evaluations, which encounter challenges such as scalability and subjective preferences, highlighting the need for more uniform evaluation methods in the field.

#### Core Contributions
- **Introduced VIEScore (Visual Instruction-guided Explainable Score)**
  - **Challenge 1: Task Awareness**
      VIEScore is designed to address the inadequacies of traditional metrics, which were often designed to measure a specific aspect of generated images. It can handle all aspects of conditional image generation evaluation due to its instruction-guiding property and can be adjusted according to different instruction requirements.

  - **Challenge 2: Explainability**
      Unlike existing metrics that usually output a single float-point score, VIEScore provides rationales in natural language, helping humans understand the evaluation process. This greatly enhances the trustworthiness of VIEScore and addresses the lack of comprehensive explanations in current metrics.

#### Implementation and Deployment
VIEScore operates using Multimodal Large Language Models (MLLMs) such as GPT-4 and LLaVA that can process image inputs and generate human-like text responses. It does not require training or fine-tuning and has been evaluated on seven prominent conditional image tasks. VIEScore (using GPT-4-v) achieved a high Spearman correlation of 0.3 with human evaluations, where the human-to-human correlation is 0.45. VIEScore competes with human ratings in generation tasks but struggles in editing tasks.

#### Summary
The paper presents an evaluation framework called VIEScore aimed at providing explainable evaluations for conditional image generation tasks. VIEScore overcomes the challenge of existing automated metrics' inability to explain their scoring rationale and is adaptable to various task requirements.