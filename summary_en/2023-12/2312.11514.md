#### Background
- **Background**
This paper focuses on efficient inference of large language models (LLMs) on devices with limited memory. The current methods struggle with meeting the memory requirements for inference, especially when the size of the model surpasses the available computational memory, limiting the ability to deploy models on various devices.

- **Existing Work**
Existing work primarily concentrates on compression techniques such as pruning, quantization, and selective execution. However, these methods don't address the issue of reducing data transfer from flash memory to DRAM (Dynamic Random-Access Memory) during inference nor do they directly solve the high latency problem of flash memory during inference.

#### Core Contributions
  - **Introduced an efficient Large Language Model inference strategy**
      - **Challenge 1: High latency in data transfer**
        The paper proposed strategies like Reducing Data Load, Optimizing Data Chunk Size, and Efficient Management of Loaded Data to minimize latency associated with flash I/O operations, increase the throughput of data transfer, and streamline the management of loaded data. This involves iteratively transferring only the essential non-sparse data from flash memory to DRAM.

      - **Challenge 2: Limited DRAM capacity**
        The paper introduces the Sliding Window Technique to manage neuron data, keeping only the neuron data necessary for the most recent set of input tokens, thus effectively utilizing memory resources. Also, the embeddings and matrices within the transformerâ€™s attention mechanism are constantly retained in the RAM, and only the non-sparse segments of the FFN are dynamically loaded into DRAM as needed.

#### Implementation and Deployment
The proposed method allows LLMs to run on devices with available DRAM memory space being half or even less than the model size, achieving an inference speed improvement of 4-5x on CPU and 20-25x on GPU compared to traditional loading methods. These significant results surpass traditional loading methods, making the deployment of advanced LLMs possible in resource-limited environments.

#### Summary
This research provides a novel and practical solution that effectively reduces the data load and significantly speeds up inference when running large language models on memory-constrained devices, holding substantial significance for practical applications.