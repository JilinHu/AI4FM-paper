#### Background
- **Background**
The paper introduces a new task called "Generating Illustrated Instructions," which involves creating visual instructions tailored to a user's needs. It details the unique desiderata for this task and formalizes it with a set of automatic and human evaluation metrics to measure the validity, consistency, and efficacy of the generated instructions.
  
- **Existing Work**
Existing work includes learning language models on sites like WikiHow for summarization, commonsense procedural knowledge, question answering, and hierarchical reasoning. Multimodal settings involve learning representations from multimodal data, which has been a strong signal source for various tasks such as zero-shot recognition, text-image, and text-video retrieval, temporal segmentation, and activity localization. However, current Text-to-Image (T2I) models can't generate visuals directly from a user query, nor can they produce images that are both faithful to the goal and the step and consistent with each other.
  
#### Core Contributions
  - **Proposed a new model called StackedDiffusion**
    - **Challenge 1: Generating consistent and effective illustrative visuals**
        Existing T2I models fail to generate visuals directly from a user query. StackedDiffusion combines pre-trained large language models with fine-tuned text-to-image diffusion models, using techniques like spatial tiling, text embedding concatenation to prevent information loss in long instructions, and new "step-positional encoding" to better delineate the different steps. These enable StackedDiffusion to generate useful illustrations even with limited instructional data.
    - **Challenge 2: Surpassing the performance of existing methods**
        StackedDiffusion consistently outperforms various baseline approaches and state-of-the-art multimodal LLMs in both automated and human evaluations. It even exceeds human-generated articles in 30% of the cases, showcasing significant potential for the approach.
  
#### Implementation and Deployment
The StackedDiffusion model was trained on stacks of instructional images from websites such as WikiHow, creating full instructional articles with a series of images to uniquely describe the steps. It leveraged large-scale pre-trained language models and fine-tuned text-to-image diffusion models without the need for introducing any new learnable parameters. Techniques employed include spatial tiling for simultaneous multi-image generation, text embedding concatenation to reduce information loss in long instructions, and a new step-positional encoding to more clearly demarcate different steps. After comprehensive ablation studies and human evaluations, results show that StackedDiffusion performs strongly on all metrics, with humans preferring it significantly over existing methods and even surpassing ground truth images in some cases.
  
#### Summary
The paper presents a novel approach called StackedDiffusion for the task of generating illustrated instructions, a task that combines text and images to describe how to achieve a goal. This method overcomes the limitations of current T2I models that fail to generate visuals from user queries directly and surpasses existing methods in human evaluations.