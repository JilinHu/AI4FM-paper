#### Background
- **Background**
Recently, the importance of Large Language Models (LLMs) has been increasing in the research field, with capabilities of such models advancing, making it increasingly crucial to assess their performance and understand their limitations. However, traditional metrics for generative models, such as BLEU, ROUGE, etc., typically capture only one or several aspects of a model's capabilities.
- **Existing Work**
Although recent studies have begun to explore the evaluation of LLMs from a more synthesized perspective, most of this work has mainly focused on the "what" and "where" of evaluation, which are basically the tasks to give LLMs during testing and the type of knowledge they should deal with. However, the "how to evaluate," which covers what standards to use, types of evaluators, how to score, and how to rank, has been discussed much less.

#### Core Contributions
- **Introduced a new evaluation dataset LLMEval and conducted evaluations on 20 LLMs**
    - **Challenge 1: Which criteria should we consider when evaluating LLMs?**
        The paper compared five criteria: accuracy, informativeness, fluency, logical coherence, and harmlessness. The results indicate that existing LLMs all perform notably well in terms of harmlessness across various criteria. The differentiating factors are in the metrics of informativeness and accuracy.
        
    - **Challenge 2: What annotation methods should be used to annotate the outputs of LLMs?**
        The paper utilized a combination of onsite crowd-sourcing and public annotators for manual annotations, along with GPT-4 for automatic evaluations. Experiments demonstrated that onsite evaluations show superior accuracy and consistency in manual evaluations, with a higher aligning level between onsite annotators and GPT-4.

#### Implementation and Deployment
In the evaluations, a total of 2186 individuals participated, generating 243337 manual annotations and 57511 automatic evaluations. The paper performed comparisons and analyses of different settings and drew 10 conclusions that provide insights for future LLM evaluation. Additionally, the paper compared two ranking systems commonly used in competitive sports: the Elo rating system (used in chess games) and the Points scoring system (used in football matches). The study found that the Elo rating system shows instability in LLM evaluation tasks, exhibiting significant variance in results when considering different match sequences and being highly sensitive to noise data, which is difficult to avoid in manual annotation. All annotated data will be released on Github after the anonymity period ends.

#### Summary
The paper focuses on how to evaluate Large Language Models (LLMs), comparing various evaluation criteria, types of evaluators, scoring methods, and ranking systems. It introduces a new evaluation dataset, LLMEval, and assesses 20 LLMs, generating a massive amount of manual and automatic evaluation results. The study provides valuable insights and conclusions for the future evaluation of LLMs.