#### Background
- **Background**       
With the rapid development of large language models (LLMs), there has been a significant demand for employing LLMs for decision-making to enable artificial general intelligence. Traditional approaches often utilize manually crafted examples to prompt LLMs to mimic the human decision process. However, designing optimal prompts is difficult, and patterned prompts may not generalize well in more complex environments.

- **Existing Work**
Previous studies have employed manually crafted examples to guide LLMs in decision-making, leading to decisions oriented in a fixed direction. Moreover, some methods are incapable of generating acceptable decisions in complex environments and are unable to learn from environment feedback due to fixed examples.

#### Core Contributions
  - **Introduced a new model named Large Decision Model with Memory (LDM2)**
    - **Challenge 1: Mimicking human decision-making and adapting to complex environments**
      The LDM2 introduces a dynamic memory mechanism for constructing dynamic prompts to guide LLMs in making proper decisions according to the current state. It comprises two phases: memory formation and refinement. In the memory formation phase, human behaviors are decomposed into state-action tuples using LLMs' summarizing ability. These tuples are stored in memory with indices generated by LLMs for facilitating retrieval of the most relevant subset of tuples based on the current state. The memory refinement stage employs tree exploration to uncover more suitable decision processes and enrich the memory by adding valuable state-action pairs.

    - **Challenge 2: Achieving dynamic learning and adaptation to new situations**
     Unlike traditional imitation learning, LDM2 features a dynamic memory refinement stage that utilizes exploration and feedback from the environment to enhance memory. This stage adds highly rewarded state-action tuples to the memory, thereby achieving dynamic learning capabilities. This allows LDM2 to expand the action space of LLMs and to address new circumstances not covered by the initial memory.

#### Implementation and Deployment
The proposed LDM2 was evaluated in two interactive environments: WebShop and ALFworld, demonstrating that it outperforms standard few-shot prompting methods and other methods prompted with verbal reasoning. The success examples were analyzed in both tasks, revealing that LDM2 has a broader action space than methods using fixed example prompts, enabling LLMs to tackle unseen or complex situations. Further ablation studies were conducted to assess the memory refinement mechanism, highlighting the effectiveness of adding highly rewarded state-action tuples into the memory.

#### Summary
The paper presents the LDM2 model, which incorporates a dynamic memory mechanism and tree exploration approach to augment the decision-making capabilities of LLMs to adapt to more complex and unknown environments, and to realize dynamic learning abilities.