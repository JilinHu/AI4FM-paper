#### Background
- **Background**
This paper discusses the challenges associated with compressing large language models (LLMs) to make them more efficient, smaller in footprint, and locally deployable while maintaining a trade-off with model quality. Previous research mainly focused on general performance metrics such as perplexity and task accuracy, neglecting finer metrics like parametric knowledge assessment. To address this Research gap, the study analyzes different model families (ENCODER, ENCODER-DECODER, DECODER) employing benchmarks like LAMA and LM-HARNESS to systematically quantify the impact of popular compression techniques on model performance.

- **Existing Work**
Current research typically revolves around single models or specific model types with metrics that are too general and do not align closely with practical task metrics. Moreover, most compression research has been focused on DECODER models, with a dearth of studies on large ENCODER and ENCODER-DECODER models.

#### Core Contributions
  - **Introduced a comprehensive analysis of the impact of compression on parametric knowledge in LLMs**
      - **Challenge 1: The importance of preserving parametric knowledge**
        The paper emphasizes the importance of preserving parametric knowledge, which is acquired during pretraining and stored in model weights. This is crucial for tasks that involve reasoning and specialized applications. The study examines the impact of different compression schemes on this parameter knowledge across various model families, using both pruning and quantization techniques, and their performance on downstream reasoning tasks.

      - **Challenge 2: Comprehensive analysis across various model families**
        The paper breaks new ground by analyzing the performance of a variety of model families across reasoning tasks, by applying pruning and quantization. It provides pragmatic insights that aid practitioners in making informed decisions about the compression of their models.

#### Implementation and Deployment
The study presents several key findings for LLMs, including that pruning all modules together significantly impacts parametric knowledge; parametric knowledge drastically declines at pruning levels of over 50% for all models; quantizing attention modules impacts performance less compared to feedforward networks across all models; and structured pruning at the final layer is more detrimental compared to unstructured pruning. These conclusions are based on a comprehensive study across a wide variety of model families and specific compression techniques, providing guidance for practical implementation.

#### Summary
This research represents one of the first large-scale investigations into the impact of compression techniques on the parametric knowledge of LLMs, offering significant insights for practitioners, especially regarding decisions related to pruning and quantization techniques.