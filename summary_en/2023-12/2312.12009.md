#### Background
- **Background**
The paper discusses the importance of actively inferring user preferences, for instance by asking pertinent questions, in any human-facing decision-making system. Instruction-tuned large language models (LLMs) while demonstrating remarkable capability in following task instructions in natural language, are inefficient in preference inference, generating less informative queries which result in excess user interaction and a reduction in system usability.

- **Existing Work**
Prior research has attempted to train NLP systems specifically for interactive inference, but these methods required expensive, task-specific data collection, severely limiting their usability. LLMs, through prompting, avoid the data collection step; however, off-the-shelf LLMs are still ineffective in preference inference and necessitate numerous interactions with users to make accurate preference predictions.

#### Core Contributions
- **Introduced an inference-time probabilistic reasoning algorithm**
    - **Challenge 1: Generating informative questions**
        The research addresses the challenge of suboptimal interactive preference inference by existing LLMs. They propose an algorithm that constructs a complete probabilistic generative model based on LLM-defined scores and selects questions that maximize information gain, thus, generating more informative queries.

    - **Challenge 2: Reducing user interaction**
        By asking more informative questions, the algorithm infers user preferences faster and reduces the number of interactions required by the users. It extracts more information in fewer questions by increasing LLM computation, considered as a correct trade-off for the future.

#### Implementation and Deployment
The approach was evaluated in a simplified online shopping setting using realistic web shopping benchmark data. The tasks involved a list of products and a user with latent preference for a target product. The proposed inference-time active inference method outperformed vanilla instruction-tuned LLMs and ReAct by effectively inferring the target product with fewer queries, demonstrating the method's potential as an enhancement of LLM capabilities.

#### Summary
This study introduced a real-time algorithm that accelerates LLMs' inference of user preferences by generating informative questions, demonstrated to reduce user interaction and improve task performance in an online shopping scenario.