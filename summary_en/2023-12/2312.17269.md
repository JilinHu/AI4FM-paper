#### Background
- **Background**
This paper addresses Conversational Question Answering (ConvQA) over Knowledge Graphs (KGs), where state-of-the-art methods often struggle with inexplicit question-answer pairs that are easy for humans to understand in the context of a conversation but challenging for machines to interpret, thus degrading ConvQA performance.
- **Existing Work**
Existing methods are unable to deal with the nuanced, inexplicit inputs that are commonly encountered in ConvQA, which hampers their ability to improve system performance.
#### Core Contributions
  - **Introduced a Reinforcement Learning-based model, CoRnNet**
      - **Challenge 1: Enhancing ConvQA Performance**
      CoRnNet improves ConvQA performance by utilizing question reformulations generated by large language models (LLMs). It adopts a teacher-student architecture, with a teacher model learning question representations from human-written reformulations and a student model indirectly trained to mimic the teacher's output using LLM-generated reformulations to achieve near-human performance.
      - **Challenge 2: Locating the Correct Answer**
      CoRnNet employs a Reinforcement Learning model trained to use the learned question representation to locate the correct answer within a KG. This approach has proven to be more effective than existing state-of-the-art ConvQA models in experiments.

#### Implementation and Deployment
The CoRnNet model demonstrated superior performance in conversational question-answering tasks on real-world datasets. On the test set, where no question reformulations are provided, the trained model outputs the answer for each question in each conversation. CoRnNet's results consistently outperformed state-of-the-art ConvQA models, validating its effectiveness in enhancing ConvQA performance.

#### Summary
CoRnNet represents a novel RL model for non-dialogue ConvQA tasks with LLM-generated reformulations, showing superior performance over other advanced models.