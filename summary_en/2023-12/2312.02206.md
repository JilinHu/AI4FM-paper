#### Background
- **Background**
This paper discusses how to better model human preferences in long-form question-answering systems, addressing the issue that current data may not capture the subtle nuances of human preferences.

- **Existing Work**
Current work does not adequately capture the subtle nuances of human preferences in long-form answers as models often overlook these nuances in their training methods.

#### Core Contributions
- **Introduced an axiomatic framework**
  - **Challenge 1: Capturing subtle differences in human preferences**
      The axiomatic framework they introduced can generate or augment training pairs to better reflect real human preferences, even when such nuances might not be present in the existing data.

  - **Challenge 2: Scoring both human and LLM-generated answers on the same scale**
      They trained standalone preference models (PM) with parameters ranging from 220 million to 7 billion, which can score both human and LLM-generated answers on the same scale while filtering out noise such as text length and style, proving to be better than training based solely on human upvotes.

#### Implementation and Deployment
According to the literature, the experimental results indicate that including proper axiomatic signals during training improves the agreement of the Preference Model (PM) with both weak human upvotes and expert human annotators, even surpassing the capabilities of GPT-4. This implies that GPT-4 may be overkill for preference scoring.

#### Summary
The axiomatic framework proposed in this paper offers a new method for preference modeling in long-form question-answering, closely examining human preferences and optimizing the accuracy and efficiency of preference scoring.