#### Background
- **Background**
The paper discusses the remarkable performance of large language models (LLMs) in complex language modeling tasks. However, the high computational and storage requirements of LLMs pose obstacles to their widespread use on edge devices. Quantization has been introduced to enhance the efficiency of LLMs on devices. Research has shown that weight quantization at 8-bit or lower is feasible with minimal impact on end-to-end task performance, but activation quantization has yet to be realized. Furthermore, mainstream edge devices still face challenges in efficiently executing these sub-8-bit quantized networks.

- **Existing Work**
Current efforts have mainly focused on 4-bit weight quantization while keeping activations in the floating-point (FP16) format. This practice limits efficient model inference speed-up on common edge devices, which generally support only 16x16 and 8x8 integer multipliers. Particularly, activation quantization tends to negatively affect task performance, especially as the model size increases, due to pronounced outliers in activations. Moreover, the large model size of LLMs makes it challenging for many academic institutions to afford the associated training costs. Therefore, post-training quantization (PTQ) has become a popular method, but it fails to minimize the quantization error caused by these outliers in activations.

#### Core Contributions
  - **Introduced Agile-Quant Framework**
    - **Challenge 1: Activation Quantization and Outlier Handling**
        The Agile-Quant framework proposed in the paper leverages hardware latency profiling and activation analysis to introduce a basic activation quantization strategy, aiming to balance task performance and real inference speed trade-offs. It then uses an activation-aware token pruning technique to reduce outliers and their adverse impact on attentivity, effectively eliminating some as they are typically concentrated within the same or adjacent channels.

    - **Challenge 2: Edge Device Hardware Implementation Optimization**
        The paper designs an edge-oriented optimization for the hardware implementation of Agile-Quant to fully utilize SIMD instructions. This includes a SIMD-based 4-bit multiplier for efficient INT4 multiplication and the Two-Refine Improved by Pruning (TRIP) matrix multiplication to mitigate the impact of outliers.

#### Implementation and Deployment
Agile-Quant was applied to LLMs of various scales, including LLaMA, OPT, and BLOOM, with 4-bit or 8-bit activation and 4-bit weight quantization. Experiments show that Agile-Quant achieves simultaneous quantization of model weights and activations while maintaining task performance comparable to existing weight-only quantization methods. In 8-bit and 4-bit scenarios, Agile-Quant achieved on-device speedups of up to 2.55x across multiple edge devices compared to its FP16 counterparts, marking pioneering progress in the field.

#### Summary
This paper introduces the Agile-Quant, an activation-guided quantization framework to accelerate the inference of large language models on edge devices. Agile-Quant overcomes challenges associated with activation value outliers and edge device hardware implementation, achieving task performance comparable to weight-only quantization methods while significantly increasing inference speed on actual devices.