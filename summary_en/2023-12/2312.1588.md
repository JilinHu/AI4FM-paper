#### Background
- **Background**
LLMs have shown superior natural language understanding and zero-shot capabilities but suffer from limitations in knowledge retention and hallucination, which impact their performance in complex reasoning and QA tasks, especially in high-stakes areas like medicine.

- **Existing Work**
Existing methods involve manually designed grammars and rules or embed semantic information into low-dimensional vectors to perform reasoning over KGQA tasks. These approaches lack generalizability and suffer from a lack of interpretability.

#### Core Contributions
  - **A novel framework, KnowledgeNavigator**
    - **Challenge 1: Efficient and accurate retrieval of external knowledge from the knowledge graph to enhance LLM reasoning**
      KnowledgeNavigator mines and enhances question constraints to guide reasoning, iteratively retrieves, and filters external knowledge from the knowledge graph with LLM guidance, and converts structured knowledge into LLM-friendly prompts to aid reasoning.

    - **Challenge 2: Overcoming the noise and multi-hop search challenges in knowledge graphs**
      The framework predicts the retrieval range for a given question and generates similar questions. It iteratively retrieves and filters potential candidate relations and entities, recalls required knowledge, aggregates it, and converts it into natural language to avoid LLM's limited processing capability on triples.

#### Implementation and Deployment
KnowledgeNavigator was evaluated on multiple public KGQA benchmarks like MetaQA and WebQSP, demonstrating significant effectiveness and generalization capabilities, outperforming previous knowledge graph enhanced LLM methods and comparably performing to fully supervised models. It exceeded KV-Mem by 16.8%, 46.1%, and 36.8% on three datasets, establishing its effectiveness and robustness. Ablation studies confirmed that each component of KnowledgeNavigator is effective and essential.

#### Summary
The paper introduces KnowledgeNavigator, a novel framework designed to enhance LLM reasoning over knowledge graphs, addressing LLM's limitations in complex reasoning tasks. The effectiveness demonstrated by the experiments suggests potential for broader application of LLMs in high-risk and sensitive domains.