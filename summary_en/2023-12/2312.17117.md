#### Background
- **Background**
The paper discusses the problem of Temporal Sentence Grounding (TSG) in long videos, which requires models to understand natural language queries and locate corresponding moments in videos. Traditional methods for TSG in short videos encounter challenges in long videos, mainly involving temporal reasoning in complex contexts and processing multimodal information with textual speech.

- **Existing Work**
Existing approaches for TSG require high computational costs for training on long-video datasets and generally suffer from dataset-specific fitting bias, resulting in poor generalization. Moreover, such methods fail to capture the rich semantics present in textual speech, which hampers their ability to handle long speech-intensive videos. Attempts to utilize multimodal large language models for videos (MLLM-V) have made some progress but still struggle to align well with the TSG task, resulting in inadequate temporal reasoning abilities for long videos.

#### Core Contributions
- **Introduced the Grounding-Prompter method**
    - **Challenge 1: Temporal reasoning and multimodal understanding in long videos**
        The paper proposes to reformulate the TSG task into a long-textual task and improve temporal reasoning abilities of LLMs in complex contexts through compressed task textualization and a Boundary-Perceptive Prompting strategy.
  
    - **Challenge 2: Efficient and robust TSG performance**
        The method constrains the LLMs by designing a multiscale denoising Chain-of-Thought (CoT), setting validity principles, and introducing one-shot In-Context-Learning (ICL) to generate reasonable predictions, thereby enhancing TSG task understanding and temporal reasoning.

#### Implementation and Deployment
Empirical results demonstrate that Grounding-Prompter achieves state-of-the-art performance on the VidChapters-mini dataset, outperforming other baseline methods significantly. Ablation studies further confirm the effectiveness of this approach, showing the benefits of leveraging both textual speech and visual modalities to handle complex and noisy contexts in long videos.

#### Summary
This paper presents the Grounding-Prompter method, addressing the TSG challenge in long videos by combining LLM with temporal reasoning and multimodal information, demonstrating the effectiveness of prompting LLM with multimodal data, and validating its superiority in TSG tasks for long videos through experiments.