#### Background
- **Background**
The paper identifies the challenge that Large Language Models (LLMs) such as ChatGPT face in understanding and interacting with Graphical User Interfaces (GUIs), which restricts their potential for enhancing automation levels.

- **Existing Work**
Previous work relied on textual inputs like HTML or OCR results, which is limited due to the characteristics of GUIs, such as the lack of standard APIs for interaction, valuable information not easily conveyed through words (icons, images, diagrams, spatial relations), and text-rendered elements on webpages (like canvas and iframe) that can't be parsed to understand their functionality by HTML.

#### Core Contributions
  - **Introduced an 18-billion-parameter Visual Language Model (VLM) CogAgent**
    - **Challenge 1: Insufficient training data**
      The paper notes that most current VLMs are pre-trained on datasets like LAION, which consist of natural images, differing from the distribution of GUI images. To address this, the authors constructed a large-scale GUI and OCR annotated dataset for continued pre-training.
    
    - **Challenge 2: Balancing high-resolution and computational demands**
      Tiny icons and text ubiquitous in GUIs are hard to recognize at the commonly-used resolution of 224×224 pixels, but increasing the resolution would significantly lengthen the sequence in language models, resulting in excessive training and inference computation. To combat this, CogAgent introduced a cross-attention branch design that balances resolution with hidden size, keeping computation within a reasonable budget. Specifically, it combines the original large ViT (4.4B parameters) with a new small high-resolution cross-module (0.30B parameters image encoder) to co-model visual features.
    - ...

#### Implementation and Deployment
CogAgent excels in popular GUI understanding and decision-making benchmarks such as AITW and Mind2Web, marking the first instance of a generalist VLM outperforming LLM-based methods that use extracted structured text. Although CogAgent is focused on GUIs, it also sets state-of-the-art performance on nine visual question-answering benchmarks, including VQAv2, OK-VQA, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. Furthermore, the high- and low-resolution branches in CogAgent significantly reduce the compute cost for processing high-resolution images. For instance, the FLOPs for CogAgent-18B with 1120×1120 inputs are less than half that of CogVLM-17B with default 490×490 inputs.

#### Summary
CogAgent breaks the limitation of pure text-based approaches by efficiently tackling the challenge of understanding and navigating GUIs with combined high and low-resolution image encoders and visual language models. The model achieves leading performance on nine visual question-answering benchmarks, propelling the future research and application of AI agents powered by advanced VLMs.