#### Background
- **Background**
The paper posits that although existing benchmarks are capable of assessing the reasoning skills of Large Language Models (LLMs), these benchmarks are limited. Current benchmarks being static and publicly accessible enable models to potentially tailor their responses to specific benchmark metrics, which might lead to inflated performance results.

- **Existing Work**
Existing evaluation methodologies mainly rely on datasets composed of human-generated questions and standard answers, such as MMLU and GAOKAO, as well as the French National Math Exam. However, these evaluations typically lack a quantitative metric that measures the difficulty level or the extent of reasoning necessary to answer them, leading to limited understanding of LLMs' logical reasoning capabilities.

#### Core Contributions
  - **Introduced a new benchmark named NPHardEval**
    - **Challenge 1: How to conduct a more comprehensive and dynamic evaluation?**
      NPHardEval incorporates a dynamic update mechanism where the data points are refreshed monthly to mitigate the risk of models overfitting to the benchmark, providing a more accurate and reliable assessment method.

    - **Challenge 2: How to assess LLMs' reasoning capabilities across a range of complexity class problems?**
      NPHardEval includes 900 algorithmic questions extending up to the NP-Hard complexity class. These questions were carefully selected to represent a wide range of complexity class below the NP-hard complexity class, offering a rigorous measure of LLMs' reasoning ability.

#### Implementation and Deployment
The NPHardEval benchmark evaluated 10 different LLMs, including GPT-4 Turbo, Claude 2, and GPT-3.5 Turbo, using zero-shot prompts as the baseline measure of performance. The experimental results showed that all models significantly declined in performance when faced with NP-Hard problems, with an average weighted accuracy of 0.26 in P and NP-Complete problems, which dropped to 0.03 for NP-Hard problems. Closed-source models generally outperformed open-source models. Across different complexity class problems, test results indicated that LLMs performed better on P and NP-Complete problems, while significantly worse on NP-Hard problems. Additionally, in the experiment on learning in context, closed-source models displayed consistency in learning algorithmic skills from examples, indicating they were learning rather than mere mimicry of solutions.

#### Summary
This paper presents a novel method of assessing the reasoning abilities of LLMs through the NPHardEval benchmark. The benchmark covers a broad range of problems from polynomial time complexity to NP-Hard levels, and it features a dynamic data updating mechanism to prevent model overfitting, ensuring reliable and authentic assessment results. The findings significantly advance the understanding of current capabilities of LLMs and pave the way for improving the reasoning abilities of these models.