#### Background
- **Background**
This paper investigates the reasoning capacities of large language models (LLMs) in solving recent competition-level programming problems on Codeforces, which are expert-crafted, unique, and demand a deep understanding and robust reasoning skills.

- **Existing Work**
Previous studies had focused on basic coding capabilities instead of the advanced reasoning and mathematical modeling skills required for AI. The competition-level programming problems on Codeforces, acknowledged by programming competition enthusiasts, are of high quality and come with comprehensive testing cases, making them suitable for evaluating the reasoning abilities of LLMs. Moreover, the problems on Codeforces are ranked using the Elo rating system, which is more accurate than common metrics due to its consideration of competitors' performance.

#### Core Contributions
  - **Introduced a comprehensive evaluation method**
    - **Challenge 1: How to assess LLMs' true reasoning capabilities and handle data contamination issues?**
      The paper challenges current LLMs' ability to solve unseen complex reasoning problems by assessing GPT-4's zero-shot performance on problems released after September 2021, which shows a drop in performance, highlighting potential data contamination and the challenges of solving these complex issues.

    - **Challenge 2: How to enhance LLMs' performance on such problems?**
      To improve performance, researchers investigated methods including targeted fine-tuning, Chain-of-Thought prompting, and problem description simplification. However, none of these methods provided consistent improvements, especially for harder problems, affirming that difficult and unseen programming questions effectively evaluate LLMs.

#### Implementation and Deployment
The team conducted an in-depth analysis of GPT-4 and other coding LLMs' zero-shot performances on Codeforces. Key insights include GPT-4's significant underperformance on problems after September 2021; limited ability to tackle difficult problems; and struggles with the first test case, potentially due to misunderstanding the problem. These phenomena were also noticed in other LLMs, suggesting insufficient reasoning ability is a common issue. The attempts and analyses indicate that these competition-level problems are excellent resources to assess LLMs' genuine reasoning capabilities and promote the development of LLMs with stronger reasoning and better generalization abilities in the future.

#### Summary
The study has revealed inadequacies in LLMs like GPT-4 when assessing their real-world reasoning capabilities using competition-level programming questions, suggested methods for improvement, and highlighted the significance of such problems as efficient evaluators of LLMs, thus fostering further research into enhancing complex reasoning abilities in LLMs.