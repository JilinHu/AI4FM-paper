#### Background
- **Background**
The paper acknowledges that pre-trained large language models (LLMs) require fine-tuning to become more responsive to natural language instructions. While existing parameter-efficient fine-tuning (PEFT) methods do contribute to the tuning of LLMs, they may not attain the performance achievable with full-parameter tuning. Federated Learning (FL) emerges as a privacy-preserving method to leverage the abundant data on end-user devices for fine-tuning models. However, traditional full-parameter tuning has been impractical due to the prohibitively high communication overhead for both servers and clients, particularly for billion-sized LLMs.

- **Existing Work**
Existing solutions for full-parameter tuning of LLMs on devices are not feasible due to high communication costs proportional to the model sizes. This is a major issue for billion-sized models. Methods like backpropagation (BP) and most BP-free methods including zeroth-order optimization (ZOO) encounter this problem. Additionally, BP-based procedures also require large memory footprints that are not realistic for most end devices.

#### Core Contributions
  - **Introduced a method named FedKSeed**
      - **Challenge 1: How to reduce communication costs?**
         Past ZOO methods compromised other performance aspects to reduce communication costs. The FedKSeed method employs a limited set of random seeds, significantly lowering the communication cost for full-parameter tuning of LLMs to less than 18 kilobytes per round, with memory efficiency on par with the requirements of inference. 

      - **Challenge 2: How to enhance model accuracy and computational efficiency?**
         FedKSeed further develops a strategy to evaluate the significance of ZOO perturbations, enabling probability-differentiated seed sampling, which prioritizes perturbations with a higher impact on model accuracy. By refining the seed pool in this fashion, it accelerates the synchronization with the latest model state, thereby increasing computational efficiency and model accuracy.

#### Implementation and Deployment
The principal implementation of FedKSeed includes using ZOO with a set of random seeds for federated full-parameter tuning, allowing direct adjustments of billion-sized LLMs on devices. In experimental research, the method has been validated across six different scenarios, involving various LLM datasets and data partitions, to outperform existing federated LLM fine-tuning methods in terms of communication efficiency and new task generalization.

#### Summary
The paper presents a novel approach, FedKSeed, for federated full-parameter tuning using ZOO with a fixed set of seeds, substantially reducing the communication overhead required for tuning billion-sized LLMs, while achieving higher model accuracy and computational efficiency.