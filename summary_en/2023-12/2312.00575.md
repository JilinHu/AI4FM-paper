#### Background
- **Background**
The study explores whether instruction-tuning enhances the alignment of large language models (LLMs) to the human language system. It evaluates 25 LLMs with parameters ranging from 77 million to 33 billion across three neural datasets with human subjects reading naturalistic stories and sentences, finding instruction-tuning generally improves the alignment of LLMs to brain activity.

- **Existing Work**
Current research focuses on enhancing LLMs performance and output quality, allowing them to adapt to new tasks with minimal task-specific training. However, these studies often overlook how instruction-tuning alters LLMs' internal representations to achieve these enhancements. Although previous work used brain activity to interpret neural networks and build more performant models, the question of how instruction-tuning affects the alignment of LLMs' internal representations with brain activity remains an open one.

#### Core Contributions
  - **Introduced an evaluation method**
    - **Challenge 1: Improving alignment between LLMs and brain activity**
        The study uses the previously developed Brain-Score package to train a linear function, using LLM representations as input features to predict fMRI voxels associated with the human language system and evaluates the alignment of LLMs using the Pearson correlation between the representation of LLMs and actual brain activity data. This method reveals that instruction-tuning aligns the internal representation of LLMs more closely with human brain recordings.
    - **Challenge 2: Exploring factors affecting the similarity between LLM representations and human brains**
        By calculating the Pearson correlation between LLM brain alignment and various properties of LLMs, researchers identified factors affecting the similarity of representation between LLMs and the human brain. The results show world knowledge and model size are key determinants of brain alignment.

#### Implementation and Deployment
The study found instruction-tuning generally enhances the alignment between large language models and brain activity. The LLMs involved in the study include versions fine-tuned by various institutions on different types of instruction datasets. Additionally, the study assessed models on performance across the MMLU benchmark for multidomain knowledge and BBH for different reasoning abilities, as well as the correlation between these performances and brain alignment. It revealed that brain alignment correlates strongly with world knowledge across all MMLU subject domains and the world knowledge category in BBH but not significantly with other problem-solving abilities in BBH like algorithmic or multilingual reasoning.

#### Summary
The study shows that large language models trained through instruction-tuning exhibit better representation of world knowledge and alignment with human brain activity. This provides a crucial perspective for the future development of LLMs to incorporate world knowledge into the models.