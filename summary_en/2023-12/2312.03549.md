#### Background
- **Background**
This paper discusses how to perform distributed training of large language models (LLMs) in heterogeneous computing environments with different types of Network Interface Cards (NICs). RDMA technology, especially the InfiniBand and RoCE implementations, is essential for data transfer and low-latency communication in high-performance computing, cloud computing, and large data centers. However, the infrastructure of existing GPU clusters does not match the rapid development of LLMs, with network connectivity between GPU clusters in different locations often being limited. Moreover, InfiniBand and RoCE are not compatible, and current training frameworks like Megatron-LM and Megatron-DeepSpeed face challenges in such heterogeneous NIC environments and cannot fully utilize GPU devices.

- **Existing Work**
Existing distributed training frameworks fail to maximize the use of GPU computing resources because they do not fully adapt to heterogeneous NIC environments.

#### Core Contributions
  - **Introduced Holmes, a distributed training framework**
    - **Challenge 1ï¼šGPU Device Allocation in Heterogeneous NIC Environments**
        The paper proposes a solution to the scheduling problem of combined computation and communication tasks in heterogeneous NIC environments. The goal is to optimize the throughput of devices during the LLM training process by utilizing the characteristics of the network environment, ensuring the maximization of device efficiency by allocating high-volume communication tasks to faster-connected GPU devices.

    - **Challenge 2: Compatibility and Scalability**
        The Holmes framework is designed to be compatible, seamlessly operating with different RDMA NICs across multiple GPU clusters and easy to scale up to thousands of GPU devices. It is based on the popular Megatron-LM framework, which has successfully handled models with up to a trillion parameters.

#### Implementation and Deployment
The Holmes framework shows a clear advantage over other mainstream LLM training frameworks (such as Megatron-LM, Megatron-DeepSpeed, and Megatron-LLaMA), especially in heterogeneous NIC environments. In terms of TFLOPS (teraFLOP/s per GPU) and throughput (number of samples processed per second), Holmes performs better. It includes innovative improvements to data parallelism components, such as Self-Adapting Pipeline Partition, Automatic NIC Selection, and Overlapped Distributed Optimizer. Additionally, the framework has a notably higher speedup ratio compared to other frameworks across varying numbers of nodes.

#### Summary
The paper successfully introduces Holmes, a framework that facilitates training LLMs in heterogeneous NIC environments. Empirical studies confirm that Holmes can achieve performance levels in these environments comparable to those possible with homogeneous RDMA NICs. This significant advancement makes LLM training more accessible and expands the potential for efficient scaling within the broader research community.