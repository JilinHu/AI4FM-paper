#### Background
- **Background**
The paper notes that training visual embeddings with labeled data supervision is the standard setup for representation learning in computer vision. Inspired by the recent success of masked image modeling (MIM) in self-supervised representation learning, the authors propose a simple yet effective setup that can easily integrate MIM into existing supervised training paradigms. In their design, in addition to the original classification task applied to a vision transformer image encoder, they add a shallow transformer-based decoder on top of the encoder and introduce an MIM task that tries to reconstruct image tokens based on masked image inputs.

- **Existing Work**
Current work mainly focuses on representation learning using classification loss as supervision and has not considered combining MIM methods. Existing supervised training methods use a single semantic label for each image as supervision, while MIM attempts to predict a large number of low-level token-level details. Therefore, existing methods may not have provided a way to integrate MIM into the supervised learning paradigm to further improve learned image representations.

#### Core Contributions
  - **A training setup that combines supervised representation learning with MIM**
      - **Challenge 1: How to integrate MIM into existing supervised training paradigms**
        The paper presents a new setup that combines supervised learning with MIM during training. The proposed setup includes an image encoder and a shallow decoder; each image is processed via two tasks: a standard supervised learning task with classification loss and a MIM task that encodes the masked image and decodes image tokens. The aggregation of the two losses from both tasks is applied to the network to guide training.

      - **Challenge 2: Ensure that this setup does not introduce significant training or inference overhead**
        Since the original image encoder is shared between the supervised learning task and the MIM task, and the decoder is shallow and introduces no extra training data, this setup imposes minimal change to any existing architecture. During testing, since only the trained encoder is needed to compute the representations, the setup has the same inference cost as standard supervised models trained without MIM.

#### Implementation and Deployment
On the ImageNet-1k dataset, the ViT-B/14 model trained with this method reached 81.72% validation accuracy, which is 2.01% higher than the baseline model. In the K-Nearest-Neighbor image retrieval evaluation on ImageNet-1k, the same model surpassed the baseline by 1.32%. In the semantic segmentation task on the ADE20k dataset, the model exceeded the baseline by 1.21%. These results demonstrate that the learned image representation was significantly improved compared to the baseline model without the MIM task. Additionally, a comprehensive ablation study on the choice of training parameters was conducted, and it was shown that the method could be easily extended to larger models and datasets.

#### Summary
This paper proposed a new training setup that integrates supervised representation learning with MIM, significantly enhancing the quality of representation learning for downstream tasks such as classification, image retrieval, and semantic segmentation without introducing significant training or inference overhead.