#### Background
- **Background**       
The article points out that Large Language Models (LLMs) have made significant progress in solving tasks requiring complex reasoning, primarily due to the application of the Chain of Thought (CoT) method. The CoT method has a significant impact on enhancing the performance of LLMs in multi-modal tasks, such as multi-modal question answering. However, integrating CoT with LLMs is still a challenge due to the inherent complexity of multi-modal examples, and the selection of the optimal multi-modal reasoning examples remains less explored.

- **Existing Work**
Existing research methods failed to provide a mechanism for dynamically and automatically selecting CoT examples for multi-modal reasoning. Given the complex combination of text and visual data, determining the most relevant and informative examples is not straightforward, necessitating new technological means to address this issue.

#### Core Contributions
  - **Introduction of a multi-modal retrieval-augmented chain of thought reasoning approach**
    - **Challenge 1: How to select appropriate multi-modal reasoning examples**
        To address this challenge, the paper proposes the use of retrieval mechanisms to dynamically and automatically select examples based on cross-modal similarities, refining the CoT reasoning process in multi-modal scenarios.

    - **Challenge 2: How to ensure the diversity of examples**
        To further ensure a more comprehensive and diverse selection, the paper innovatively applies Stratified Sampling, grouping examples according to their features and retrieving from different groups to promote the diversity of examples.

#### Implementation and Deployment
The paper demonstrates significant improvements in LLM performance through a series of experiments on the ScienceQA dataset, establishing a new state-of-the-art in multi-modal reasoning tasks. The ChatGPT-based method of the approach outperformed Chameleon (ChatGPT) by 2.74% with an accuracy of 82.67%, while the GPT-4-based method surpassed Chameleon (GPT-4) by 0.89% with an accuracy of 87.43%. Additionally, an early comprehensive assessment of GPT-4V on ScienceQA reveals that its zero-shot performance substantially outperforms the text-only GPT-4, highlighting the benefits of integrating visual context.

#### Summary
The paper successfully elevated the CoT reasoning capabilities of LLMs in multi-modal tasks by introducing a dynamic automatic retrieval mechanism and stratified sampling method. The proposed approach not only improved model performance but also refined the reasoning process through diverse example selection, setting a new performance benchmark in the field of multi-modal reasoning.