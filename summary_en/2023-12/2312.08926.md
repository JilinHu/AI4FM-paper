#### Background
- **Background**
The paper discusses the challenges that Large Language Models (LLMs) face in solving complex mathematical problems, which require a range of abilities, including parsing statements, associating domain knowledge, conducting compound logical reasoning, and integrating intermediate rationales. Addressing all these issues simultaneously can be difficult for LLMs, leading to confusion in the generation process.

- **Existing Work**
Existing studies attempt to extend LLMs with agent-based systems to mimic social interactive behaviors, such as task decomposition, collaboration, competition, and interaction with the environment. These include leveraging LLMs potential through approaches like CoT, Least-to-Most, ToT, etc. However, there has not yet been exploration in systematically decomposing and meticulously modeling the complex mathematical problem-solving process.

#### Core Contributions
  - **Introduced a Planner-Reasoner-Executor-Reflector (PRER) framework**
      **Challenge 1: Complex Mathematical Reasoning**
      The paper proposes a MathAgent framework named PRER, which simulates a logical function known as PRER, inspired by Belief-Desire-Intention (BDI). The framework features four key modules: Planner, Reasoner, Executor, and Reflector, each simulated by an LLM core. It emulates the human-like process of solving complex issues incrementally via multiple steps.

  - **Challenge 2: Adaptation of Different MathAgents**
     The paper implements two versions of MathAgents aligned with LLMs (MathAgent-M) and human problem-solving methods (MathAgent-H). MathAgent-M is adapted to LLMs, while MathAgent-H aligns with human reasoning styles. Both define logical forms and inherent relations in different grains and orientations, and jointly with an LLM core, enhance the ability to solve mathematical problems.

#### Implementation and Deployment
Experiments on the MiniF2F and MATH datasets showcased the effectiveness of the proposed MathAgents, achieving significant performance improvements over GPT-4, including an average enhancement of 9.26% on the MATH dataset and 12.3% on the miniF2F dataset. MathAgent-H performed better than MathAgent-M, especially on the easier path, indicating that an agent leaning more towards human problem-solving methods can outperform LLMs across all subjects. MathAgent-M, however, showed a decline in subjects like Algebra and PreAlgebra, speculated to be due to the decomposition and modeling of the reasoning process magnifying the LLMs' inherent weakness in calculations.

#### Summary
The paper suggests enhancing LLMs' ability to solve complex mathematical problems through the MathAgent framework, namely Planner-Reasoner-Executor-Reflector (PRER). By breaking down the problems into phases and simulating human-like problem-solving processes, MathAgents significantly improve solving capabilities on challenging mathematical datasets, particularly in areas demanding higher estimation and synthesis skills.