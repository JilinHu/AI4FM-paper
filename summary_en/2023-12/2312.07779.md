#### Background
- **Background**
Researchers examined how LLMs generalize from abstract declarative statements in their training data, such as forecasting weather reports for London in 2050 based on scientific papers about climate change, to determine if LLMs merely match the statistics of previous training or can incorporate new declarative facts.
  
- **Existing Work**
Previous studies indicated LLMs can generalize from declarative statements not present in context but hadn't tested this where such statements conflict with statistical patterns, such as predictions based on climate change research versus past weather reports.

#### Core Contributions
  - **Introduced a new research angle**
    - **Challenge 1: Conflict between declarative statements and procedural information**
Research comprised three simplified tasks to study the counterfactual effect of declarative statements on generalization. For instance, in one task, the model was trained on chat interactions where an AI always refuses medical advice and was tested on the effect of also training on declarative statements suggesting giving medical advice under some conditions.
    
    - **Challenge 2: Impact of model size on generalization**
The second task was designed to assess the impact of model size on generalization. It involved predicting demographic features where statistical patterns and declarative information conflicted. Results showcased that declarative statements have a surprisingly minimal impact on model probabilities, even as model size increases from 330 million to 175 billion parameters.

#### Implementation and Deployment
Across the tasks, it was found that declarative information has a subtle but systematic effect on LLM generalization, being statistically significant but small in absolute terms. Ablation studies indicated that this effect is not due to simple keyword matching from the prompt to memorized declarative facts. Regarding model size, smaller models with only 330M parameters systematically responded to declarative information. The increase of influence with larger models was notably smaller than increases seen in other practical LLM benchmarks.

#### Summary
The paper investigates how models generalize when declarative statements in training data conflict with statistical patterns or procedural examples. The findings have important implications for AI safety (regarding the “treacherous turn”) and fairness.