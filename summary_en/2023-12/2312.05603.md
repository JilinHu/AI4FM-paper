#### Background
- **Background**
Semantic Textual Similarity (STS) assessment is a crucial task in natural language processing. Prior approaches to STS are largely dependent on unsupervised methods or training signals only partially related to textual similarity due to a lack of large-scale labeled data.

- **Existing Work**
Current methods for harnessing STS fail to address the shortage of supervised training data effectively. Even though Large Language Models (LLMs) like GPT can provide accurate semantic similarity scores for sentence pairs, leveraging them for every new pair comes at high costs and entails slower processing speeds.

#### Core Contributions
  - **Introduced Sim-GPT framework**
      - **Challenge 1: Reliably generating STS training data using LLMs while mitigating costs**
      The proposed Sim-GPT framework employs GPT-4 to generate a large amount of STS-labeled data at one time. A model is then trained on this dataset using BERT or RoBERTa as a backbone, offering significant long-term savings in terms of cost and speed in contrast to repeatedly invoking LLMs for each sentence pair.

  - **Challenge 2: Enhancing model performance on STS tasks**
      Sim-GPT trained on a dataset synthesized from GPT-4 (646K, with 371K from it) and NLI data (275K) exhibits state-of-the-art (SOTA) results on seven widely-used STS benchmarks, achieving a +0.99 improvement over supervised-SimCSE (Gao et al. 2021) and +0.42 over the most recent SOTA PromCSE (Jiang et al. 2022) models. Sim-GPT demonstrates the potential of using LLMs to offer powerful training signals for the STS task and shows advantages in both cost efficiency and speed.

#### Implementation and Deployment
In terms of implementation, the Sim-GPT leverages LLMs to generate annotated data of high quality, addressing the lack of direct supervised signals for training STS models. The dataset comprising 371K STS annotated examples from GPT-4 will be released to the community to foster further advancements. The trained model showcases significant performance improvements on STS benchmarks, outperforming existing models like Supervised-SimCSE and current SOTA PromCSE. Sim-GPT exhibits the capability of LLMs to provide robust training signals for STS tasks while being more cost and time-efficient compared to repeatedly using LLM for inference.

#### Summary
Sim-GPT is a framework that uses data labeling by GPT-4 to effectively train STS models. It incurs a one-time cost for data generation, is faster, and the model outperforms on multiple STS benchmarks.