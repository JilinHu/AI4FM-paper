#### Background
- **Background**       
Existing open-source Large Language Models (LLMs) still face limitations in handling long-context tasks. Most of these models, like long-context chat models, display poor accuracy in long-text handling due to a context window that does not exceed 4k.

- **Existing Work**       
Open-source models such as Llama-yarn and LongAlpaca, despite their claims to handle text up to 32k, still show unsatisfactory accuracy in long text chat situations. More powerful models like GPT-4-128K and Claude-2-200K are not open-sourced due to the high training costs and the need to keep the training data confidential. Attempts to address this issue, such as linear positional interpolation (PI), "NTK-aware" interpolation, and Yarn, involve interpolating ROPE and then fine-tuning the model with a small amount of long-text data to expand the context window.

#### Core Contributions
  - **Proposed using the original text paraphrase task to improve context windows**
    - **Challenge 1: Long Text is not effectively utilized**
      Current models lose grasp of important middle text information as they tend to focus only on information at the ends due to two language characteristics: "closer is more relevant" and "emphasis at the beginning and end." The paper suggests that to train a model to handle long contexts, "effective" rather than just long text data is needed, such as "W-shaped data" to tackle the "lost in the middle" issue.

    - **Challenge 2: Existing models are not accurate in long-text scenarios**
      The low accuracy of previous models in long-text scenarios is due to insufficient effectiveness of the output. The paper proposes that by using effective instruction fine-tuning data, a LLM for long text question answering with high accuracy can be trained using the Qlora method.

#### Implementation and Deployment
The paper successfully extended the context window of the Qwen-14B-chat model from 2k to 32k and achieved extremely high accuracy in multi-document QA tasks, outperforming all existing open-source models of the same scale. The model and the training data have been open-sourced on HuggingFace and WiseModel.

#### Summary
The paper presents a low-cost, effective approach to extending the capability of existing language models to handle long texts, significantly improving accuracy in long-context question answering by theoretical demonstration and experimental validation.