#### Background
- **Background**       
This paper focuses on the in-depth exploration of the robustness of Large Language Models (LLMs) to majority label bias in In-Context Learning (ICL) capabilities, particularly within text classification tasks. Previous research has shown that LLMs in ICL tasks are susceptible to sample selection biases, such as majority label bias and recency bias, leading to a propensity of the model to predict those labels that frequently appear in the context examples.

- **Existing Work**
Existing work did not thoroughly test the impact of different proportions of class distributions in context prompts on model performance in text classification tasks. There was also no comprehensive exploration of the robustness boundaries of the model's performance under majority class bias.

#### Core Contributions
- **Presented a comprehensive study on the robustness boundaries to majority label bias**
  - **Challenge 1: Assessing the impact of majority label bias on ICL**
      The research focused on the ICL performance of different LLMs across various proportions of class label context examples. The challenge is to determine whether these models can maintain stable performance when faced with variations in label distributions.
      
  - **Challenge 2: Understanding the effect of model size and instructional prompts on robustness**
      The study proposed ablation experiments that include comparisons of model sizes and the impacts of adding informative instructional prompts on the model's resistance to majority label bias.

#### Implementation and Deployment
The study used publicly available open-source models for empirical research to ensure transparency and reproducibility. Experiments utilized BoolQ and RTE-1/2/3 datasets with binary labels and a real-world multi-class dataset COVID-5G Conspiracy to artificially create prompts with varying degrees of majority label bias. The paper mentions that certain LLMs demonstrated approximately 90% robustness to majority label bias in ICL. The experiment varied the proportion of Yes/No labels in the context examples (from 0% Yes - 100% No to 100% Yes - 0% No with 10% increments) and evaluated the models using weighted F1 scores, repeating this variation process to assess the model's robustness to majority label distribution variations.

#### Summary
The article conducts a comprehensive study on the robustness of LLMs when faced with majority label bias in ICL, finding significant stability in certain models in handling such bias.