#### Background
- **Background**
    The paper investigates the susceptibility of Large Language Models (LLMs) in persuasive conversations involving misinformation, particularly focusing on changes in the models' beliefs in response to factual questions. Previous studies have mainly focused on single-turn dialogues when studying model susceptibility but not on the belief changes that can occur in multi-turn persuasive conversations.

- **Existing Work**
    Existing work failed to address this issue because it mostly concentrated on single responses, neglecting how beliefs could evolve through persuasive multi-turn dialogue, which is a common phenomenon in real-world communications.

#### Core Contributions
  - **Introducing the Farm dataset and test framework**
    - **Challenge 1: Tracking belief changes in LLMs during persuasive dialogue**
      The researchers develop a Farm dataset, which includes factual questions paired with systematically generated persuasive misinformation, and they devise a testing framework to track the belief changes in LLMs during persuasive dialogues.
    - **Challenge 2: Quantifying and assessing belief changes in LLMs**
      Experiments quantify LLMs' belief changes by comparing the responses in initial belief checks with those in final belief checks, revealing that LLMs could be easily influenced by various persuasive strategies when faced with misinformation.

#### Implementation and Deployment
Using the test framework, the experiments demonstrate that LLMs' correct beliefs on factual knowledge can be easily manipulated with different persuasive strategies. For instance, ChatGPT's beliefs were altered by 49.8%, and GPT-4's by 20.5%. It also highlights that different models may exhibit significant performance differences when confronted with the same misinformation.

#### Summary
This paper is the first to thoroughly investigate the robustness of LLMs against factual misinformation in a persuasive conversation setting, revealing the susceptibility of LLMs to persuasive misinformation.