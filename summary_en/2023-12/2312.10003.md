#### Background
- **Background**
The paper addresses the issue of answering complex natural language questions, noting that it often requires multi-step reasoning and the integration of external information. Current systems that combine knowledge retrieval with large language models (LLMs) encounter various failure cases and cannot be directly trained end-to-end to rectify these because the interaction with external knowledge is non-differentiable.

- **Existing Work**
The present systems are not able to self-improve in addressing failure cases of interaction with external knowledge due to the non-differentiable nature of such interactions.

#### Core Contributions
- **Introduced a ReAct-style LLM agent with reasoning and external knowledge interaction capabilities** and refined the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. After just two iterations of the algorithm starting with a prompted large model, the study was able to produce a fine-tuned smaller model achieving comparable performance on challenging compositional question-answering benchmarks with significantly fewer parameters.
  - **Challenge 1: Improving LLM agent reasoning and external knowledge interaction to answer complex natural language questions**
      The authors proposed the ReAct-style LLM agent with reasoning abilities and the capacity to act upon external knowledge, refined through a method similar to ReST, which incorporates AI feedback and growing-batch reinforcement learning, creating a different interface and interaction mode from traditional methods.

  - **Challenge 2: Increasing precision and reducing model size**
      The agent applies iteratively improved algorithms and self-distillation methods that significantly reduce the needed parameters while maintaining performance, allowing smaller models to achieve comparable results to larger models on complex problems.

#### Implementation and Deployment
The study utilized different sizes of PaLM 2 “base” models and an internal Google Q&A API to run the Search Agent. Four datasets were used to provide initial questions for Search Agent trajectories, and the completion of the Search Agent trajectory was split into reasoning steps to build a fine-tuning mixture. An instruction-tuned PaLM 2-L was employed for the ranking "reward" model, and the LLM made selections based on instructions to choose the best sample for fine-tuning. The self-improvement algorithm involved progressively training an improved model and verifying its enhanced performance, complemented by training smaller models for self-distillation. Evaluation was primarily conducted using the Bamboogle dataset and ensured to prevent overfitting by introducing the BamTwoogle dataset as an analog to a test set, as well as verifying correctness through both manual judgment and LLM-based auto-evaluation aligned with human ratings.

#### Summary
This paper outlines creating an LLM agent capable of reasoning and interacting with external knowledge, along with a self-improvement algorithm that enables smaller models to perform comparably to large models in compositional question-answering benchmarks. The proposed method not only improves reasoning capabilities but also significantly reduces the required parameter count of the models.