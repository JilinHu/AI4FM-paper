#### Background
- **Background**       
The paper acknowledges the constant need for educators to develop and maintain effective up-to-date assessments in computing education. While there is growing research on using Large Language Models (LLMs) for generation and engagement with coding exercises, the use of LLMs for generating programming Multiple Choice Questions (MCQs) has not been extensively explored. This paper analyzed GPT-4's ability to produce MCQs aligned with specific learning objectives (LOs) from higher education Python programming classes and developed an LLM-powered (GPT-4) system for MCQ generation from high-level course context and module-level LOs.

- **Existing Work**  
Past research has mainly focused on using smaller LLMs models to deal with individual steps of MCQ processing, or on the LLM production of reading comprehension-like questions without a framework for large-scale evaluation. Although there were attempts to use GPT-3 and GPT-4 to generate MCQs, these studies did not undertake the task of automatically generating new higher education programming class MCQs.

#### Core Contributions
- **A GPT-4 Based Automated MCQ Generation System**
    - **Challenge 1: Alignment with Specific LOs**
        The paper uses a BERT classifier to predict the Bloom's Taxonomy level of the provided LO, which in turn determines the appropriate type of questions, ensuring that the generated MCQs are well-aligned with the LO. 
    
    - **Challenge 2: Comparable Quality to Human-crafted MCQs**
        The study investigated how GPT-4's generated quality compares to MCQs designed by experienced course designers, through various assessment criteria including the presence of multiple correct options and the quality of distractors. The evaluation showed that automatically generated MCQs were better aligned with the LOs than human-crafted ones, with 81.7% of the automatically generated MCQs passing all evaluation criteria, although there were some gaps in quality.

#### Implementation and Deployment
For evaluating the automatically generated MCQs, this study generated 651 automatically generated MCQs corresponding to 246 LOs. Via a set of six criteria rubrics developed, seven student annotators and six CS instructor annotators (all authors of this paper) scored these MCQs along with 449 human-crafted MCQs, resulting in the distribution of 651 automatically generated MCQs. The automatically generated MCQs were compared to the human-crafted ones in regards to the six categories from the developed rubric using Fisherâ€™s exact test. The results indicated that automatically generated MCQs do not differ from human-crafted MCQs in terms of providing sufficient information in clear language (RQ1.i), and containing syntactically and logically correct code (RQ1.iv). However, when it comes to having a single correct answer (RQ1.ii), and high-quality distractors (RQ1.iii), the automatically generated MCQs are somewhat lacking.

#### Summary
The main contribution of this paper is the development of an automated MCQ generation system based on GPT-4, which, through a specialized flexible architecture and precise LO alignment mechanism, successfully generates MCQs consistent with higher education Python courses LOs. The findings show that the automatically generated MCQs maintain good alignment with the LOs and are close in quality to human-generated MCQs, but fall short on having a single correct answer and high-quality distractors, suggesting future work should focus on alleviating these issues.