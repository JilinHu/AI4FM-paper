#### Background
- **Background**
Conventional text-to-motion generation models are mainly trained on limited text-motion pairs and struggle to generalize to open-world scenarios.

- **Existing Work**
Current methods rely on the pre-trained CLIP model to align motion and text spaces, but they are still constrained in generating motions due to the differences between natural languages and motion descriptions and lack of temporal priors in CLIP.

#### Core Contributions
  - **Challenge 1: Bridging the Gap Between Natural Languages and Pose Descriptions**
      The motion planner translates complex natural language descriptions into posture scripts, leveraging motion commonsense in LLMs, effectively addressing out-of-distribution challenges.

  - **Challenge 2: Achieving Script-to-Posture Generation**
      Posture generation is accomplished using a smaller labeled dataset and a diffusion-based posture-diffuser model which understands the connections between structured pose descriptions and body parts. A posture planning module selects key poses for consistency and semantic alignment.

#### Implementation and Deployment
Experimental results on various datasets demonstrate the superiority of PRO-Motion over state-of-the-art approaches and its capability to generate diverse and realistic motions from complex open-world prompts, showcasing its practical effectiveness for text-to-motion generation in open-world scenarios.

#### Summary
The researchers introduced a new framework named PRO-Motion to overcome limitations of traditional text-to-motion generation methods, successfully generating more diverse and realistic motions in open-world scenarios.