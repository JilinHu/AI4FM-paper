#### Background
- **Background**
Existing methods for evaluating LLMs using pre-existing Knowledge Graphs (KG) typically overlook the structure of the graph, leading to arbitrary selections of which parts to evaluate.

- **Existing Work**
Challenges faced by these methods include information that has been trained or evolved over time, resulting in data leakage, and the issue of dynamic knowledge, where information constantly updates while datasets remain old and fixed.

#### Core Contributions
- **Introduced KGLens methodology**
  - **Challenge 1: Generating natural questions**
      Current methods struggle to transform KG edges into natural questions and assess LLM responses due to a lack of context and the ambiguity of entities and predicate diversity. KGLens uses a graph-guided QG strategy to enhance question naturalness and reduce ambiguity by including entity aliases for added context to reduce ambiguity.
  
  - **Challenge 2: Utilizing a parameterized KG**
      The parameterized KG (PKG) allows efficient evaluation of the knowledge reliability of LLMs, taking advantage of the structural convenience of KG for more efficient reliability evaluations during information updates.
  - ...

#### Implementation and Deployment
KGLens can assess not only the overall performance of LLMs but also provide analyses of topics, temporal aspects, and relationships. The experiments show that KGLens can evaluate the overall performance of LLMs, and also offer topic, temporal, and relationship analyses, demonstrating KGLens' adaptability and customizability, highlighting its ability to focus evaluations based on specific criteria.

#### Summary
The paper introduces KGLens, a new framework for assessing factual knowledge in LLMs. KGLens generates natural language questions using the KG structure for evaluations and is aided by a parameterized KG and a graph-guided QG strategy to improve the quality of natural question generation and the efficiency of the assessment process.