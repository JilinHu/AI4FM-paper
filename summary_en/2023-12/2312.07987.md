#### Background
- **Background**
The paper acknowledges that Large Language Models (LLMs) have shown impressive capabilities and versatility. However, training massive Transformer models requires substantial computational power and memory, which remains inaccessible for most researchers, academic institutions, and even companies.
- **Existing Work**
Existing Transformer models use a multi-head self-attention (MHA) mechanism that has a computational complexity proportional to the square of the sequence length. This leads to a dramatic increase in computation and storage requirements as the scale of models grows, and optimizing this for the numerous LLMs remains a critical open research issue.

#### Core Contributions
  - **Introduced a method called SwitchHead**
      - **Challenge 1: Reducing resource consumption without compromising performance**
          The presented SwitchHead approach selectively uses attention heads to reduce computation and memory consumption while maintaining the essential properties of attention.
      - **Challenge 2: Achieving resource savings while maintaining model performance**
          Unlike standard MoE methods, no regularization is required for SwitchHead, which, in comparison with the standard Transformer, provides good predictive performance with significantly lower numbers of heads H, ultimately reducing resource usage. Moreover, the method is not dependent on the specific implementation of attention, facilitating experimentation and research.

#### Implementation and Deployment
The article discusses in detail the compute and memory usage of different attention variants. Preliminary experiments using a non-competitive selection function verified the feasibility of the SwitchHead method for language modeling on WikiText-103. This method performs conditional computation on the projections independently for the source side (K and V) and destination side (Q and output), avoiding conditional computation involving the attention matrix itself. Maintaining a single copy of Q and K projections and reusing them for all experts is found beneficial. The experimental results presented in the article highlight SwitchHead's good predictive performance in typical configurations and significantly lower resource utilization compared to the standard Transformer model, proving the method's improved computational efficiency.

#### Summary
SwitchHead is a novel approach that optimizes resource usage in the multi-head self-attention structure, resulting in reduced resource consumption while maintaining model performance. The method has practical application potential, especially for researchers and institutions with limited resources.