#### Background
- **Background**
Large language models (LLMs) have shown impressive capabilities in various tasks, yet they struggle with mathematical reasoning. Current efforts to optimize Chain-of-Thought (CoT) prompts and fine-tune LLMs have not fully explored the potential of few-shot learning to improve LLM math reasoning capabilities.

- **Existing Work**
Existing efforts focus on optimizing CoT prompts and fine-tuning LLMs under the zero-shot setting, without sufficiently exploring the potential of few-shot learning in improving mathematical reasoning. Additionally, current methods for compressing prompts or selecting useful examples have limited performance, especially in the context of math reasoning tasks.

#### Core Contributions
- **Introduced CoT-Max**
  - **Challenge 1: Selection of useful examples and limited number of examples due to restricted context window length**
    CoT-Max introduces a coarse-to-fine pruner as a plug-and-play module for LLMs, which first identifies crucial CoT examples from a large batch, and further prunes unimportant tokens. It collects a math reasoning dataset with diverse difficulty, introduces a reward to measure the input's effectiveness for math reasoning and token length constraints, and proposes a novel training approach with reinforcement learning.

  - **Challenge 2: Extending the context window requires expensive LLM tuning and increases inference overhead; existing compression methods underperform in math reasoning tasks**
    CoT-Max optimizes the input context through a pruning method that significantly enhances math reasoning capabilities without the need for additional tuning or increased inference costs, avoiding added inference overhead.

#### Implementation and Deployment
Extensive experiments on various LLMs and five mathematical datasets demonstrate the effectiveness of CoT-Max. It significantly boosts the reasoning capabilities of LLMs, establishing a new prompting-based benchmark for math reasoning accuracy, achieving 2.13-4.55% absolute improvements without any fine-tuning or additional inference costs. Notably, LLaMA2-70B with CoT-Max outperforms a broad range of larger LLMs, including surpassing GPT-3.5 by 2.5% on GSM8K. CoT-Max also significantly outperforms retrieval and prompt compression baselines in example selection and identifying crucial tokens.

#### Summary
This paper presents CoT-Max, a method that enhances LLMs' mathematical reasoning capabilities using a coarse-to-fine pruning technique, effectively improving the effects of few-shot learning in math reasoning tasks.