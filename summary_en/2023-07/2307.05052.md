#### Background
- **Background**
The paper discusses the in-context learning (ICL) capabilities of large language models (LLMs) in NLP tasks. ICL requires only a few input-label pairs for demonstration and does not need fine-tuning the model parameters. However, understanding which part of the demonstrations influences prediction when presented to the model remains an open research question. Past work has provided mixed results, such as findings that ground truth labels have little impact on ICL performance, while the order of examples does have a strong influence and only larger-scale LLMs have been observed to learn from flipped input-label mappings.

- **Existing Work**
Existing research has not fully explained the effects of different parts of ICL demonstrations, including flipping labels, changing input, and adding complementary explanations. For instance, LLMs at smaller scales like all GPT-3 models cannot override pretraining knowledge with in-context demonstrations, meaning LLMs do not flip their predictions when the truth labels in the demonstrations are flipped. However, larger models like InstructGPT and PaLM-540B show the emergent ability to override prior knowledge in the same setting. Additionally, the effect of input distribution is under-researched, where some change the input to random words, and others do not investigate input distribution at all.

#### Core Contributions
- **Proposed a study method using contrastive demonstrations and saliency maps**
    - **Challenge 1: Understanding the contribution of different parts of the demonstrations to ICL predictions**
        This study employed qualitative and quantitative analyses of contrastive examples built in different ways to understand these parts' contributions to predictions. Demonstrations were created by flipping labels, neutralizing inputs, and adding complementary explanations, followed by contrasting their saliency maps with each other. The study found that true labels in demonstrations became less salient after label flipping in the context of sentiment classification tasks.

    - **Challenge 2: Quantifying the impact of input distribution and complementary explanations**
        The study edited different components of input text finely to correspond to task-specific purposes. For sentiment analysis tasks, the study adjusted sentiment-indicative terms in the input text of demonstrations to neutral terms, finding that such input perturbation (neutralization) doesn't impact as much as changing the truth labels. Moreover, it was found that complementary explanations might not necessarily benefit sentiment analysis tasks though their saliency seems as prominent as the tokens from the original input.

#### Implementation and Deployment
The research adopted three methods for constructing contrastive demonstrations and used them alongside saliency maps to understand the impact of different demonstration parts on the model's predictions. Results showed reduced saliency of true labels after flipping and a minor effect from neutralizing sentiment words in the input text on model predictions. This suggests that models might rely on pretrained knowledge to make reasonable predictions. Complementary explanations, despite being salient, did not significantly enhance the sentiment analysis task. These findings caution us to carefully generate and evaluate complementary explanations and consider whether they genuinely benefit the target task when seeking to enhance ICL performance.

#### Summary
This study analyzed the internal mechanisms of ICL in LLMs using contrastive demonstrations and saliency map analysis, revealing the differential impacts of label flipping, input changes, and complementary explanations on predictions, providing insights for practitioners on curating demonstrations.