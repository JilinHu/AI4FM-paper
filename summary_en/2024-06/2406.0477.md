#### Background
- **Background**       
The paper introduces WILDBENCH, a benchmark for evaluating Large Language Models (LLMs) using challenging tasks from real-user scenarios to test and differentiate the performance of various LLMs.
- **Existing Work**
Existing works lack a benchmark that reflects real user use cases and fails to sufficiently discriminate capabilities in long contexts and complex tasks.

#### Core Contributions
  - **Introduced an evaluation benchmark: WILDBENCH**
      - **Challenge 1: How to capture and assess the diverse requirements for LLMs in the real world?**
      By employing real user-chatbot conversation datasets from WildChat, implementing basic filtering, difficulty annotations, and human review to select tasks that represent real-world use cases.
      
      - **Challenge 2: How to create an automated evaluation process that is both interpretable and standard?**
      By generating a checklist for each test query in WILDBENCH, consisting of 5-10 clear, verifiable questions. Checklists are generated by LLMs such as GPT-4-Turbo and Claude-3-Opus to mitigate the bias of using a single LLM as the evaluator.

#### Implementation and Deployment
The paper proposes two automated evaluation metrics: WILDBENCH-Score and WILDBENCH-Reward. These metrics have shown high correlation with human judgment (Pearson correlation of 0.98 and 0.95 respectively), surpassing other benchmarks. Additionally, by incorporating structured checklists and pairwise evaluation, the paper introduces a method to reduce length bias in evaluations.

#### Summary
WILDBENCH provides an evaluation framework that incorporates real user task challenges, automated indicators, and interpretive checklists, enabling more accurate assessments of Large Language Models' performance in complex tasks.