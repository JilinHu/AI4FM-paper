#### Background
- **Background**
The paper addresses the underexplored foundational ability of comprehending long multimodal content within multimodal large language models (MLLMs), particularly noting deficiencies in vision-centric evaluation metrics.

- **Existing Work**
Existing MLLMs exhibit considerable progress in multimodal tasks yet struggle with long multimodal contexts due to context window size limitations. Moreover, the absence of appropriate evaluation benchmarks hinders further development in long-context multimodal understanding.

#### Core Contributions
- **Introducing the Needle In A Multimodal Haystack (MM-NIAH) benchmark**
    - **Challenge 1: Lack of Evaluations for Long Multimodal Document Comprehension**
        As MLLMs rapidly advance, there remains a gap in evaluating these models' ability to understand long multimodal content essential for real-world applications. MM-NIAH addresses this gap by providing the first systematic evaluation for this purpose, incorporating tasks such as multimodal retrieval, counting, and reasoning, focused on different key information within the documents.

    - **Challenge 2: Constructing a Benchmark for Long Multimodal Document Comprehension**
        MM-NIAH, the first of its kind, is specially designed to systematically evaluate MLLMs' comprehension capabilities for long multimodal documents. It involves integrating diverse interleaved image-text documents into one extensive context and embedding essential 'needles' of information within the text or images. Outlining three types of tasks—retrieval, counting, and reasoning—MM-NIAH demands models to extract and synthesize scattered cues across the document. The research indicates the models struggle with image-based prompts, have no advantage when pre-trained with image-text interleaved data over image-text pair data, and cannot maintain the long-context ability of underlying LLMs. Furthermore, while Retrieval-Augmented Generation (RAG) boosts performance in text-based tasks, it falls short for image-based prompts within MM-NIAH.
    
#### Implementation and Deployment
Experiments on MM-NIAH to assess both open-source and closed-source MLLMs reveal a shared difficulty in comprehending long multimodal documents. Findings show that models are less effective in image-related tasks, MLLMs pre-trained with image-text interleaved data did not surpass those pre-trained only with image-text pairs on MM-NIAH, and the models failed to retain the long-context capacity of base LLMs. RAG's improvement on text needle tasks did not translate effectively to image needle tasks, highlighting the persistent challenge in long multimodal document comprehension.

#### Summary
The presented MM-NIAH benchmark is a novel evaluation platform for advancing MLLM performance in comprehending long multimodal documents. By exposing limitations and challenges of current MLLMs, the paper provides an instrumental platform for further research in long multimodal document comprehension.