#### Background
- **Background**
The paper discusses Large Language Models (LLMs) and their performances on temporal reasoning tasks. These models have shown impressive reasoning capabilities but are still prone to errors, especially in tasks involving complex temporal logic.
- **Existing Work**
Existing research on LLM performance in temporal reasoning has relied on diverse datasets and benchmarks. However, studies often use real-world data that LLMs may have seen during pre-training or use anonymization techniques that can inadvertently cause factual inconsistencies.

#### Core Contributions
  - **Introduced the Test of Time (ToT) benchmark**
      - **Challenge 1: How to assess LLMs' abilities in temporal reasoning?**
      This study introduces specially designed synthetic datasets aimed at evaluating temporal reasoning abilities in a more comprehensive and controlled way. ToT benchmarks focus on capturing and assessing the two essential skills of temporal reasoning: understanding the semantics and logic of time, and executing temporal arithmetic operations.

      - **Challenge 2: Preventing LLMs from simplifying reasoning tasks by relying on vague knowledge**
      ToT measures the two fundamental skills of temporal reasoning using synthetic and crowdsourced tasks, helping to independently assess the ability to understand temporal semantics and logic (ToT-Semantix), as well as to handle calculations involving time points and durations (ToT-Arithmetic). The design of these tasks ensures that LLMs cannot rely on prior knowledge to take shortcuts and must genuinely reason through the presented facts.

#### Implementation and Deployment
The ToT benchmark has provided valuable insights into the strengths and weaknesses of LLMs in temporal reasoning tasks. Compared to existing benchmarks that rely on knowledge graphs, ToT offers a more rigorous evaluation method through a wider variety of graph structures and precisely controlled data types. The experimental results confirm that evaluations against ToT cannot use the LLMs' latent knowledge as shortcuts; instead, they must truly comprehend the facts presented, allowing for a more accurate assessment of temporal reasoning abilities.

#### Summary
This paper introduces a novel benchmark, ToT, which comprehensively evaluates LLMs' temporal reasoning abilities in various scenarios using synthetic datasets and crowdsourced tasks, and exposes the advantages and shortcomings of these models in temporal reasoning.