#### Background
- **Background**
The paper discusses the challenges Large Language Models (LLMs) face in mathematical reasoning, requiring extensive and precise reasoning chains for accuracy. Ensuring each step's correctness is crucial, and the paper aims to improve LLMs’ robustness and factuality through human feedback learning.
- **Existing Work**
Existing work shows limitations of DPO in long-chain mathematical reasoning. Models applying DPO struggle to identify detailed errors in incorrect answers due to lack of granular process supervision.

#### Core Contributions
  - **Introduced Step-DPO**
    - **Challenge 1: Optimizing accuracy for long-chain reasoning**
      Step-DPO treats individual reasoning steps as units for preference optimization, not evaluating answers in a holistic manner. It finds that self-generated data is more effective than that from humans or GPT-4 because of its out-of-distribution nature. The research suggests that as few as 10K preference pairs and fewer than 500 training steps can improve accuracy nearly by 3% on MATH datasets for models with over 70 billion parameters.

    - **Challenge 2: Creating high-quality datasets**
      A data construction pipeline for Step-DPO is developed, enabling the creation of a high-quality dataset with 10K step-wise preference pairs—essential for model performance and adaptability.

#### Implementation and Deployment
The paper's method, Step-DRO, results in Qwen2-72B-Instruct achieving 70.8% and 94.0% accuracy on MATH and GSM8K test sets respectively, surpassing several closed-source models like GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro. The code, data, and models are available in their public GitHub repository. This research proves that targeted improvements such as Step-DPO can significantly enhance the performance of LLMs in critical tasks like mathematical reasoning, even outperforming the current state-of-the-art closed systems.

#### Summary
The paper introduces a new optimization method, Step-DPO, enhancing LLMs' accuracy and robustness in long-chain mathematical reasoning by optimizing individual reasoning steps rather than evaluating answers holistically.