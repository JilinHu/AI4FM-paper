#### Background
- **Background**       
Large language models (LLMs) struggle to provide up-to-date information due to one-time training and the evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, these approaches have difficulties extracting stored knowledge. The paper is inspired by the remarkable success of the Feynman Technique in promoting effective human learning and introduces the SELF-TUNING framework.

- **Existing Work**
The standard method for injecting new knowledge into LLMs is through continued pre-training on raw corpora with new knowledge. This direct method faces challenges in effectively enabling LLMs to extract the acquired knowledge during inference. Instruction tuning on QA data post pre-training is also extensively employed but has limited effectiveness.

#### Core Contributions
- **SELF-TUNING**
  - **Challenge 1: Quickly Outdated Knowledge**
    SELF-TUNING is designed to overcome the rapid obsolescence of stored knowledge in LLMs by improving an LLM's ability to effectively acquire new knowledge from raw documents through self-teaching. It involves a strategy that augments documents with a set of knowledge-intensive tasks created in a self-supervised manner.

  - **Challenge 2: Limited Knowledge Extraction and Application**
    Addressing the issue of the constrained knowledge retrieved from LLMs, SELF-TUNING incorporates strategies for memorization, comprehension, and self-reflection, based on the Feynman Technique. Moreover, it introduces the Wiki-Newpages-2023-QA datasets for a detailed analysis of LLM knowledge acquisition in terms of memorization, extraction, and reasoning.

#### Implementation and Deployment
Extensive experimental results on the LLAMA2 family of models show that SELF-TUNING consistently performs superiorly across all knowledge acquisition tasks and excels at preserving previously acquired knowledge. Furthermore, SELF-TUNING significantly outperforms all other methods on knowledge memorization and extraction tasks and maintains high accuracy on reasoning tasks while the comparative methods fluctuate in performance across different scenarios.

#### Summary
The paper introduces SELF-TUNING, a framework aimed at improving LLMs' knowledge acquisition capability via self-teaching and validates its effectiveness on crucial knowledge acquisition tasks using the Wiki-Newpages-QA datasets.