#### Background
- **Background**
The paper discusses uncertainty quantification in large language models (LLMs) to identify when there is substantial uncertainty in the models' responses. It addresses both epistemic uncertainty, which stems from a lack of knowledge, and aleatoric uncertainty, caused by inherent randomness such as multiple valid answers.

- **Existing Work**
Existing methods primarily work in scenarios where there is a single correct response, aiming to detect dominant or semantically identical responses, indicating little uncertainty. However, this approach falls short when multiple correct responses exist due to aleatoric uncertainty in the question itself.


#### Core Contributions
- **Introduced an Information-Theoretical Measure**
  - **Challenge 1: Separating Epistemic from Aleatoric Uncertainty**
    The paper proposes an iterative prompting process to build a joint distribution of multiple responses generated by LLMs, quantifying the gap between the LLM-derived distribution and the ground truth. This gap is insensitive to aleatoric uncertainty, allowing for the quantification of epistemic uncertainty even with multiple valid responses. The process can significantly detect epistemic uncertainty.

  - **Challenge 2: Computable Lower Bound of Uncertainty**
    Researchers derived a computable lower bound of the uncertainty measure and proposed a finite-sample mutual information (MI) estimator. This estimator suffers only negligible error sometimes, even when LLMs and their joint distributions span potentially infinite supports (all possible strings in a language).

#### Implementation and Deployment
The thesis demonstrates the superiority of the new formulation through a series of experiments. Experiments were conducted on closed-book open-domain question-answering benchmarks such as TriviaQA, AmbigQA, and a dataset synthesized from WordNet, showing that the MI-based hallucination detection method outperforms a naive baseline based on likelihood of response and achieves similar performance to a more advanced baseline based on output entropy.

#### Summary
This paper focuses on the study and introduction of a novel information-theoretical metric to quantify uncertainty in large language models, specifically for the phenomenon of hallucinations during response generation. This research offers new insights and solutions for identifying and addressing hallucinations in LLMs.