#### Background
- **Background**
LLMs have made significant impacts on NLP, particularly in simple reasoning tasks. Chain-of-Thought (CoT) prompting has been a notable advancement, helping LLMs to tackle complex reasoning tasks by generating step-by-step rationales. However, the self-consistency variant of CoT, which improves prediction accuracy through multi-path inference and answer frequency voting, results in increased deployment costs.

- **Existing Work**
Existing work on self-consistency enhances LLM inference capabilities but incurs higher inference costs due to multiple inferences for each prompt, failing to fully exploit LLM's inference potential in single-path scenarios.

#### Core Contributions
- **Introduced Nash CoT**
  - **Challenge 1: Maintaining performance while reducing inference costs**
      This research introduces Nash CoT, which conceptualizes language decoding as a preference consensus game to achieve performance comparable to self-consistency with fewer inference paths. It constructs a bi-player gaming system within each path, striving for Nash Equilibrium and reducing the need for multiple inferences.

  - **Challenge 2: Template-guided output selection**
      To tackle this, the researchers use LLMs to autonomously select contextually relevant templates to guide inference in each path, increasing the likelihood of correct problem-solving. The advent of Preference Equilibrium helps balance template guidance and normal generation, fostering robust context matching for the given questions.

#### Implementation and Deployment
Nash CoT was tested on LLMs like Mistral-Instruct and GLM4 for Arabic reasoning, Commonsense Question Answering, and Symbolic inference tasks, yielding comparable or superior results to self-consistency with fewer inference paths and up to 50% reduced inference costs.

#### Summary
The study proposes a novel Nash CoT approach, which effectively utilizes the concept of Preference Equilibrium to maintain performance while substantially lowering the deployment costs for LLMs by reducing the number of inference paths necessary.