#### Background
- **Background**
This paper addresses a main issue in evaluating reasoning strategies for large language models (LLMs): traditional evaluations only focus on performance metrics and neglect the increase in effectiveness due to additional compute, which can lead to a skewed view of strategy efficiency.

- **Existing Work**
Comparing these strategies fairly and comprehensively has been a challenge due to their varied computational requirements. Strategies like tree of thoughts (ToT) require branching into multiple sequences with self-evaluation, making them more compute-intensive than other strategies, and an evaluation framework that only takes performance metrics into account can miss crucial practical factors such as computational cost.

#### Core Contributions
  - **Introduced a budget-aware evaluation framework**
    - **Challenge 1: How to balance the performance of reasoning strategies with their computational resource consumption?**
      The paper introduces a framework that incorporates compute budget into the performance measurement of different reasoning strategies. This budget-aware comparison provides a more balanced perspective, considering both the quality of the output and the computational resources used. The research found that complex reasoning strategies often don't surpass simpler baselines due to algorithmic ingenuity, but rather due to the allocation of more computational resources. With comparable compute resources, a simple baseline like chain-of-thought reasoning with self-consistency (CoT SC) often achieves the best balance between performance and budget.

    - **Challenge 2: Assessing the performance of self-evaluation strategies across different models and datasets**
      The paper conducts an in-depth investigation into self-evaluation strategies and finds that their performance truly depends on the models and datasets. It further investigates the effect of two specific types of budgets on performance: the answer generation budget and the evaluation budget and finds a strong correlation between success in reasoning strategies that leverage self-evaluation and the calibration via a correctness prediction proxy.

#### Implementation and Deployment
The study conducted a comprehensive evaluation of seven LLM reasoning strategies across five datasets using five models including GPT-4. Results reveal that traditional evaluation metrics often overlook the performance gains obtainable through additional computational resources. This is strongly supported by the fact that the simple CoT SC strategy can match or even surpass more complex strategies in effectiveness. Additionally, the paper presents an ablation study on ToT and Reflexion by segregating the budget into answer generation and evaluation, and proposes a new strategy SC2 to explore self-evaluation capabilities.

#### Summary
The study presents a framework for LLM reasoning strategies evaluation that considers compute budget and demonstrates the ability of simple strategies to outperform complex ones with equal computational resources. By highlighting the importance of self-evaluation, it sets the groundwork for more efficient budget use and the development of more effective reasoning strategies.