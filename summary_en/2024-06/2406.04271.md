#### Background
- **Background**
The paper introduces the performance of Large Language Models (LLMs) such as GPT-4, PaLM, and LLaMA in various reasoning tasks. To enhance their functionality and performance, more effective prompting methods have been proposed. Current prompting methods can be categorized into single-query reasoning and multi-query reasoning; however, they face limitations like lack of universality, generality, and high computational intensity.

- **Existing Work**
Existing work relies on task-specific examples and reasoning structures, ignoring the extraction of universal and high-level thoughts and templates from completed tasks that could improve efficiency and accuracy when solving similar problems.

#### Core Contributions
  - **Introduced Buffer of Thoughts (BoT), a new thought-augmented reasoning framework**
    - **Challenge 1: Enhancing reasoning accuracy, efficiency, and robustness of LLMs**
        BoT introduces a meta-buffer, a lightweight library that houses universal high-level thoughts (thought-templates) distilled from various problem-solving processes which can be shared across tasks. For each problem, a relevant thought-template is retrieved and instantiated with a specific reasoning structure for efficient thought-augmented reasoning, addressing the issue of manually constructing reasoning structures.
        
    - **Challenge 2: Ensuring scalability and stability of the framework**
        BoT also introduces buffer-manager to dynamically update the meta-buffer, effectively enhancing its capacity as more tasks are solved, thus ensuring the framework's scalability and stability.

#### Implementation and Deployment
BoT has shown significant improvements in performance through extensive experiments across 10 challenging reasoning-intensive tasks. Compared to previous state-of-the-art methods, it has achieved an increase of 11% in Game of 24, 20% in Geometric Shapes, and 51% in Checkmate-in-One while only requiring on average 12% of the cost of multi-query prompting methods. Notably, the Llama3-8B + BoT model has the potential to surpass the performance of the Llama3-70B model.

#### Summary
BoT enhances the accuracy, efficiency, and robustness of reasoning in LLMs by providing a meta-buffer to store high-level thought templates. It overcomes the limitations of existing methods and demonstrates significant performance gains.