#### Background
- **Background**
The paper introduces that large language models (LLMs) have been showing impressive capabilities across a variety of natural language processing tasks. However, they have a tendency to hallucinate, generating content that sounds plausible but is factually incorrect and arbitrary. This poses significant risks in high-stakes domains where factuality is critical, making it paramount to reliably detect or mitigate hallucinations.

- **Existing Work**
Previous works approached hallucination detection in LLMs using a multi-sample strategy predicated on the idea that if a model "knows the answer", it would consistently provide the same response to a given prompt. If the model is hallucinating, its responses may vary across generations. However, the substantial increase in computation cost by 5 to 10 times with the existing methods impedes their practical adoption.

#### Core Contributions
- **Introduced Semantic Entropy Probes (SEPs)**
  - **Challenge 1: High computational costs**
      SEPs address the problem of computational expenses as they approximate semantic entropy (SE) from the hidden states of a single generation, avoiding the need for multiple generations. SEPs are simple to train and eliminate the need for sampling multiple models at test time substantially reducing the overhead of semantic uncertainty quantification.
  
  - **Challenge 2: Prediction and generalization regarding hallucinations**
      SEPs are trained to predict semantic entropy instead of directly predicting model accuracy, setting a new cost-efficient benchmark for hallucination detection. The SEPs can predict hallucinations and outperform probes that are trained directly to predict accuracy, showing better generalization to new tasks.

#### Implementation and Deployment
SEPs are able to extract semantic uncertainty information from the hidden states of a single generation from LLMs, indicating that these hidden states do encode such information. Through ablation studies across various models, tasks, layers, and token positions, SEPs show strong performance and provide insights into the inner workings of LLMs. Furthermore, SEPs have demonstrated superiority in generalization and do not require generating multiple samples at test time, establishing a new standard for cost-efficient hallucination detection.

#### Summary
The paper proposes SEPs as a cost-effective and reliable method for detecting hallucinations, capable of capturing semantic uncertainty directly from the hidden states of LLMs with a single model generation.