#### Background
- **Background**
The paper introduces challenges faced in the traditional RAG framework, where retrieval units tend to be short, such as the 100-word passages used by DPR, making the retriever to search through large corpora to find the necessary units, while the reader's task is comparatively lighter.
- **Existing Work**
Existing RAG models rely on retrieving a large number of units coupled with complex re-rankers to perform optimally. Additionally, short retrieval units often lead to semantic incompleteness due to document truncation, restricting the ultimate performance.

#### Core Contributions
  - **Introducing LongRAG framework**
    - **Challenge 1: Short retrieval units leading to suboptimal performance**
        To address the problem of information loss and limitations caused by short retrieval units, LongRAG processes the entire Wikipedia into units longer than 4K tokens, significantly reducing the number of information units from 22 million to 600K and easing the retriever's task while improving retrieval scores.
    - **Challenge 2: Inability of current RAG readers to handle long texts**
        LongRAG provides a solution with its design of a "long retriever" and a "long reader." The long retriever is responsible for identifying coarse relevant information from the corpus, and the top retrieved units are concatenated to serve as a long context for further action. The long reader then extracts answers from the long context, typically about 30K tokens, by prompting existing long-context LLMs like Gemini or GPT-4 with the question.

#### Implementation and Deployment
In experiments on the NQ and HotpotQA datasets, LongRAG reduced the corpus size from 22 million to 600k document units, improving answer recall@1 to 71%. Similarly, it reduced the HotpotQA corpus size from 5 million to 500k, improving recall@2 to 72%. Moreover, LongRAG achieved an EM of 62% on NQ and 64% on HotpotQA, comparable to the best fine-tuned RAG models like Atlas and MDR.

#### Summary
LongRAG is a novel framework for open-domain question-answering tasks that addresses the limitations of traditional RAG by incorporating larger retrieval units and leveraging the capabilities of long-context LLMs. Its approach results in notable improvements in performance through reduced retrieval units and enhanced retriever effectiveness, alongside utilizing long-context LLMs for zero-shot answer extraction.