#### Background
- **Background**
  The paper discusses the challenges of Cross-document event coreference resolution (CDECR), focusing on clustering event mentions across multiple documents that refer to the same real-world events. Existing methods primarily rely on fine-tuning small language models (SLMs) like BERT to address the compatibility among the contexts of event mentions, but due to the complexity and diversity of contexts, these models tend to learn simple co-occurrences instead of genuinely coreference-related terms.
  
- **Existing Work**
  Current work attempts to use Small Language Models (SLMs) to encode event mentions and obtain their embeddings for supervised coreference resolution. However, these approaches have shortcomings, particularly when dealing with the diversity and complexity of contexts, they are prone to learning pseudo-features. Additionally, Large Language Models (LLMs) have shown excellent contextual understanding abilities but struggle to adapt to specific Information Extraction (IE) tasks.

#### Core Contributions
  - **A collaborative approach is proposed**
    - **Challenge 1: Contextual similarity of event mentions**
      Different events in different documents can be portrayed in a very similar way, especially for events of the same type. The model must grasp comparable coreference evidence from varied contexts and make judgments based on it. The proposed method comprehensively establishes connections between event mentions and their corresponding contextual elements, including contextual words, entity mentions, and other event mentions, by leveraging the intrinsic knowledge and out-of-the-box context comprehension ability of LLMs, effectively addressing the challenge of context similarity and making a significant contribution to performance improvement.
      
    - **Challenge 2: The portrayal difference of the same event across documents**
      Descriptions of the same event can vary significantly across different documents. The paper counters this by designing a two-step workflow using LLMs to summarize event mentions alongside separate generic prompts to guide its comprehension of the context of each mention, as opposed to task-specific in-context learning or fine-tuning. Joint representation learning is employed to integrate the original document and the generated summary into the SLM, enhancing its understanding of event mentions during fine-tuning and enabling it to make coreference judgments based on more focused contexts.

#### Implementation and Deployment
The authors conducted experiments on three CDECR datasets. The collaborative approach showed significant improvements over methods that relied solely on LLMs or SLMs, exhibiting a complementary advantage in performance. Overall, this collaborative approach achieved state-of-the-art results on the ECB+, GVC, and FCC datasets, with increases in CoNLL F1 scores of 1%, 2.7%, and 7%, respectively.

#### Summary
The paper introduces a novel collaborative approach for addressing the task of cross-document event coreference resolution. By combining the universal capabilities of LLMs with task-specific SLMs, the performance of the model was significantly enhanced.