#### Background
- **Background**
    The paper introduces a new benchmark designed to evaluate the capabilities of large language models (LLMs) in retrieval-augmented generation tasks, specifically focusing on four capabilities: noise robustness, negative rejection, information integration, and counterfactual robustness. The authors underline the biases in existing question-answering evaluation formats, which might fail to accurately assess LLMs' abilities to retrieve and apply external information in complex real-world scenarios because of the models' extensive built-in knowledge.

- **Existing Work**
    Although benchmarks have been used in the evaluation of LLMs, existing work has not fully assessed LLMs' multiple abilities in retrieval-augmented generation tasks, nor has it deeply examined the models' performance when faced with noisy information and when processing information from multiple sources.

#### Core Contributions
  - **Introduced a retrieval-augmented generation benchmark based on real news articles**
    - **Challenge 1: Robustness to noisy information**
        The proposed benchmark is capable of evaluating the noise robustness of LLMs by incorporating varying ratios of negative documents (documents that do not contain the answer), using accuracy to measure the impact of noise document ratio on model performance.

    - **Challenge 2: The ability to integrate information from multiple sources**
        The benchmark was specifically designed to assess LLMs' information integration capabilities, where the model needs to combine information from various documents to answer the question, using precise matching to measure the accuracy of the model.

    - **Challenge 3: Rejecting to make incorrect responses**
        The benchmark measures the model's negative rejection capability by requiring the LLM to output specific content indicating the inability to answer questions when provided with only noisy documents.

    - **Challenge 4: Counterfactual robustness against errors in external knowledge**
        The benchmark evaluates the performance of models when confronted with potentially erroneous external knowledge, expecting the model to identify inaccurate information and provide correct answers.

#### Implementation and Deployment
The data construction process for the benchmark included collecting actual news articles, using ChatGPT to generate questions and answers, and retrieving external documents using search engines. The evaluation used two metrics, accuracy and rejection rate, to assess models' performance on noise robustness and information integration capabilities while examining LLMs' negative rejection capability when only noisy information is available. The experimental results showed that model accuracy decreases with increasing noise ratios, revealing the limitations of current LLMs when dealing with complex information environments.

#### Summary
This paper introduces a new benchmark based on real news articles for comprehensive assessment of large language models' capabilities in complex informational environments and illustrates the existing limitations of LLMs through the experimental results.