#### Background
- **Background**
The paper discusses a critical issue in assessing the performance of large language models (LLMs), which is the need for a fair and reliable evaluation method. The emergence of subjective or non-subjective cheating phenomena, like test set leakage and prompt format overfitting, presents significant challenges to reliable LLM evaluation.

- **Existing Work**
Current evaluation frameworks typically depend on Regular Expression (RegEx) for answer extraction, which is not very accurate. LLMs might adjust their responses to comply with specific easily extractable formats by RegEx, which leads to frequent extraction errors. Recent research increasingly focuses on fine-tuning LLMs for evaluation, but these prompt and fine-tuning dataset designs often result in evaluation LLMs with poor generalization, making them difficult to compare with strong LLMs like GPT-4.

#### Core Contributions
  - **Introduced xFinder**
    - **Challenge 1: Increasing the Accuracy of Key Answer Extraction**
        The paper aims to improve extraction accuracy, reduce the reliance of LLMs on specific answer formats, and enhance the reliability of LLM evaluation by optimizing the key answer extraction module. xFinder is designed to align the extraction of key answers from LLM outputs with the key answers expected by humans, especially suitable for tasks with deterministic answers such as multiple-choice questions and math problems.

    - **Challenge 2: Lack of Dedicated Dataset for Key Answer Extraction**
        To fine-tune xFinder effectively, a high-quality dataset, KAF, was provided, which is novel in existing research. The KAF dataset consists of a variety of evaluation tasks, including questions, optional answer ranges, LLMs' responses, and the extracted key answers, covering training, testing, and generalization for assessing xFinder's performance.

#### Implementation and Deployment
xFinder created question-response pairs using different LLMs across 18 typical evaluation task datasets to maintain diversity. Eight types of prompt configurations were designed to induce varied responses from the LLMs and enrich the dataset further. Experimental results demonstrated that xFinder improved the precision of key answer extraction and thereby helped to assess the real capabilities of LLMs, avoiding the inaccuracies in evaluation due to RegEx errors.

#### Summary
The focus of the paper is the introduction of a method called xFinder, which aims to improve the accuracy of extracting key answers from LLM outputs. It addresses gaps not met by existing methods and provides a more reliable approach for evaluating LLMs.