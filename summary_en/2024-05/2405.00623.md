#### Background
- **Background**
The article points out that Large Language Models (LLMs) have started to transform our daily lives, with people making widespread use of these models for tasks like searching for information, writing, and programming. Despite LLMs' ability to generate fluent and seemingly plausible outputs, these outputs are sometimes incorrect, potentially leading to serious consequences due to users' overreliance on their answers.

- **Existing Work**
Although overreliance is an issue, mitigating this phenomenon is challenging, as many solutions, such as explanations, have been found ineffective or could even increase overreliance. The research community has proposed making LLMs express the uncertainty of their output to the end-users as a way to address this, but currently, there is very little understanding of how to effectively express uncertainty in natural language to users.

#### Core Contributions
- **Presented a Study**
  - **Challenge 1: Impact of uncertainty expression on user trust and reliance**
      Through experimental findings, the paper shows that employing first-person expressions of uncertainty (e.g., "I'm not sure, but...") can reduce users' confidence in the system and their tendency to agree with the system's answers, while also increasing user accuracy. The exploratory analysis suggests that this increase can be attributed to a reduction, although not a complete elimination, in users' overreliance on incorrect answers.

  - **Challenge 2: Precise language choice**
      The research indicates that using natural language expressions of uncertainty might be an effective method for reducing overreliance on LLMs, but the specific language used is very important. This underscores the importance of user testing prior to wide-scale deployment of LLMs.

#### Implementation and Deployment
In the experiment, 404 participants answered medical questions and accessed responses from a fictional search engine infused with LLM technology. The research tested how different natural language expressions of uncertainty affect users' reliance on the system, trust, and overall task performance through behavioral and self-reported measures. The results showed that employing first-person expressions can improve user accuracy, indicating a reduction in users' overreliance on incorrect system answers. However, when uncertainty was expressed from a general perspective (e.g., "It is not clear, but..."), these effects were weaker and not statistically significant. These findings highlight the importance of user testing before the wide-scale application of LLMs, and understanding users' reactions to expressions of uncertainty in advance.

#### Summary
The paper, through large-scale experiments, demonstrates that by expressing uncertainty in natural language, LLMs can reduce user overreliance and improve accuracy in task performance. Specifically, first-person expressions have a significant effect on improving user accuracy. Moreover, the research emphasizes the importance of user testing before the practical application of LLMs to adjust the way uncertainty is communicated.