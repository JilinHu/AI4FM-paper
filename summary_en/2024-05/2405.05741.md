#### Background
- **Background**
The paper highlights significant progress of large language models like ChatGPT in natural language understanding (NLU) tasks. Despite such advancements, existing research often overlooks fine-grained explorations, which are crucial for understanding the unique comprehension mechanisms of LLMs, aligning with human cognition, and ultimately enhancing their general NLU capabilities.

- **Existing Work**
Existing work has failed to address the limited focus on surface-level semantic understanding, neglecting the deeper, shared comprehension of semantics in communication which restricts a comprehensive assessment and enhancement of model capabilities.

#### Core Contributions
- **Introduced a Lexical Semantic Comprehension (LeSC) dataset and new evaluation methods**
  - **Challenge 1: Fine-grained Semantic Understanding**
    Existing LLMs perform poorly in understanding uncommon meanings of common words. The study constructs the LeSC dataset, which introduces the first benchmark that includes fine-grained and cross-lingual dimensions, testing the performance of models on this basic lexical-meaning understanding task.

  - **Challenge 2: Cross-lingual Semantic Transfer Capability**
    Most existing models exhibit deficiencies in cross-lingual semantic understanding. Through cross-lingual tests in the LeSC dataset, the paper further explores the comprehension abilities of models and experimentally highlights the deficiencies in language capability transfer among current LLMs.

#### Implementation and Deployment
Experimental results with the LeSC dataset showed that even the most advanced LLMs, such as GPT-4 and GPT-3.5, failed to meet human standards in lexical comprehension tasks, lagging behind 16-year-olds by 3.9% and 22.3% respectively. Additionally, despite the introduction of various advanced prompting techniques and retrieval-enhanced generation to mitigate this issue, limitations persist.

#### Summary
This study reveals significant shortcomings in large language models' understanding of the uncommon meanings of common words by establishing a new assessment framework and dataset, offering a new direction for enhancing models' NLU capabilities.