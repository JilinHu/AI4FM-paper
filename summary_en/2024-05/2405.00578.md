#### Background
- **Background**       
Large language models (LLMs) have demonstrated powerful capabilities in understanding human instructions and generating high-quality responses, which has led to widespread downstream applications such as question answering, tool learning, and information seeking.

- **Existing Work**       
Current LLM alignment techniques are designed to prevent the production of unhelpful or harmful responses. Yet, these alignment methods, including methods based on human annotations and principles, AI assistance, and online demonstrations, still face limitations due to lengthy training processes and predefined preference biases.

#### Core Contributions
- **A novel LLM alignment framework called Reinforcement Learning with Human Behaviors (RLHB)**
  - **Challenge 1: Overcoming predefined preferences and limited adaptability**
To tackle the limitations of LLMs, the paper introduces RLHB, which leverages real online human behaviors for aligning LLMs. Using a generative adversarial framework, RLHB is able to provide active and sustainable online alignment by training the generator (LLM) to produce responses following expected human behavior and having the discriminator confirm whether the triplet <query, response, human behavior> originated from real online environments.

  - **Challenge 2: Achieving and continuously updating the alignment of LLM with real human behaviors**
To make full use of the LLMs' abilities to understand and follow natural language, human behaviors are represented in natural language form and included as generation instructions. During the model training, the generator and discriminator undergo adversarial training, and after convergence, the generator is capable of generating responses that match the specified human behaviors. In the inference stage, the well-aligned generator can be directly deployed online and utilize user queries and the most preferred behavior signals as inputs.

#### Implementation and Deployment
The researchers conducted experiments to test RLHB's effectiveness, using both human and automated evaluations (e.g., using models like GPT-4). Results confirmed the efficacy of RLHB in aligning LLMs with online human behaviors. By leveraging online human behaviors directly, the RLHB framework removes the dependency on manual annotations for LLMs, enabling broader application scenarios. Moreover, RLHB can continuously learn and adapt as human behavior updates, owing to its mechanism of multi-model simultaneous training and natural language-based behavior modeling.

#### Summary
This paper introduces a novel framework, RLHB, for aligning large language models with real online human behaviors innovatively, overcoming the limitations of current approaches and effectively demonstrating its methods through experimentation.