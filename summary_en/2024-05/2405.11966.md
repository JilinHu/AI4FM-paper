#### Background
- **Background**
The paper introduces the current popular benchmarks for evaluating LLMs, with most of them using open-ended generation for model evaluation, which leads to numerous false negatives when extracting and comparing the expected answers. Thus, the authors pose a question: whether it is feasible to convert short-answer generation benchmarks like GSM8K and MATH into multiple-choice (MC) format to improve evaluation accuracy.
- **Existing Work**
Existing work, such as GSM8K and MATH, often produce false negatives during the extraction and evaluation process of model-generated answers due to an inability to precisely match the answer format, potentially making the evaluation process inefficient and the results not robust.

#### Core Contributions
  - **Introduced MC-formatted datasets**
      - **Challenge 1: Evaluation efficiency and accuracy**
      To address the issues caused by the current evaluation methods, the authors constructed MC-formatted datasets—GSM-MC, MATH-MC, and PythonIO—and demonstrated their strong correlation with the original versions (GSM8K and MATH) and their robustness to option choices. They also showed that the use of the MC format in evaluation could reduce the time consumption by up to 30 times. 
      - **Challenge 2: Preventing invalid answers from affecting evaluation accuracy**
      By collecting incorrect predictions from over 50 open-source models on GSM8K and MATH to construct a pool of distractors for each problem, the authors proved that LLMs' performance on GSM-MC is quite stable against distractor choices and option orders. This research contributes to an improved assessment method for LLMs, avoiding the accuracy disturbance by invalid answers.

#### Implementation and Deployment
The experiment results highlighted that GSM-MC, MATH-MC, and PythonIO datasets have a strong correlation with the original benchmarks (GSM8K and MATH as well as HumanEval and MBPP) and a high stability against choices and order. These datasets reduce the time required to evaluate LLMs by up to 30 times without sacrificing accuracy. Additionally, the authors tested several renowned open-source LLMs to validate the effectiveness and robustness of the proposed evaluation method.

#### Summary
The study successfully converted conventional open-ended generation problems into a multiple-choice format, significantly improving the efficiency and accuracy of LLM evaluations. This method has made strides in preventing the impact of invalid answers and enhancing evaluation efficiency.