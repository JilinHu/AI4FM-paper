#### Background
- **Background**       
In recent years, large language models (LLMs) have achieved notable success in various domains, but challenges such as the timeliness and costs associated with knowledge updates, coupled with issues related to LLMs generating erroneous information, have hindered their application in knowledge-intensive tasks. Retrieval-Augmented Generation (RAG) technology can be helpful in addressing these concerns.

- **Existing Work**       
Current retrieval-augmented generation models typically rely on similarity to retrieve documents from an external corpus, and use these documents as context for LLMs to enhance generation capabilities. While these methods perform better than purely parametric LLMs in knowledge-intensive tasks, their dependence on similarity can detract from their effectiveness.

#### Core Contributions
- **Introduced the METRAG framework**
  - **Challenge 1: Moving beyond similarity-oriented thought**
    Current models often fail to capture the utility between queries and documents. METRAG introduces a small-scale utility model supervised by an LLM, which goes beyond similarity-oriented thinking and proposes a "smarter" model by combining both similarity and utility-oriented thoughts.

  - **Challenge 2: Handling the size and isolation of the retrieved document set**
    The large sets of documents retrieved are often difficult to manage and use in isolation. METRAG suggests employing an LLM as a task-adaptive summarizer, endowing retrieval-augmented generation with compactness-oriented thought.

#### Implementation and Deployment
Extensive experiments on multiple knowledge-intensive tasks demonstrate the superiority of the METRAG framework. METRAG not only combines the advantages of similarity and utility but also increases the compactness and relevance of generated information by using an LLM as a task-adaptive summarizer.

#### Summary
METRAG offers a novel framework for retrieval-augmented generation that addresses the limitations of current models by incorporating utility and compactness-oriented thinking, and it exhibits enhanced performance in knowledge-intensive tasks.