#### Background
- **Background**
This study explores Reinforcement Learning from Human Feedback (RLHF) within Large Language Models (LLMs), highlighting its significance and advantages in current research.

- **Existing Work**
While previous RLHF studies have focused predominantly on offline learning, this paper seeks to explore online iterative RLHF and address gaps in existing research.

#### Core Contributions
- **Introducing a systematic workflow for online iterative RLHF**
  - **Challenge 1: Difficulty in obtaining online feedback**
    Online human feedback collection is generally impractical for open-source communities. By constructing proxy preference models using open-source datasets to approximate human feedback, the paper resolves this challenge.

  - **Challenge 2: Practical application of theory and algorithms**
    The paper discusses the theoretical foundations and algorithmic principles of online iterative RLHF in depth and provides a detailed practical implementation guide.

#### Implementation and Deployment
The pretrained model demonstrated outstanding performance across several LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks. Using fully open-source datasets, the model achieved state-of-the-art performance through supervised fine-tuning (SFT) and iterative RLHF.

#### Summary
The paper presents a comprehensive workflow for online iterative RLHF, which is innovative theoretically and offers a practical application framework through its detailed implementation guide.