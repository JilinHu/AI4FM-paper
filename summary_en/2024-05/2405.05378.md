#### Background
- **Background**
This paper discusses the prevalence of Large Language Models (LLMs) in modern society, especially in user-facing applications such as personal assistants and recruitment tools. Despite their widespread utility, studies have shown that they still harbor systemic biases.
- **Existing Work**
Previous studies on LLMs have predominantly focused on Western concepts such as race and gender biases, neglecting cultural concepts from other parts of the world. These studies usually treat "harm" as a singular dimension, ignoring the diversity and subtlety with which harms manifest. Existing studies haven't adequately detected or understood these issues.

#### Core Contributions
- **Introduced a Covert Harms and Social Threats (CHAST) metric system consisting of seven measures grounded in social science literature**
    - **Challenge 1: Evaluating across cultural differences**
        Past work often overlooks non-Western cultural concepts like the Indian caste system. By introducing CHAST metrics, this paper can assess covert harms in conversations involving both caste and race contexts generated by LLMs. This methodology reveals prejudices and discrimination that existing methods struggle to detect.
    - **Challenge 2: Subtle and diverse forms of harm**
        CHAST not only reveals outright discriminatory remarks but also can detect malicious views expressed in seemingly neutral language that are usually missed by existing methods. Through this detailed assessment, the research promotes a deeper understanding of how LLMs handle threats towards identity groups in generated conversations.

#### Implementation and Deployment
Testing on 8 open-source and OpenAI language models showed that all models generated conversations containing CHAST when discussing race and caste concepts. Particularly, the open-source LLMs and OpenAI's GPT-3.5-Turbo model produced significantly more CHAST-containing content in caste-centric conversations. Popular baseline models like Perspective API and Detoxify performed poorly in detecting harms and threats towards identity groups, a capability that CHAST successfully achieves.

#### Summary
This study reveals potential harms in complex social interactions involving a wide range of cultures and identities that LLMs might cause through the innovative CHAST assessment system, emphasizing the necessity of thorough bias audits before deploying these models.