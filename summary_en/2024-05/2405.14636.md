#### Background
- **Background**
The exponential growth in the number of LLM users poses a challenge for bandwidth-constrained cloud servers to process a massive volume of LLM services in real-time. While edge-cloud infrastructures are adopted to improve the efficiency of large-scale LLM services, the diversity of task requirements and dynamic resource allocation present significant challenges to inference scheduling.

- **Existing Work**
The adoption of edge-cloud computing addressing bandwidth limitations and high energy costs associated with large-scale data transmission and high-parameter model inference has been proposed. However, edge computing predominantly supports simple tasks, and service quality may be at risk when processing complex tasks.

#### Core Contributions
  - **Proposed the PerLLM framework**
    - **Challenge 1: Complexity of diverse services and scheduling**
      PerLLM integrates the upper confidence bound algorithm based on the constraint satisfaction mechanism to optimize service scheduling and resource allocation solutions within the edge-cloud infrastructure and meets the processing time requirements while minimizing energy costs.
    
    - **Challenge 2: Enhancing throughput and reducing energy costs**
      Experimental results show that compared to other methods, PerLLM is capable of effectively meeting personalized service processing time requirements, achieving a throughput increase of 2.2×, 2.1×, and 1.6× and reducing energy costs by more than 50%.

#### Implementation and Deployment
PerLLM models the decision-making process in edge-cloud collaborative architectures as a combinatorial multi-armed bandit problem and proposes an algorithm for efficient service scheduling and resource allocation. The experimental results demonstrate that PerLLM can significantly increase throughput, reduce processing time, and lower energy costs when serving diverse processing time requirements. Compared to baseline methods, PerLLM offers improved efficiency in handling personalized inference services, increasing throughput by more than 1.6× and reducing energy costs by more than 50%.

#### Summary
This paper introduces the PerLLM framework that leverages edge-cloud collaboration to handle a large volume of inference services, significantly enhancing service scheduling and resource allocation, thereby increasing throughput and reducing energy costs, showcasing its substantial applicative value.