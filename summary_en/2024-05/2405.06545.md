#### Background
- **Background**
The document discusses the issue of hallucinations in large language models, particularly in high-risk applications like healthcare, where the models' outputs do not align with real-world facts.
- **Existing Work**
Existing enhanced knowledge retrieval methods such as RAG, although capable of mitigating hallucinations, still face challenges of being resource-intensive and inefficient in practical applications.

#### Core Contributions
  - **Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR)**
      - **Challenge 1: Reducing the rounds of retrieval and verification**
      To address the issue of multiple retrieval and verification rounds in existing approaches, Re-KGR preliminarily identifies tokens that may produce hallucinations by analyzing the prediction probability distributions of tokens, thus reducing the number of verification rounds.
      - **Challenge 2: Increasing the truthfulness of responses**
      The approach enhances response truthfulness by correcting inaccuracies using retrieved knowledge in the post-processing phase.

#### Implementation and Deployment
Experimental results demonstrate that the method enhances the factual capabilities of LLMs across multiple foundation models, particularly showing effectiveness in increasing the truthfulness of answers on a medical dataset.

#### Summary
This work effectively reduces hallucinations in large language models through a novel Self-Refinement Enhanced Knowledge Graph Retrieval method, particularly enhancing practical application efficacy in the medical field.