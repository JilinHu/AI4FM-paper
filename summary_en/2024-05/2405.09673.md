#### Background
- **Background**       
    The paper discusses the challenges of finetuning large language models (LLMs) which require substantial GPU memory. It explores how parameter-efficient finetuning methods like LoRA (Low-Rank Adaptation) aim to reduce memory usage during training while attempting to maintain model performance on original tasks.

- **Existing Work**       
    Most previous studies focus on comparing LoRA with full parameter finetuning often not on LLMs with billions of parameters. Past evaluations mostly use coarse benchmarks that are less relevant for contemporary LLMs. In domains like code and mathematics, LoRA usually underperforms compared to full parameter finetuning.

#### Core Contributions
- **Introducing an integrated assessment system for LoRA and full parameter tuning**
    - **Challenge 1: How to effectively boost performance in the target domain while retaining performance on the source task**       
        By comparing the performance of LoRA and full parameter finetuning in the domains of code and mathematics, it was found that LoRA performs closer to full finetuning in mathematics, but significantly less so in code. However, LoRA showed better performance in maintaining the capability of the source domain.

    - **Challenge 2: How to improve parameter efficiency in LoRA**
        The finetuning process under LoRA configurations, particularly the selection of low-rank perturbations, was found to be highly sensitive to learning rates and target module choices. Moreover, by comparing to standard regularization techniques, it was discovered that LoRA provides stronger regularization effects.

#### Implementation and Deployment
The implementation was evaluated on specific coding and mathematics benchmarks, showing that in most cases, LoRA's performance could not match that of full parameter finetuning. However, compared to full finetuning, LoRA significantly maintains better performance on the source domain. Additionally, the regularization provided by LoRA at the output level helped the model maintain more diverse solutions, showcasing stronger regularization effects than traditional methods like dropout and weight decay.

#### Summary
Although LoRA often does not match the learning efficiency and accuracy of full parameter finetuning on target tasks, it exhibits better performance and stronger regularization capabilities in maintaining source task performance. Based on the study, recommendations are made for best practices when finetuning with LoRA, particularly noting the sensitivity to learning rates, choice of target modules, and rank of perturbations.