#### Background
- **Background**
Large Language Models (LLMs) have shown strong capabilities in solving a variety of tasks, yet they face challenges like hallucinations and outdated knowledge. Retrieval Augmented Generation (RAG) has become an essential technology to boost the abilities of LLMs by incorporating external knowledge. In open-domain QA, for instance, LLMs can first retrieve related documents and then generate answers. However, retrieving with the original query does not always yield correct and relevant documents. Query rewriting has been extensively used to reformulate the query to retrieve more appropriate documents for a better response.

- **Existing Work**
Existing efforts have attempted to leverage powerful LLMs for direct generation of rewrites or specific smaller models for query rewriting, along with reinforcement learning with feedback to improve performance. These methods typically require annotations like relevant documents or answers, or domain-specific pre-designed rewards, which lack generalization and fail to effectively use signals tailored for query rewriting.

#### Core Contributions
  - **Introduced the RaFe framework**
    - **Challenge 1: Annotation Costs and Signal Alignment**
      RaFe uses feedback from a reranker to train query rewriting models, reducing the need for annotations and aligning feedback well with rewriting objectives. The reranker is able to score documents without additional labels, hence used for providing feedback to the query rewriting model.

    - **Challenge 2: Training Framework Generalizability**
      RaFe doesn't require annotated labels or special scores, ensuring the generalizability of the training framework. The framework is intuitively designed to align with the goal of query rewriting (to retrieve documents relevant to the original query) and supports both offline and online RL feedback training.

#### Implementation and Deployment
RaFe undergoes a two-stage process. Initially, a query rewriting model is trained by standard supervised fine-tuning. Following this, ranking scores from the reranker are utilized to conduct feedback training on the query rewriting model. The approach is empirically proven using a general, publicly available reranker, signifying the effectiveness and potential generality of the proposed method. Furthermore, its effectiveness is validated on cross-lingual datasets in various settings, and a comprehensive investigation into what constitutes better query rewriting and how ranking feedback operates is conducted.

#### Summary
RaFe presents a novel framework for query rewriting using reranker feedback, requiring no annotations, supporting offline and online feedback training, and showcasing adaptability and effectiveness.