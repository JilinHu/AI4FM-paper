#### Background
- **Background**
    The paper addresses the challenging issue of effectively evaluating and comparing the capabilities of large language models (LLMs) which has drawn extensive attention from both the academic and industrial sectors. Current evaluation paradigms utilize either human annotators or model-based evaluators to assess LLMs across various tasks.

- **Existing Work**
     The existing methods are hindered by high costs, low generalizability, and inherent biases, which limit their capability to support the sustainable development of LLMs over the long term.

#### Core Contributions
- **Introduced a Peer Review Evaluator (PRE)**
    - **Challenge 1: High Cost of Evaluation**
        Existing methods are costly due to extensive annotation work, with costs growing in proportion to the number of LLMs evaluated, rendering them unsustainable in the long term. PRE dramatically lowers costs by employing an automatic evaluation mechanism akin to the peer review process in academia.

    - **Challenge 2: Low Generalizability**
        Current methods often require task-specific dataset construction and evaluator pre-training, thus their evaluators cannot be applied to tasks beyond the target task of the references or the training data. PRE does not depend on such constraints and can be easily generalized to different tasks.

    - **Challenge 3: Inherent Bias**
        Evaluation tools have inherent biases due to their model structure or algorithm design. PRE mitigates bias impact through using multiple reviewer models to carry out assessments and aggregating the results.

#### Implementation and Deployment
The PRE model demonstrated the highest consistency with human preferences (ground truth) in extensive experiments for text summarization tasks and outperformed all baseline models, including GPT-4. This validates the effectiveness of the peer-review mechanism.

#### Summary
The PRE model presented in this paper provides a novel framework for automatically evaluating LLMs by simulating the peer review system commonly used in academia, significantly lowering costs and exhibiting increased generalizability and reliability.