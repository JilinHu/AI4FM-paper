#### Background
- **Background**
This paper discusses the critical role of natural language critique in the development of Large Language Models (LLMs), assisting in the training of better models, alignment evaluation of model generations, and refinement of defective model outputs. The issue at hand is the lack of an automated and efficient method to evaluate the quality of these critiques.

- **Existing Work**
Existing work lacks quantifiable standards to determine the critique rating, insufficient transparency to produce comparable scores, and difficulty in grasping the complex relationships among multiple concepts in critique evaluation.

#### Core Contributions
  - **Introduced METACRITIQUE as an evaluation framework**
    - **Challenge 1: Lack of Quantifiable Standards**
      To address this challenge, METACRITIQUE establishes criteria for critique evaluation such as precision and recall scores.
    
    - **Challenge 2: Insufficient Transparency**
      METACRITIQUE introduces Atomic Information Units (AIUs) to enhance the transparency of the evaluation process by breaking down critiques into the most basic and indivisible units of information, thus reducing ambiguity in the evaluation process.

    - **Challenge 3: Complex Relationship Understanding**
      METACRITIQUE provides step-by-step natural language rationales for each AIU's judgment, known as Chain-of-Thought (CoT), which increases the reliability of the judgments and encourages human involvement in the evaluation loop.

#### Implementation and Deployment
The METACRITIQUE framework is implemented through three steps: reference generation, AIU extraction, and hypothesis critiquing. It utilizes GPT-4 generated content as a proxy for reference answers and critiques, with fine-tuned instructions and manual reviews to align with human preferences. METACRITIQUE performs detailed decomposition of critiques into AIUs by prompting GPT-4 for precise extraction. It also evaluates factuality of the hypothesis critique and the coverage over the reference critique through AIU-level binary classification tasks. METACRITIQUE demonstrated near-human performance in comparative studies, establishing its viability and effectiveness as a critique evaluation tool. The relevant code and meta-evaluation datasets have been made publicly available.

#### Summary
METACRITIQUE is the first framework to evaluate natural language critiques, assessing the quality of critiques using principles of precision and recall, and has achieved a high level of interpretability and transparency.