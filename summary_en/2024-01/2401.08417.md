#### Background
- **Background**
The paper points out that moderate-sized large language models (LLMs) with 7B or 13B parameters, such as the ALMA model with 13B parameters, show impressive performance in machine translation (MT) tasks. However, despite their prominent performances, these LLMs still cannot match the performance of the state-of-the-art traditional encoder-decoder translation models or larger LLMs like GPT-4.

- **Existing Work**
The paper mentions that although existing studies have tried to improve the performance of these LLMs through Supervised Fine-tuning (SFT), the outcomes are relatively modest. One of the reasons is that LLMs are predominantly pre-trained on English-centric datasets resulting in limited linguistic diversity. Furthermore, the ALMA model initially fine-tuned LLaMA-2 on monolingual data in various languages to enhance multilingual comprehension and then supervised fine-tuning with small but high-quality parallel data to steer the model toward translation generation. Even though this method made ALMA models outperform all previous moderate-sized LLMs and even GPT-3.5, the performance still lags behind GPT-4 and WMT competition winners.

#### Core Contributions
- **Introduced a new method called Contrastive Preference Optimization (CPO)**
  - **Challenge 1: Avoid generating translations that are adequate but not perfect**
      CPO aims to address two fundamental shortcomings of SFT: SFT caps model performance at the quality level of training data, a significant limitation since even human-written, high-quality data is not free from quality issues. CPO resolves this by training models to avoid generating merely adequate translations.

  - **Challenge 2: Pushing the performance boundary of SFT**
      CPO breaks the inherent bottleneck in SFTâ€™s reference-mimicking learning process, pushing the performance boundary of models that have saturated due to SFT training.

#### Implementation and Deployment
The researchers applied the CPO method to ALMA models, achieving significant improvements with only 22K parallel sentences and 12M parameters (equivalent to 0.1% of the original model size). The resulting model, known as ALMA-R, can match or exceed the performance of WMT competition winners and GPT-4 on the WMT'21, WMT'22, and WMT'23 test datasets. Notably, the ALMA-13B-R developed by further training the ALMA-13B-LoRA using the proposed CPO method either matches or surpasses the most advanced translation models, effectively narrowing the performance gap between moderate-sized LLM translators and state-of-the-art translation systems.

#### Summary
This paper introduces CPO, a novel LLM fine-tuning method that effectively overcomes the bottlenecks in SFT for MT tasks and achieves significant performance enhancements in moderate-sized LLM translation models with minimal resource expenditure, competing alongside the most advanced state-of-the-art translation systems.