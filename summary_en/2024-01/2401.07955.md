#### Background
- **Background**
This paper addresses the limitations that large language models (LLMs) face in solving multiple-choice questions (MCQ) tasks.
- **Existing Work**
Current studies fail to adequately address issues of LLMs correctly answering MCQ tasks, being independent of choice order, and understanding task requirements.

#### Core Contributions
- **Conducted MCQ performance tests on 26 open-source models**
    - **Challenge 1: Poor understanding of MCQ by models**
        Research indicates that most models are unable to adequately understand MCQ tasks. The paper designs experiments, which reveal that over 65% of the models are, in fact, poor at solving MCQ problems and struggle with understanding the task at hand. The paper overcomes this challenge by designing experiments with randomized choice order to avoid the model's bias towards specific answers due to their ordering, yielding a truer measure of the modelâ€™s capabilities.

    - **Challenge 2: Model's choice influenced by the order of options**
        The paper identifies that responses from more than 70% of the models depend on the order of the options rather than the options themselves, rendering their responses unacceptable. The paper used various methods to extract the model's answers to ensure the assessments were as accurate and fair as possible.

#### Implementation and Deployment
The paper tested the performance of 26 different open-source large language models on MCQ tasks. The selected models included small top-performing models based on the OpenLLM leaderboard and those trained on the largest datasets. The test consisted of 15 instruction-tuned models, 10 base models, and 1 reinforcement learning tuned model. To avoid bias due to a lack of samples, the paper ensured that there was at least one example from each category and each letter response. Experiments randomizing choice order were run to eliminate model bias toward choice order. Three evaluation methods were proposed: parsed text response, logit bias usage, and probability usage. The parsed text response method, with an accuracy rate of 95%, involved extracting a single letter answer from the models' generated text responses.

#### Summary
The study investigates the limitations of LLMs in MCQ tasks, highlighting poor performance by most models in such tasks. It also finds model answers often depend on the order of options and proposes effective assessment methods to eliminate these biases. The paper recommends exercising caution when using MCQs to evaluate LLMs and testing whether models truly understand the task at hand.