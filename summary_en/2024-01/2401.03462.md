#### Background
- **Background**
The paper discusses the challenges that Large Language Models (LLMs) face when processing long contexts due to their fixed context window length limitations, which prevents them from handling some real-world scenarios effectively.
- **Existing Work**
Current methods are costly in terms of training and inference due to the extensive computational requirements of longer context windows. Also, the continued training on long sequences could compromise LLMs' general capabilities with shorter contexts.
#### Core Contributions
  - **Introduced Activation Beacon**
    - **Challenge 1:** Redundant Information
With the hypothesis that LLMs' raw activations are information redundant, this work proposes that those activations can be condensed into more compact forms with minimal information loss, hence perceiving a broader context within a short context window, using special tokens called beacons to generate condensed activations for each interval, enhancing information density and extending context length.
    - **Challenge 2:** Training Efficiency and Model Compatibility
Beacons are designed as a plug-and-play module that is compatible with existing LLMs and infrastructure. They process long-sequence data in a streaming manner using sliding windows, improving working efficiency during training and inference. Beacons are learned through an auto-regression task, requiring only short-sequence data for training, and it takes less than 9 hours on a single GPU machine.

#### Implementation and Deployment
Empirical studies show that Activation Beacon is capable of extending the context length of the Llama-2-7B model from 4K to 400K. Activation Beacon achieves superior results on long-context language modeling and understanding tasks, comparable and sometimes exceeding fine-tuned full-attention baselines. Evaluations also include performance on various tasks such as single and multi-document QA, summarization, few-shot learning, code completion, where Activation Beacon exhibited high performance, similar to fine-tuned methods like LongChat-32K and LongAlpaca-16K in topic retrieval tasks.

#### Summary
The paper introduces Activation Beacon, a new technique to extend the context length of Large Language Models, enabling the perception of extensive context within a limited context window, while fully preserving capability on short contexts. Activation Beacon provides an effective, efficient, compatible, and low-training-cost method for extending LLMs' context length.