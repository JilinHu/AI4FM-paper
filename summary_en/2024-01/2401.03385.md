#### Background
- **Background**
The paper discusses the variation in In-context learning (ICL) capabilities among large language models (LLMs), noting that factors such as model type, architecture, volume of learning data, and parameter size significantly impact ICL capabilities. Generally, larger models with more extensive data exhibit stronger ICL abilities.

- **Existing Work**
Existing research identifies the ICL capabilities of models as being driven by the distributional characteristics of data itself, emphasizing the importance of data structure. However, these works do not offer an effective method to transfer the capabilities of large models to weaker models to enhance their ICL abilities.

#### Core Contributions
  - **Introduced a method named SLEICL (Strong LLM Enhanced ICL)**
    - **Challenge 1: Improving the ICL capability of weak language models**
      Current methods require the model itself to learn from prompt examples and choose the most suitable examples for optimizing performance. The SLEICL method learns example functions from strong models and transfers these skills to weak models, thus simplifying their learning process and addressing the issue of enhancing the ICL capability of weak models.

    - **Challenge 2: Ensuring the best grimoire selection for various models and tasks**
      Researchers developed a grimoire ranking method to automatically select the most suitable grimoire for different models and tasks, overcoming the challenge of automating the best example selection in a wide range of scenarios.

#### Implementation and Deployment
The study's experimental findings indicate that the SLEICL method enables models with various parameter sizes to achieve significant performance improvements on a variety of tasks, especially smaller models. Weak language models even outperformed the GPT4-1106-preview zero-shot performance on some datasets using SLEICL. Four distinctive sample selection methods (KCS, HCS, HSS, RSS) and two grimoire generation templates (PG, SG) were proposed, leading to the development of 10 types of single grimoires. The experiments involved 5 models and 8 datasets across 4 task types, confirming the effectiveness of SLEICL in actual deployment.

#### Summary
The paper introduces a method named SLEICL that significantly enhances the ICL capability of weak language models by learning and transferring skills from strong language models. The effectiveness of the method is validated through experiments, demonstrating the potential of this technology in enhancing weak language models' context learning abilities.