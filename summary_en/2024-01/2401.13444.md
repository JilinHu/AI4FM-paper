#### Background
- **Background**
Integrating knowledge graphs with large language models (LLMs) can result in inaccuracies when dealing with unfamiliar queries because existing approaches demand high computational capabilities from LLMs, which is not suitable for models with lower computational capacity.

- **Existing Work**
Current methods that blend LLMs with knowledge graphs fail to consider the limitation and bottlenecks LLMs face in path exploration within the knowledge graphs, thus overburdening the LLMs and limiting the applicability in less powerful models.

#### Core Contributions
- **Introduced a Clue-Guided Path Exploration (CGPE) framework**
  - **Challenge 1: Reduce LLM capability requirements**
      By using information from questions as clues for systematically exploring knowledge paths, CGPE decreases the overall capability demands on LLMs, enabling models with fewer parameters to effectively carry out question-answering tasks.
  
  - **Challenge 2: Lower computational resource consumption**
      CGPE requires minimal LLM invocations, significantly lowering computational resource consumption. Experiments show lower invocation frequency for knowledge acquisition tasks, reducing computational overhead.

#### Implementation and Deployment
The CGPE framework performed well in experiments on various open-source datasets, surpassing previous methods especially in terms of performance with LLMs that have fewer parameters. In certain cases, ChatGLM3 with 6 billion parameters could even match the performance of GPT-4. CGPE reduces the demand on LLMs by extracting clues from questions, identifying initial knowledge path nodes in the knowledge base, and exploring related knowledge paths. Furthermore, the minimal frequency of invoking the CGPE framework indicates its effectiveness in reducing computational resource usage. Comparative experiments and ablation studies provide insights into further reducing computational resource consumption.

#### Summary
The CGPE framework introduced in the paper effectively supports the application of LLMs in question-answering tasks by using a clue-guided path exploration mechanism, lowering the capability requirements for LLMs, and significantly reducing computational resource consumption, which has important practical significance for individuals and organizations with limited computational resources.