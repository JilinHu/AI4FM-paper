#### Background
- **Background**
This paper discusses the Supervised fine-tuning (SFT) of large language models (LLMs) on instruction datasets, which has been crucial for enhancing zero-shot generalization capabilities. As the variety of tasks covered by instruction datasets continues to expand, the annotation efforts required to generate high-quality response instructions increasingly become prohibitively expensive.

- **Existing Work**
Although there are increasing efforts to expand these datasets to cover more tasks to improve LLM generalization, expanding the dataset sizes requires annotating a vast number of instructions with detailed responses, which is costly at scale. Even though automatic annotation methods have been proposed to alleviate the burden on human annotators, existing LLMs may not be able to create high-quality annotations, especially for domain-specific tasks and datasets.

#### Core Contributions
- **Introduced an experimental design framework**
  - **Challenge 1: How to improve the label efficiency of models without high-cost label work**
      Experimental design techniques select the most informative samples for labeling, usually maximizing some concept of uncertainty and/or diversity. The study proposed a framework for evaluating a range of current and novel experimental design methods, which have shown substantial gains in labeling efficiency with minimal computational overhead.
    
  - **Challenge 2: The empirical benefits of experimental design have been underexplored**
      A range of experimental design techniques that significantly improve the label efficiency of SFT were proposed, developing novel scores such as maximum token uncertainty to quantify LLMsâ€™ uncertainty on a given sample, correlating positively with its utility as training data.

#### Implementation and Deployment
According to the evaluation implemented in the paper, the framework using experimental design methods achieves the same generalization performance with only 50% of the annotation costs required by random sampling. This framework includes many scenario-designed tests and, across different annotation budgets, the new strategies significantly outperform random sampling by more than 2% accuracy. Compared to prior works, this study is the first to observe annotation cost savings on generative tasks by using only 50% of the annotation budget (compared to random sampling) to achieve the same level of generalization performance.

#### Summary
The paper proposes an experimental design framework intended to improve the label efficiency of large language models during Supervised fine-tuning (SFT). It shows that experimental design techniques can significantly increase label efficiency while maintaining low computational costs, saving up to 50% annotation costs in some tasks compared to random sampling.