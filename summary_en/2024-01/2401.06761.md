#### Background
- **Background**
LLMs are increasingly integral to AI applications necessitating efficient deployment for low latency and high throughput. However, the intrinsic AR structure of these models presents considerable challenges for efficient serving.

- **Existing Work**
The auto-regressive decoding process is suboptimal in speed due to its dependency on previously generated tokens for each new token, resulting in potential memory-bound limitations and GPU underutilization. Additionally, Transformer attention computations across preceding tokens limit serving throughput, particularly for long responses, due to linearly scaling computational costs.

#### Core Contributions
  - **Introduced an Auto-Parallel Auto-Regressive (APAR) generation method**
    - **Challenge 1: Slow decoding speed and low GPU compute utilization**
      APAR enables LLMs to independently plan their generation process, resulting in a significant reduction in the number of generation steps. This achieves up to a 2× speed-up alone and up to a 4× speed-up when combined with other inference acceleration methods like speculative decoding.

    - **Challenge 2: Latency and key-value cache consumption in high-throughput scenarios**
      APAR minimizes key-value cache usage and attention computations during generation, leading to a throughput increase of 20-70% and a latency reduction of 20-35% in high-throughput scenarios compared to advanced serving frameworks.

#### Implementation and Deployment
APAR's parallel decoding capitalizes on the inherent parallelizable structure in LLM generation by fine-tuning models on data with hierarchical structures. It transforms the linear generation process into a parallelizable paragraph tree structure, enhancing decoding parallelism and allowing earlier release of consumed key-value cache memory. Experiments demonstrate a speed increase of 2× in memory-bound scenarios and up to 4× or 6× when combined with speculative decoding strategies. Generation quality remains consistent with variations within ±2%.

#### Summary
The research presents APAR as a method that significantly enhances the decoding efficiency and generation speed of LLMs in both memory-limited and high-throughput scenarios while maintaining generation quality, providing a potent new approach for deploying large language models efficiently.