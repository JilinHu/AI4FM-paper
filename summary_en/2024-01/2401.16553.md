#### Background
- **Background**
The paper discusses the importance of training Large Language Models (LLMs) on large and diverse instruction datasets to make models understand and follow human instructions. A small but high-quality set of instructions has been shown to outperform larger, noisier ones, but traditional active learning methods cannot be directly used for selecting unlabeled instructions because they require labeled responses.

- **Existing Work**
Creating a large-scale and diverse annotated instruction dataset requires extensive human labeling and poses a major challenge for instruction-tuning. Synthetic datasets created by advanced LLMs like GPT-4 have partially addressed this challenge but often contain low-quality data, indicating a need for more focused dataset refinement.

#### Core Contributions
- **Introduced SELECTLLM method**
    - **Challenge 1: Selecting high-quality instructions**
        SELECTLLM utilizes a clustering algorithm to divide unlabeled instructions into multiple clusters, then uses LLMs to select high-quality instructions within each cluster via prompting, without needing corresponding labels (i.e., responses). This method has shown comparable or even slightly better performance on popular instruction benchmarks compared to recent state-of-the-art selection methods.

    - **Challenge 2: Preserving the global structure of the dataset**
        SELECTLLM first divides the entire dataset into several small subsets using equal-size K-means clustering to create each subset with diverse unlabeled instructions. It then constructs input queries for each subset using a carefully designed input prompt for selection with LLMs. Subsequently, queries are generated, and a few instructions expected to be the most helpful in fine-tuning LLMs are selected.

#### Implementation and Deployment
The effectiveness of SELECTLLM was demonstrated on two popular benchmarks for instruction-tuning: Dolly and Cleaned Alpaca, compared to various state-of-the-art selection methods. It consistently outperformed the baselines across different selection sizes. For example, when selecting samples from Dolly, SELECTLLM exhibited nearly 2.5%-3% relative improvements over the strongest baseline in terms of the Rouge F1 score and cosine similarity across all sample sizes (1k/2k/3k). Additionally, SELECTLLM showed better cross-task generalizations, an important characteristic for instruction-tuned LMs.

#### Summary
This work introduces a novel method SELECTLLM for using LLMs to select unlabeled high-quality instructions, challenging traditional selection algorithms and enhancing selection efficiency while maintaining the global structure of the dataset. The experiments demonstrate superior performance on instruction-tuning benchmarks.