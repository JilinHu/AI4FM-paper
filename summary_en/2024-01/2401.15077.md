#### Background
- **Background**
Large Language Models (LLMs) suffer from time-consuming auto-regressive decoding during inferencing. Despite speeding up the process, existing speculative sampling methods face challenges.

- **Existing Work**
Existing speculative sampling methods use a smaller low-parameter version of an LLM of the same series as the draft model to reduce inference latency, validating these tokens in parallel through a single forward pass of the LLM during the verification stage. Common problems among these methods include their lower draft generation accuracy and difficulty in finding a suitable draft model for the smallest model variants.

#### Core Contributions
  - **Introduced a framework named EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency)**
      - **Challenge 1: Reduce time overhead and increase draft acceptance rate**
        The key to enhanced acceleration in speculative sampling is to reduce time overhead and improve the acceptance rate of the draft by the original LLM. EAGLE diverges from direct token prediction and instead performs auto-regressive operations at the feature level, sidestepping the uncertainty associated with feature-level auto-regression by incorporating tokens from one timestep ahead.

      - **Challenge 2: Maintain the consistency of generated text distribution with the original LLM**
        The acceleration provided by EAGLE is lossless: it does not involve fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. Even when generating non-greedy decoded text (with temperature=1 setting), EAGLE maintains output distribution consistent with the original LLM.

#### Implementation and Deployment
The evaluation of EAGLE on MT-bench shows that it is the fastest known framework for auto-regressive decoding and speeds up models twice as fast as Lookahead and 1.6 times as fast as Medusa. EAGLE has low training costs; for the LLaMA2-Chat 70B model, EAGLEâ€™s decoder layer has less than 1 billion parameters and requires only 1-2 days of training. Moreover, the EAGLE framework is easily deployable in production environments, does not involve any fine-tuning of the original LLM, and is simple to apply, adding only a lightweight plugin (a single Transformer decoder layer) to the LLM.

#### Summary
The paper proposes a new framework named EAGLE to increase the auto-regressive decoding speed of Large Language Models (LLMs) while maintaining the consistency of the generated text distribution with the original LLMs. EAGLE has significantly improved upon speculative sampling methods in reducing time overhead and increasing draft acceptance rate, offering faster acceleration compared to Lookahead and Medusa, with low training cost and ease of deployment.