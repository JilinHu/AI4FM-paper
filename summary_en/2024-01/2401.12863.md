#### Background
- **Background**
This paper addresses Large Language Models (LLMs) that have exhibited impressive performance in natural language processing tasks by leveraging chain-of-thought (CoT) reasoning, which enables step-by-step thinking similar to human cognitive processes.

- **Existing Work**
Existing systems suffer from high computational costs and substantial hardware resource requirements. Traditional language models without explicit intermediate steps often provide sub-optimal answers in complex reasoning scenarios. Recent extensions of LLMs with multimodal capabilities have become a research interest but require large-scale models needing significant hardware infrastructure. However, approaches such as fine-tuning smaller models for multimodality and CoT capabilities often lead to hallucinations, where the model generates plausible but incorrect reasoning and answers.

#### Core Contributions
  - **Proposed the KAM-CoT Framework**
    - **Challenge 1: Integrating Multimodal Data and Knowledge Graphs**
      The paper introduced the KAM-CoT framework that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities for a comprehensive understanding of multimodal tasks. The two-stage training process with KG grounding generates effective rationales and answers, incorporating external knowledge from KGs during reasoning to achieve a deeper contextual understanding, reducing hallucinations, and improving answer quality.
      
    - **Challenge 2: Increasing Efficiency and Performance**
      KAM-CoT achieved new state-of-the-art performance levels using only 280M trainable parameters. On the ScienceQA dataset, it reached an average accuracy of 93.87%, surpassing GPT-3.5 by 18% and GPT-4 by 10%, demonstrating its cost-efficiency and efficacy.

#### Implementation and Deployment
KAM-CoT integrates a language model for language context, a vision encoder for visual feature encoding, and a graph neural network (GNN) to reason over knowledge graphs. It seamlessly stitches together text, vision, and graph features, enabling machines to think and reason coherently, akin to human cognition. With extensive experiments and evaluations on the ScienceQA benchmark dataset, KAM-CoT achieved an average accuracy of 93.87%, outperforming both GPT-3.5 and GPT-4 while only using 280M trainable parameters.

#### Summary
KAM-CoT is a multimodal Chain-of-Thought reasoning framework that integrates CoT reasoning, knowledge graphs, and multiple modalities. It outperforms state-of-the-art approaches with fewer trainable parameters, showcasing superior performance and cost-efficiency.