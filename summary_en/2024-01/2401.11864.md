#### Background
- **Background**
The article introduces that the capabilities of Large Language Models (LLMs) in mathematical reasoning can be enhanced by prompting them to articulate intermediate steps towards a solution.

- **Existing Work**
The paper mentions that existing work has generated executable code for LLMs, used multiple reasoning paths with a voting mechanism, prompted LLMs to create plans before reasoning, or used diverse reasoning prompts to solve problems. Although these methods have improved the reasoning abilities of models to a certain extent, there is still room for improvement in the ability to convert mathematical problems into solvable equation systems and in the need to reduce calculation errors and improve the handling of undefined variables.

#### Core Contributions
- **Introduced a new framework named Equation-of-Thought Distillation (EoTD)**
  - **Challenge 1: Transforming mathematical reasoning into equations**
       EoTD transforms mathematical reasoning into equations for finetuning Small Language Models (SLMs). Unlike Chain-of-Thought Distillation and Program-of-Thought Distillation, EoTD mitigates calculation inaccuracies by generating solvable equation systems and employing a deterministic solver, effectively improving mathematical reasoning capabilities.
 

  - **Challenge 2: Related datasets and experimental design**
       Researchers employed ChatGPT as the teacher model to construct the training dataset and used various scaled CodeT5 models as student SLMs. They manually created guiding examples to generate four reasoning paths for each dataset and finetuned the models using these datasets.

#### Implementation and Deployment
The EoTD method proposed in the paper demonstrates effectiveness through experiments on four mathematical reasoning datasets, achieving significant absolute improvements ranging from 25.51% to 43.52% in SLMs. Meanwhile, the Mix Thoughts Distillation (MTD) framework leads SLMs to state-of-the-art reasoning performance with improvements between 48.14% and 57.58%. The results also revealed that the size of the SLMs is crucial for distillation efficacy; larger models assimilate more reasoning knowledge, translating to superior performance.

#### Summary
This paper stated that EoTD and MTD show it is possible to distill LLMs' mathematical reasoning capabilities into Small Language Models (SLMs) with fewer than one billion parameters. The methods preserve and enhance SLMs' reasoning abilities, enabling them to achieve state-of-the-art performance on reasoning tasks. This advancement opens the door for broader applications of SLMs in resource-constrained environments, bridging the gap between the demand for powerful reasoning models and computational resource limitations.