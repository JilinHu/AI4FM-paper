#### Background
- **Background**
The paper discusses the advancements of modern LLMs such as GPT-4, which can perform comparably to humans in certain tasks. However, the ability of these models to process long and highly technical texts, such as scientific papers, has not been fully explored. Existing LLMs are usually limited to processing texts of a certain length and are primarily trained on non-technical content such as news articles and websites.

- **Existing Work**
The previous studies focused on automatically generating peer-review feedback for scientific papers but mainly utilized relatively small models that cannot consume the entire text of a paper or relied on template-filling instead of generating nuanced free-form comments. Although more advanced attempts like using GPT-4 to verify authors' checklists have been made, they are limited in terms of the variety of comment types generated. Previous work has not attempted to overcome the input size limitations of GPT-4 nor construct specialized prompts and "experts" for various types of comments.

#### Core Contributions
  - **Introduced a feedback generation method named MARG**
    - **Challenge 1: Input length limitation**
      By using multiple GPT instances (agents), distributing paper text across agents that engage in internal discussion, MARG is able to handle texts exceeding the input length limitations of the base LLM. Specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact), MARG enhances the helpfulness and specificity of feedback.
  
    - **Challenge 2: Generating feedback for specific types**
      Introducing aspect-specific "expert" GPT agents to individually assist with generating comments on experiments, clarity, and impact. MARG-S, a specialized variant of MARG, significantly outperforms the method of using a single agent to generate all types of feedback simultaneously.
  
#### Implementation and Deployment
In user studies, MARG-S averaged 3.7 "good" comments per paper, compared to 1.7 good comments for a simple baseline that had a single agent generate all comments, and only 0.3 for a recently proposed method by Liang et al. (2023). Also, while the majority of comments generated by the baselines were viewed as generic by users, a significant majority (71%) of MARG-Sâ€™s comments were rated as specific. The authors analyzed the weaknesses of MARG-S, including high costs and internal communication errors, such as failing to include key information in some messages, and proposed directions for future work.

#### Summary
This paper presents an innovative multi-agent review generation method (MARG) capable of overcoming the context size limitations of the base model and of generating high-quality peer-review feedback for scientific papers. The quality of feedback generated by MARG significantly surpasses the baselines in user studies and automated metrics, with a 2.2-fold increase in the number of helpful comments and a greater generation of specific comments.