#### Background
- **Background**
This paper explores the performance of Large Language Models (LLMs) on multilingual tasks after Parameter Efficient Finetuning (PEFT). Previous research shows a significant gap in the performance of LLMs on non-English languages compared to English, particularly when compared to smaller open-source models. By fine-tuning the models, we can effectively shrink this gap and make language models more equitable.

- **Existing Work**
Most studies have focused on the training, fine-tuning, and evaluation of LLMs in English tasks. While PEFT techniques like LoRA have been proven to enhance the multilingual capabilities of open-source LLMs, an extensive analysis of the impact of different PEFT choices, configurations, and settings on multilingual tasks has not been done.

#### Core Contributions
  - **Introduced a xxx**
    - **Challenge 1: How to improve performance on multilingual tasks without compromising English performance?**
        Researchers fine-tuned LLMs with various LoRA ranks and quantization values to evaluate multilingual performance and analyzed the effect of these configurations on both English and other low-resource language performance. By comparing models before and after fine-tuning, they found that, in some cases, higher ranks and quantization benefited low-resource languages while sometimes sacrificing English performance.

    - **Challenge 2: How to quantify the changes in model performance post-fine-tuning?**
        The researchers conducted experimental evaluations across a diverse set of datasets involving five downstream tasks and twenty-three languages. By analyzing the performance of models across different tasks and languages, they determined the specific effects of parameter-efficient fine-tuning on various languages and provided directions for future research on PEFT in multilingual settings.

#### Implementation and Deployment
They experimented with LLAMA-7B and MISTRAL-7B models, fine-tuned on the MULTIALPACA dataset, using different LoRA ranks and quantization parameters. The experimental results indicated that, in some cases, the fine-tuned models were able to perform well on multilingual tasks, especially for lower-resource languages. However, they also noted a potential degradation in English performance. For evaluation, the researchers used the `lm-eval-harness` framework and `Alpaca Eval` benchmark to evaluate multilingual capabilities.

#### Summary
This study investigates the performance of large language models on multilingual tasks following parameter-efficient fine-tuning, especially in the context of low-resource languages and English tasks. It demonstrates the potential of PEFT and highlights areas for future work.