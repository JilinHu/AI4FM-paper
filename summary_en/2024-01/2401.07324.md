#### Background
- **Background**
This paper discusses the limitations of traditional approaches in task planning, memory management, tool invocation, and result summarization, particularly with smaller language models, and the need for retraining of LLMs when tools are updated.
- **Existing Work**
The existing solutions involve training a single LLM to integrate the aforementioned capabilities but have noticeable performance limitations in smaller open-source LLMs and may require retraining when tools are updated frequently.

#### Core Contributions
  - **Introduced a multi-LLM framework α-UMi**
      - **Challenge 1: Modular multifunctionality**
          The approach overcomes the challenge through a framework that splits capabilities into planner, caller, and summarizer components. These single LLM-implemented components focus on particular functions and collaborate to complete tasks, enhancing performance and allowing updates as needed.
      - **Challenge 2: Effective training**
          The paper introduces a novel two-stage fine-tuning strategy, first fine-tuning an LLM backbone on the whole dataset to enhance overall task understanding, then using the fine-tuned LLM to instantiate the planner, caller, and summarizer for further performance improvements through continuous fine-tuning on their respective sub-task datasets.

#### Implementation and Deployment
The multi-LLM framework was evaluated against multiple tool-use and program-assisted mathematical reasoning agent benchmarks, demonstrating its superiority over the traditional single-LLM approach across different model and data sizes. Additionally, the importance of the two-stage training strategy for the success of the framework was showcased, along with an in-depth analysis of why performance improved.

#### Summary
The study reveals the weakness of small LLMs as tool learners and introduces the α-UMi multi-LLM framework, which outperforms the single-LLM approach. It highlights a crucial two-stage fine-tuning strategy and delves into data-scaling laws.