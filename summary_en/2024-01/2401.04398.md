#### Background
- **Background**
Tables are extensively used in everyday life and understanding tabular data with language models benefits various downstream tasks such as table-based fact verification.

- **Existing Work**
Several approaches have been suggested to train language models for table understanding, including specialized embedding layers or attention mechanisms and synthesizing SQL query-response pairs for an encoder-decoder model. Additionally, LLMs have been prompted with reasoning chains to enhance their reliability across tasks. However, these methods may not be well-suited for complex table scenarios.

#### Core Contributions
- **Introduced CHAIN-OF-TABLE Framework**
  - **Challenge 1: Effectively using tabular data for reasoning**
     Existing reasoning approaches, such as Chain-of-Thought, deploy reasoning chains in textual context without explicitly utilizing tabular data within the reasoning process. The CHAIN-OF-TABLE framework overcomes this by guiding LLMs with in-context learning to iteratively generate operations and update the table to represent the reasoning chain, thereby providing a dynamic plan for each operation and continuously evolving the tables, showcasing a structured reasoning process.

#### Implementation and Deployment
CHAIN-OF-TABLE has achieved state-of-the-art performance on three tabular benchmarks (WikiTQ, FeTaQA, and TabFact) through its innovative approach of forming a chain of evolving tables, representing intermediate reasoning results and continuously iterating through the reasoning process until a final conclusion is reached.

#### Summary
The paper introduces the innovative CHAIN-OF-TABLE framework, which enhances reasoning capabilities of LLMs by explicitly incorporating tabular data into the reasoning chain, dynamically planning and updating the process, thereby increasing accuracy and reliability for table-based reasoning tasks.