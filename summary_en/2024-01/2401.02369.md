#### Background
- **Background**
The paper acknowledges that clinicians are required to write lengthy summaries each time a patient is discharged from the hospital. The task is time-consuming due to the vast number of unique clinical concepts covered during admission. Identifying and covering salient entities is crucial for the summary to be clinically useful. The paper analyzes the healthcare industry's current state and reveals issues caused by Electronic Health Record (EHR), such as information overload and documentation burden, leading to significant clinician burnout.

- **Existing Work**
Large Language Models (LLMs) have the potential to relieve the documentation burden and serve as a tool to alleviate the workload of clinical doctors. However, LLMs are mostly applied to well-defined, granular tasks in healthcare, such as closed-book question answering and single-document radiology report summarization. For long document tasks spanning the entire hospital stay, LLMs have not seen adequate application.

#### Core Contributions
- **Introduced SPEER**
    - **Challenge 1: Cover key entities in lengthy clinical summaries**
        Existing fine-tuned open-source large language models such as Mistral and Zephyr often produce incomplete and unreliable summaries. The paper proposes training a smaller encoder-only model to predict salient entities, treated as content plans, to guide the LLM for better coverage and information extraction.

    - **Challenge 2: Integrating the content plan with the source notes**
        To better align the content plan with the original notes, the SPEER method is proposed: Sentence-Level Planning through Embedded Entity Retrieval. The approach involves marking special boundary tags around each salient entity and instructing the LLM to retrieve these marked spans before generating each sentence. This sentence-level planning serves as a form of state tracking, enabling the model to clearly record the entities it uses.

#### Implementation and Deployment
SPEER, after being fine-tuned on the existing LLMs and trained using a large-scale and diverse dataset of approximately 167,000 in-patient records, is evaluated on three datasets from different EHRs. The results demonstrate that SPEER exceeds both unguided and guided baseline models in terms of coverage and faithfulness metrics. Moreover, the paper substantiates that content selection should be regarded as an independent classification task even with the presence of LLMs and proves that explicit content selection performed by a smaller encoder-only classifier outperforms implicit content selection from autoregressive LLM decoding. SPEER's method is easy to implement and significantly improves coverage and summary quality.

#### Summary
This paper proposes SPEER, a sentence-level planning method through embedded entity retrieval for long document tasks of hospital discharge summaries. It guides large language models (LLMs) to better cover key entities and generate more complete and credible clinical summaries. The research demonstrates that the SPEER method can improve document coverage and accuracy in practical applications, thereby reducing the documentation burden on clinicians.