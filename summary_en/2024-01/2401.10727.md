#### Background
- **Background**
The paper addresses the remarkable achievements of large language models (LLMs) in natural language tasks and the subsequent explorations in using them as central controllers for agent systems. Current LLMs are limited to single text queries which may fail to adequately capture users' real intentions due to inherent ambiguity in text instructions.

- **Existing Work**
Existing studies have extended LLMs to domains like restaurant reservations and online shopping, relying solely on textual inputs. This approach could lead to confusion and misinterpretation when user intentions aren't clearly conveyed through text alone.

#### Core Contributions
- **Tool-LMM System Introduced**
    - **Challenge 1: Handling Multi-Modal Inputs**
        The proposed Tool-LMM system incorporates multi-modal encoders with open-source LLMs, enabling the learned LLMs to be aware of multi-modal instruction and correctly select function-matched tools. This marks the first effort to train a large multi-modal model for tool agent learning.

    - **Challenge 2: Assessing Modelâ€™s Tool Selection Capability**
        To facilitate the evaluation, a novel dataset named ToolMMBench was collected featuring multi-modal input tools, and containing multiple potential choices for the same instruction due to the presence of identical and synonymous functions, which provides more potential solutions for the same query.

#### Implementation and Deployment
Experimental results on 24 popular LLMs demonstrate an impressive 88.19% accuracy in tool selection after fine-tuning Tool-LMM on the ToolMMBench dataset, indicating the effectiveness of the proposed method. Additionally, evaluation metrics were designed to assess LLMs' awareness and selection abilities for external tool usage with text-ambiguous queries through extensive subset ablations.

#### Summary
Tool-LMM stands out as the first system aimed at training a large multi-modal model to learn tool agency, innovatively integrating multi-modal inputs with the correct selection of external tools, overcoming ambiguity in text, and showcasing the ability to automatically select appropriate tools in response to multi-modal instructions.