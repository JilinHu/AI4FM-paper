#### Background
- **Background**
This paper discusses the significant impact of large language models (LLMs) on the development of the cloud-based LLM services industry. However, the dynamic autoregressive nature of LLM services and the requirement to support ultra-long context lengths have posed significant challenges in designing cloud-based LLM service systems, such as inefficient management that can lead to performance degradation or resource wastage.

- **Existing Work**
Previous work has attempted to solve issues by exchanging data between GPU and CPU memory. However, this method has its limitations, such as PagedAttention being limited to GPU and CPU memory within a single node. It uses a paging strategy that might lead to memory fragmentation, and because it relies on request-level KV cache exchange, it does not utilize more granular scheduling opportunities in a distributed cloud environment, and swapped-out requests might affect the performance of running tasks.

#### Core Contributions
  - **Introduced a new distributed attention algorithm called DistAttention**
      - **Challenge 1: Cloud-based LLM service support for dynamic and ultra-long contexts**
      DistAttention enables distributed processing and storage of the attention module by segmenting the KV cache into smaller, manageable units (rBlocks), supporting the LLM services with dynamic changes and extremely long contexts.

      - **Challenge 2: Optimizing and managing memory resources across data centers**
      DistAttention utilizes all accessible GPU and CPU memory resources within the data center, especially those currently underutilized, thus avoiding the performance fluctuations typically associated with data swapping or live migration processes.
  
#### Implementation and Deployment
Based on DistAttention, the paper further introduces a distributed LLM service system called DistKV-LLM, which dynamically manages the KV cache across the entire data center and effectively coordinates the use of CPU and GPU memory. In tests conducted in a cloud environment, the system demonstrated end-to-end throughput improvements of 1.03-2.4Ã— and supported context lengths 2-19 times longer than the current state-of-the-art LLM service systems across 18 benchmark datasets with context lengths up to 1900K.

#### Summary
The paper presents an efficient system for cloud services supporting long-context language models. Through the distributed algorithm DistAttention, it optimizes the processing and storage of the attention module, and the DistKV-LLM service system manages and coordinates it. It achieves efficient allocation and management of resources in a distributed environment, demonstrating significant performance improvements.