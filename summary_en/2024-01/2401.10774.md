#### Background
- **Background**
Large Language Models (LLMs) have shown significant improvements in language generation as their size and the number of parameters increase. However, this growth has led to increased inference latency, posing challenges in practical applications. The inference process of LLMs is mainly memory-bound, and the latency bottleneck comes from the memory bandwidth of accelerators, underutilizing the computation potential during auto-regressive decoding.
- **Existing Work**
Previous solutions like speculative decoding aimed to address this by increasing the arithmetic intensity of the decoding process and reducing the number of decoding steps. Nevertheless, they faced challenges in getting and maintaining a separate draft model.

#### Core Contributions
  - **Challenge 1: Accelerating LLM Inference While Maintaining Generation Quality**
      Medusa method is designed to accelerate LLM inference without compromising generation quality. The approach uses extra decoding heads to predict multiple subsequent tokens in parallel, reducing the number of decoding steps required.
  - **Challenge 2: Adapting to Different Training Needs for Various Use Cases**
      Medusa provides two fine-tuning strategies. Medusa-1 directly fine-tunes on a frozen LLM backbone for lossless inference acceleration, whereas Medusa-2 is jointly fine-tuned with the backbone for improved prediction accuracy and higher speedup but requires a special training recipe.

#### Implementation and Deployment
Experimental results demonstrated that Medusa-1 can achieve over a 2.2x speedup in inference without impacting the generation quality. Medusa-2 further enhances the speedup to between 2.3x to 3.6x across various model sizes and training procedures, including scenarios with self-distillation. The code has been made available on GitHub for public access.

#### Summary
The paper presents Medusa, an efficient method for accelerating LLM inference by adding multiple decoding heads that parallelly predict multiple tokens, thereby substantially reducing the number of decoding steps and significantly improving the inference speed of large models.