#### Background
- **Background**
This paper discusses the challenging task of Temporal Knowledge Graph Completion (TKGC), which predicts missing event links at future timestamps by leveraging existing temporal structural knowledge. The generative capabilities inherent in Large Language Models (LLMs) provide a novel approach to this task.

- **Existing Work**
Existing work primarily involves embedding-based models, which are limited in providing a precise ranking of all entities in the graph and require enumerating all possible candidate entities while predicting. This approach introduces impractical search costs and cannot directly remember all entities for LLMs.

#### Core Contributions
  - **Efficient Fine-tuning Approach**
      - **Challenge 1: Adapting to Specific Graph Textual Information and Temporal Patterns**
           The authors introduced an efficient fine-tuning method allowing LLMs to adapt to specific graph textual information and patterns found within historical event sequences. For data selection, they prioritize real history directly related to a query and preferentially include data close to the current timestamp.

      - **Challenge 2: Enhancing LLMs' Awareness of Structural Information and Reasoning Capability**
           By using structure-augmented historical data and integrating reverse knowledge, the paper emphasizes LLMs' understanding of structural information, thus enhancing their reasoning capabilities. The concept of reverse logic is also introduced to overcome the issues LLMs face in reverse reasoning.

#### Implementation and Deployment
LLMs were fine-tuned using a cross-entropy loss function and predicted using beam search. The experiments employed the ICEWS14, ICEWS18, ICEWS05-15, and YAGO datasets for benchmarking and compared against both embedding-based and LLM-based methods. The results showed that the proposed model outperforms existing embedding-based models across multiple metrics, achieving the state-of-the-art performance in the raw setting.

#### Summary
The paper presents a method for temporal knowledge graph completion utilizing large language models. By implementing efficient fine-tuning methods and historical data augmentation with structural information, the model's reasoning capabilities and performance were improved. Experiments demonstrate that this approach effectively enhances the precision of temporal knowledge graph predictions, achieving state-of-the-art results.