#### Background
- **Background**
The study discusses enhancing the capabilities of Large Language Models (LLMs) by composing foundational models with domain-specific models.

- **Existing Work**
Existing works tend to be challenging and expensive when augmenting LLMs with new skills due to their monolithic structure. Further pre-training or fine-tuning the anchor model to the augmenting model's domain is computationally intensive, which limits feasibility due to privacy concerns and organizational boundaries.

#### Core Contributions
- **Introduced CALM (Composition to Augment Language Models) Framework**
    - **Challenge 1: How to effectively integrate two separate models to enable new capabilities**
      CALM introduces a small number of trainable parameters allowing cross-model attention, enabling the integration of different model capabilities without changing the weight of any model.

    - **Challenge 2: Expanding capabilities in limited data domains**
      The composed model demonstrated through the CALM framework shows significant improvements in tasks involving low-resource language translation and code generation, which were not capabilities of the individual models.

#### Implementation and Deployment
CALM demonstrated its effectiveness in various experiments. For instance, in tasks that combined key-value mappings with arithmetic operations, the CALM-composed model outperformed the standalone models, achieving a 40% relative improvement over the baseline model. Additionally, CALM's extra parameters allowed for an extension of model capabilities without altering the model weights, resulting in an absolute increase of up to 13% in tasks such as low-resource language translation and arithmetic reasoning.

#### Summary
The paper presents a new framework for model extension - CALM, which successfully integrates two large language models to perform new tasks and demonstrates its effectiveness across multiple experiments.