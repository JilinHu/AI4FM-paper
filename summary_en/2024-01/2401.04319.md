#### Background
- **Background**       
The paper explores a new user targeting method where non-expert marketers could rely solely on demands in natural language to select target users. The crux of the issue is how to transform natural language into practical structured logical languages for a structured understanding of marketer demands.

- **Existing Work**    
Previous research indicates that the reasoning ability of LLMs can be significantly enhanced through chain-of-thought (CoT) prompting. However, current methods have some limitations: Some of them use simple "Let's think step by step" techniques or provide fixed examples in demonstrations without considering the compatibility between prompts and questions, thus performing inadequately in complex reasoning tasks such as structured language transformation. Additionally, existing methods are often implemented in closed-source models or excessively large models, which are not practical in industrial contexts.

#### Core Contributions
- **Introduced a model named ARALLM (Analogical Reasoning Augmented Large Language Models)**
    - **Challenge 1: Natural language to structured logical language transformation**
        They first introduced a structured language called SELL that is suitable for marketing scenarios and built a reasoning library with several high-quality examples of translating natural language to SELL (NL2SELL). Then they adopted a retrieval-based method for performing analogical reasoning with assistance from the reasoning library. Experimental results show that prompting with analogical reasoning outperforms ordinary prompting methods in NL2SELL tasks.

    - **Challenge 2: Deployment of the model in industrial practice settings**
        They further distilled knowledge from super LLMs (like ChatGPT) through analogical reasoning and constructed an NL2SELL dataset to fine-tune smaller student LLMs in a multi-task training paradigm. This enables the models to be easily deployed in practical user targeting environments.

#### Implementation and Deployment
In the experiments, the researchers first tested the effectiveness of the analogical reasoning-based prompting method on ChatGPT and proposed four additional variants of prompts for comparison. The results demonstrated that ARALLM outperforms other prompting methods, especially in structural accuracy which showed an improvement of over 7%. It was also found that even randomly selected few-shot reasoning examples performed better than fixed ones. This indicates that providing fixed reasoning examples for all inputs is often suboptimal and highlights the necessity of establishing a reasoning library to provide diverse reasoning samples. For knowledge distillation, ChatGLM2-6B-32K and Baichuan2-13B-Chat were selected as student LLMs and were fine-tuned using LoRA with a multi-task training approach. Additional three training strategies were also proposed as baselines, which included training without a multi-task strategy, training without providing reasoning steps, and the most common training method where the model input simply combined instructions with demands.

#### Summary
The paper presents a new method named ARALLM that combines analogical reasoning and multi-task model distillation to effectively enhance LLMs' ability to understand and transform natural language into structured logical expressions. This method allows non-expert marketers to use natural language for user targeting, which potentially changes the practice of user targeting. The improvement in this capability not only has practical value in marketing scenarios but also contributes valuable exploration to the functionality and practicality of large language models.