#### Background
- **Background**
The paper notes that Large Language Models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become a major obstacle to their widespread application. Many studies have extensively investigated the risks in LLM systems and developed corresponding mitigation strategies. Leading enterprises such as OpenAI, Google, Meta, and Anthropic have also made significant efforts on responsible LLMs. Therefore, there is a need to organize existing studies and establish comprehensive taxonomies for the community.

- **Existing Work**
Despite efforts to understand and mitigate the potential risks in LLM systems, there is no unified analytical framework to systematically assess and enhance the reliability of LLM systems. Existing works focus on assessing and analyzing output content using various metrics, lacking a comprehensive taxonomy that covers potential risks across all modules of the LLM system.

#### Core contributions
  - **Proposed a module-oriented taxonomy of risks and mitigation strategies**
    - **Challenge 1: Systematization of risk understanding**
        The proposed systematic risk taxonomy helps developers to understand the root causes behind potential risks, thereby promoting the development of beneficial LLM systems. It is more comprehensive than previous taxonomies, especially considering security issues closely related to the toolchain, rarely discussed in prior surveys.

    - **Challenge 2: Establishing evaluation benchmarks**
        This paper reviews widely adopted risk assessment benchmarks and discusses the safety and security of prevalent LLM systems, providing a summary for assessing the safety and security of LLM systems.

#### Implementation and Deployment
Based on the abstract and table of contents, the article audits the safety and security of LLM systems by examining the risks and mitigation strategies related to the core modules of the system, as well as the assessment tools. The paper systematically introduces the safety and security concerns associated with the input module, language model, toolchain module, and output module, and outlines the mitigation strategies employed by different system modules. For the experimental part, evaluation results, and comparison with other related work, further reading of the full text is needed for detailed understanding.

#### Summary
This paper provides a comprehensive overview of the risk taxonomy, mitigation measures, and assessment benchmarks for large language model systems, offering a new systematic framework to help developers more comprehensively understand and deal with the potential risks of LLM systems.