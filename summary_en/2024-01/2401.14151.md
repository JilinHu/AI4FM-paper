#### Background
- **Background**
Large Language Models (LLMs) show great performance in diverse tasks but struggle with simple decision-making due to misalignment with environments.
- **Existing Work**
Current RL agents align well with environments but lack efficiency due to starting from random policies. Despite attempts to improve sample efficiency with prior knowledge, integrating this with RL to assist in decision-making remains a challenge.

#### Core Contributions
- **Proposed an online framework, TWOSOME**
  - **Challenge 1: Reducing the generation of invalid actions**
      TWOSOME uses LLMs to calculate joint probabilities of valid actions, forming reasonable behavior policies that tackle the issue of invalid actions while retaining LLM knowledge.
  - **Challenge 2: Optimizing policy stability and robustness**
      To address the observed issue that longer actions tend to have lower joint probabilities, TWOSOME introduces token and word normalization according to actions' token and word counts. Additionally, a high-efficiency training architecture with shared frozen LLM and LoRA is designed, along with principles for efficient prompt design to enhance LLM reasoning capabilities.

#### Implementation and Deployment
Extensive experiments were conducted in the classical RL environment Overcooked and the simulated household environment VirtualHome to evaluate TWOSOME. The results show that TWOSOME has much higher sample efficiency and performance compared to conventional RL method PPO and prompt tuning method SayCan across tasks. TWOSOME also demonstrates strong generalization in unseen tasks and maintains LLM's original capabilities during online PPO fine-tuning.

#### Summary
The TWOSOME framework effectively aligns LLMs with embodied environments using RL, improving sample efficiency and task generalization while retaining LLMs' original functionality.