#### Background
- **Background**
The paper discusses how Large Language Models (LLMs) benefit from chain-of-thought (COT) prompting, especially in tasks that require systematic reasoning processes. However, it also highlights new vulnerabilities associated with COT prompting, namely that the model may output malicious content when specific backdoor-triggered conditions occur during inference.
- **Existing Work**
The existing work mainly addresses backdoor attacks by contaminating the victim model's training set with instances containing the trigger or manipulating the model's parameters during deployment through fine-tuning or "handcrafting". These methods are not feasible for commercial LLMs, which are commonly accessed only through APIs.

#### Core Contributions
- **Introducing BadChain, a backdoor attack method**
  - **Challenge 1: How to conduct backdoor attacks on LLMs without accessing training datasets or model parameters**
      BadChain inserts a backdoor reasoning step into the sequence of reasoning steps produced by LLMs, thereby changing the final response when a backdoor trigger is present in the query prompt. This manipulation involves altering a subset of demonstrations to incorporate a backdoor reasoning step during COT prompting. Thus, any query prompt containing the backdoor trigger will mislead the LLM to output unintended content.
  - **Challenge 2: Effectiveness and Defense**
      The study empirically demonstrates the effectiveness of BadChain across two COT strategies, four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4), and six complex benchmark tasks involving arithmetic, commonsense, and symbolic reasoning. While showing the failure of traditional backdoor attacks in these complex tasks, it also highlights that LLMs with stronger reasoning capabilities are more susceptible to BadChain attacks. Two shuffling-based defenses were proposed, yet they were mostly ineffective against BadChain, pointing to a significant security risk for LLMs.

#### Implementation and Deployment
The effectiveness of BadChain was tested experimentally on various LLMs and complex tasks, emphasizing the urgent need for defenses against such attacks. Besides proposing and evaluating the attack method, the paper also explored potential defense strategies, which turned out to be largely ineffective against BadChain, indicating that LLMs still face serious security threats.

#### Summary
This article proposes BadChain, a backdoor attack on LLMs using COT prompting that does not require access to training datasets or model parameters and has low computational overhead. The method effectively reveals the security vulnerabilities under COT prompting in LLMs and emphasizes the importance of carrying out backdoor attacks and designing effective defenses.