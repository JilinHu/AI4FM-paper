#### Background
- **Background**
The paper discusses the growing need for intelligent systems capable of interpreting and reasoning about visual content, driving the development of Large Multi-Modal Models (LMMs) that require not only accuracy but also explicit reasoning capabilities.
  
- **Existing Work**
Existing models usually learn the alignment between images and text using large datasets and then refine their text generation capabilities for various tasks through instruction tuning with smaller, high-quality datasets. Yet, they often struggle with hallucination â€” producing outputs not aligned with the given input, such as mentioning non-existent objects in images. Furthermore, when hallucination occurs, models cannot explain their reasoning, making it challenging to identify and correct mistakes. This research aims to tackle these challenges by integrating an explicit reasoning process and the ability to generate questions during reasoning. 

#### Core Contributions
  - **Proposed an LMM using CoR**
    - **Challenge 1: Hallucination and Lack of Explanatory Capability**
      Current models encounter hallucination when generating outputs related to visual content, and they cannot explain their reasoning process, especially during hallucination. To address this challenge, the research introduces a method of integrating an explicit reasoning process and the ability to ask questions. This way, models can learn and articulate the reasoning process more openly, asking questions to gain knowledge when needed and generating questions during reasoning, pausing to obtain knowledge and then continuing the reasoning to reach more accurate and reliable conclusions.
    - **Challenge 2: Weaker Text Generation and Combatting Uncertainty**
      Since LMMs have relatively weaker text generation capabilities compared to LLMs, achieving Chain-of-Thought reasoning through prompting alone is challenging. To counter this, the research created a new dataset including the explicit reasoning process. This dataset, generated using LLMs with image annotations and a few manually created examples, trains the model to generate questions in uncertain reasoning scenarios. A novel LMM architecture trained on this dataset incorporates improved region-awareness and integrates a pre-trained image encoder and text decoder, enabling the model to generate explicit reasoning steps and ask questions when uncertain, thereby enhancing the reliability of its inferences.

#### Implementation and Deployment
The results indicate a significant advancement toward a more robust, accurate, and interpretable LMM capable of explicit reasoning and proactively seeking information when confronted with ambiguous visual input. The new dataset and model have shown substantial progress, laying the groundwork for future developments in LMMs.

#### Summary
This research innovatively incorporates an explicit reasoning process and question-generation ability into LMMs, promoting more reliable inferences. By creating a new dataset and leveraging it for model training, it sets a precedent for future advancements in LMMs and enables the model to generate explicit reasoning steps and questions when faced with uncertainty.