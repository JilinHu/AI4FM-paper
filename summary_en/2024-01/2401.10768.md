#### Background
- **Background**
The article notes that Large Language Models (LLMs) have shown outstanding performance in completing a wide variety of tasks with human assistance and alignment. However, they can still confidently generate responses that contradict context or worldly knowledge, which is termed as "hallucination".

- **Existing Work**
Existing studies have suggested that inconsistency should be alleviated by reforming alignment training data that exceeds the capability boundaries of the models. However, these methods have two primary limitations: discrepancies in the capabilities between base and aligned LLMs do not accurately reflect the inconsistency between the pretraining corpus and alignment training data; given the diversity of real-world tasks, the precise evaluation of LLMs capabilities under various scenarios remains a significant challenge.

#### Core Contributions
- **Introduced a new method called Knowledge Consistent Alignment (KCA)**
    - **Challenge 1: Knowledge inconsistency causing hallucinations**
        The paper delineates addressing the issue of hallucinations in LLMs by detecting knowledge inconsistencies in the training data. The KCA method employs a strategy of using well-aligned models to design multiple-choice questions for the training data, thus comprehensively assessing LLMs' understanding of implicit knowledge.

    - **Challenge 2: Accurate evaluation of LLMs capabilities**
        The proposed method evaluates the performance of LLMs in handling knowledge consistency issues, including strategies like open-book tuning, discarding tuning, and refusal tuning.

#### Implementation and Deployment
By employing LLMs of different backbones and scales, including Pythia 7B, Llama-2 7B, Mistral 7B, and Llama-2 13B, the effectiveness of the KCA method in mitigating hallucinations in instruction-tuning setups was tested across six benchmarks. The evaluation domains included general instruction-following, truthful question answering, search and retrieval, and clinical report generation, among others, confirmed with both metric-based and LLM-based judgments. Results show that the KCA method consistently and significantly lowers knowledge inconsistencies and, hence, the hallucination rates compared to the baseline, with certain benchmarks and base LLMs exceeding improvement by over 10 points.

#### Summary
This paper introduces an innovative KCA method that reduces the inconsistency between external and intrinsic knowledge, thereby mitigating hallucinations in LLMs during alignment. The study offers several insights for future research, notably the excellent performance of the KCA method across various scenarios and the combination of its simplicity and effectiveness.