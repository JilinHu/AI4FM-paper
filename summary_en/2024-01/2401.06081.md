#### Background
- **Background**
Researchers have proposed supervised fine-tuning (SFT) and reinforcement learning (RL) methods to better adapt LLMs to specific domains and downstream tasks after pre-training. However, existing RL methods mostly utilize instance-level rewards which fail to provide fine-grained supervision for complex reasoning tasks and cannot concentrate on the few key tokens that lead to incorrectness.

- **Existing Work**
The current RL methods are not effective in learning and correcting the key tokens responsible for errors. They use instance-level rewards that are generally inefficient in providing accurate, fine-grained supervision for complex reasoning tasks.

#### Core Contributions
- **Introduced a new RL method named RLMEC**
  - **Challenge 1: Inability to provide fine-grained supervision**
    Existing RL methods cannot offer fine-grained supervision for complex reasoning tasks focusing on key tokens causing incorrectness. The proposed RLMEC method overcomes this by employing a generative model as the reward model, trained to rewrite erroneous solutions under a minimum editing constraint, resulting in token-level rewards for intricate task supervision.

  - **Challenge 2: Unstable training process**
    The authors address the problem of stabilizing the RL process by designing a token-level RL objective based on the generative reward model and incorporating an imitation-based regularization that zeroes in on learning the key tokens involved in erroneous solutions, thus diminishing the impact of other non-critical tokens.

#### Implementation and Deployment
Experimental results demonstrate the effectiveness of RLMEC on mathematical tasks and question-answering tasks. The method significantly outperformed other competitive SFT and RL methods on complex reasoning tasks using 7B and 13B LLMs. Additionally, analysis experiments show that RLMEC can stabilize the RL training process and reduce the wrong steps in the LLMsâ€™ sampled outputs.

#### Summary
This paper presents RLMEC, a novel RL method that employs generative reward models with a minimum editing mechanism, enabling precise supervision and stability in training large language models with RL.