#### Background
- **Background**
LLMs often produce fluent and convincing explanations. However, unlike humans, they frequently generate inconsistent explanations across different inputs. For example, an LLM might generate the explanation "all birds can fly" when answering whether sparrows can fly but say "no" to the question of whether penguins can. Explanations should be consistent across related examples to allow a human to simulate the LLM's decision process across multiple examples.

- **Existing Work**
Current popular methods such as supervised fine-tuning or reinforcement learning from human feedback do not clearly solve the problem of inconsistency in the explanations provided by LLMs.

#### Core Contributions
  - **Developed Explanation-Consistency Finetuning (EC-Finetuning)**
      - **Challenge 1: Improving LLM's Explanation Consistency**
      The challenges include improving LLMs to generate more consistent natural-language explanations across related examples. EC-Finetuning was introduced to tackle this challenge by fine-tuning LLMs on synthetic data carefully constructed to contain consistent explanations.
  - **Challenge 2: Enhancing the Generalizability of LLMs into Different Domains**
      The method also aims to improve the generalizability of LLMs to out-of-distribution datasets not seen during fine-tuning. The experiments showed that EC-Finetuning yielded a 10.0% relative improvement in explanation consistency on four fine-tuning datasets and a 4.5% relative generalization to seven unseen datasets.
      
#### Implementation and Deployment
The proposed EC-Finetuning method showed effectiveness in experiments, improving the explanation consistency of LLMs across related questions and observing improvement in each dataset, in particular, substantial gains in MedQA-Diff, indicating that EC-Finetuning can also increase the LLM's explanation consistency on related questions that are dissimilar to the original questions. Moreover, these improvements in consistency were accompanied by moderate improvements in prediction accuracy, and the paper provided examples of how EC-Finetuning enhances explanation consistency.

#### Summary
The EC-Finetuning method has successfully increased the consistency of explanations generated by LLMs and demonstrated its ability to generalize to unseen datasets, showing a 10.0% relative improvement in explanation consistency on fine-tuning datasets and a 4.5% improvement on out-of-distribution datasets, along with moderate improvements in prediction accuracy.