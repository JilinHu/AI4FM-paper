#### Background
- **Background**
The paper introduces current multi-modal multi-agent systems which employ multi-modal large language models (LLMs) as the central control unit for agents and use multi-modal signals as the medium for exchanging messages between agents. Current systems face challenges in enhancing the system's multi-agent capabilities and performance in simulating human communication.

- **Existing Work**
While present multi-agent or multi-modal systems can to some extent handle complex interaction scenarios, they are typically limited to their modes and lack the ability to simulate real human communication and rich conversational scenes with emotion. Furthermore, these systems often lack realism and emotional expression when primarily interacting through speech.

#### Core Contributions
  - **Introduced a multi-modal multi-agent system named SpeechAgents**
    - **Challenge 1: Conveying authentic emotions and reasonable rhythm.**
      The challenge lies in how to simulate real human communication dialogues with correct content, authentic rhythm, and rich emotions. SpeechAgents addresses this through the use of a multi-modal language model as the central control for agents and multi-modal signals for message exchange. Experimental results show that the SpeechAgents system can simulate human communication dialogues with relatively high consistency and quality.

    - **Challenge 2: Enhancing system scalability in multi-agent scenarios.**
      The challenge is to improve system scalability as the number of agents in scenarios increases. SpeechAgents demonstrated that even in scenarios involving 25 agents, scripts of relatively high consistency and quality could be generated, showcasing the framework's strong scalability.

#### Implementation and Deployment
According to experimental results, the SpeechAgents model had high consistency (C-Score) and quality scores (Q-Score) in the simulation of multi-agent human communication. Compared to other baseline systems, such as LLaMA2-MAT and Speech-ChatGPT, SpeechAgents performed comparably in these two scores—demonstrating the effectiveness and potential of using multi-modal signals as the medium for information exchange among agents. Additionally, SpeechAgents also showed a certain level of general ability (General Ability), performing as well on speech-to-speech dialogue tasks as on the multi-agent tuning employed for human communication simulation tasks. This further confirms the contribution of multi-agent tuning to enhancing system general ability.

#### Summary
The paper proposed a multi-modal large language model-based multi-agent system—SpeechAgents, capable of simulating human communication scenarios involving up to 25 agents, exhibiting exceptional scalability. By utilizing multi-modal signals as the medium for agent communication, the system not only can simulate dialogues with correct content, authentic rhythm, and rich emotions but also can be applied to tasks such as drama creation and the generation of audio novels.