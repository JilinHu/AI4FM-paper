#### Background
- **Background**
The article notes that LLMs have become capable of processing more complex types of inputs, including structured documents and multimodal contents. However, existing methods face challenges such as high computational complexity and large memory requirements when dealing with arbitrarily long input sequences.

- **Existing Work**
Existing research has proposed the use of recurrent states or continuous memory to maintain context between chunks of an input sequence. For example, memory used to store the keys and values of an attention sub-layer allows subsequent queries to be attended to. But these methods are limited, for example, they only handle unidirectional attention and are not suitable for model structures that used bidirectional attention during pretraining.

#### Core Contributions
  - **Introducing a memory-based transformer that reduces memory size and adapts to various architectures by using eviction policies**
      - **Challenge 1: Reducing memory size while preserving performance**
        Current methods usually require a large capacity of memory to achieve the desired performance. The eviction policies such as LRA and LFA proposed in the paper, implemented at insertion time into the memory, effectively reduce the required memory size. The attention score computed is used as a proxy for importance to decide which key-value positions to keep or evict from the memory.
      - **Challenge 2: Supporting bidirectional attention and allowing for the processing of "future" content**
        Traditional methods typically only handle key-value pairs from a prior or present context and do not consider the "future" context. The proposed ATTENDRE layer introduces a wait-to-attend mechanism by retrieving key-value memory (K/V memory) with evicted queries to support the model's attention computation on "future" key-value pairs.

#### Implementation and Deployment
The TriviaQA reading comprehension task was used for experimental evaluation, and the results show that with a memory size of only 128 (as opposed to the baseline method's 2048), the strategies proposed in the paper still achieve comparable performance. Additionally, the introduction of the ATTENDRE layer enabled the original model to process the entire long sequence and achieve the performance level of the original model processing the entire long sequence.

#### Summary
The paper successfully proposes a new memory-based transformer method that effectively reduces memory demands and supports bidirectional attention through storage eviction policies and the ATTENDRE layer, demonstrating performance on par with traditional methods in long-sequence processing.