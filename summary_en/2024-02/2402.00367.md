#### Background
- **Background**
The paper discusses the issue of knowledge gaps within large language models (LLMs), where knowledge can be missing or outdated, leading to models "hallucinating" or exhibiting biases when lacking reliable information. The interaction with users, in this case, highlights the prominence of the phenomenon, suggesting that models might need to abstain (abstain) from generating outputs when they cannot provide accurate answers.

- **Existing Work**
Other research has attempted to expand the knowledge of LLMs through retrieval augmentation, search engine integration, and multi-LM collaboration. However, these methods are limited and depend on external datasets, while existing baseline methods often require a separate dataset for training and hyperparameter tuning, which could harm the generalization capabilities of the model across different knowledge domains.

#### Core Contributions
  - **Introduced research on how LLMs can abstain from answering questions**
    - **Challenge 1: Knowledge Missing**
        Models should abstain from answering when training data are missing knowledge or sourced from unreliable locations. The paper proposes two new methods based on multi-LM cooperation (COOPERATE and COMPETE), where one LLM uses feedback from other models on the proposed answer and synthesizes the outputs into an overall abstain decision.
 
    - **Challenge 2: Self-Reflection**
        Existing methods rely on the "self-reflection" assumption that a single LLM can evaluate its own generated texts. Challenges such as hallucination and confirmation bias cast doubt on this assumption. The new methods (COOPERATE and COMPETE) engage multiple LLMs in cooperation to reflect on the generated text, removing the dependence on separate datasets and enhancing the model's ability to abstain.

#### Implementation and Deployment
The evaluation of the baseline methods and the proposed cooperative approaches was conducted on four knowledge-intensive QA tasks. The experiments showed that COOPERATE and COMPETE outperform all baselines in 9 out of 12 settings across tasks and models, with an improvement of up to 19.3% in abstain accuracy. Further analysis indicated that the collaborative approaches based on multi-LLM could help identify failures in retrieval augmentation, pinpoint knowledge gaps in multi-hop reasoning, and contribute to increased reliability, reduced hallucinations, and mitigated biases of the models.

#### Summary
This article focuses on identifying knowledge gaps in large language models (LLMs) and abstaining from answering questions when necessary. The study proposes two novel multi-LLM collaboration methods, which showed through comparative experiments that they can effectively improve the ability of LLMs to abstain from generating outputs with low confidence.