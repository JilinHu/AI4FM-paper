#### Background
- **Background**
The paper discusses the abstract reasoning capabilities exhibited by large language models in mathematical domains. Pre-training and supervised fine-tuning on math problems are critical steps in building such models.

- **Existing Work**
Previous attempts lacked the integration of chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter under a unified seq2seq format to supervise the model making it a versatile math reasoner. Additionally, former studies usually required separate verification models to judge different reasoning processes and answers, which were inefficient and labor-intensive.

#### Core Contributions
  - **Introduced InternLM-Math model**
    - **Challenge 1: Enabling the model to have versatile capabilities**
      InternLM-Math unified various capabilities to serve as a comprehensive math reasoner, a verifier for further development on the next generation of math LLMs and for self-iteration.

    - **Challenge 2: Solving and proving mathematical problems**
      Explored the potential of using LEAN as a unified platform for solving and proving mathematical problems and examined its performance in a multi-task learning setting.

#### Implementation and Deployment
The model achieves open-sourced state-of-the-art performance in various informal and formal mathematical reasoning benchmarks, under the settings of in-context learning, supervised fine-tuning, and code-assisted reasoning. For instance, on benchmarks like GSM8K, MATH, Hungarian math exams, MathBench-ZH, and MiniF2F, the pre-trained model without fine-tuning reached a score of 30.3 on the MiniF2F test set. Notably, the model's pre-training performance demonstrated the potential of using LEAN to solve mathematical problems and prove mathematical concepts in a multi-task learning setting.

#### Summary
The InternLM-Math model is a mathematical reasoning tool based on LLMs that integrates various capabilities and provides supervised learning to help the model achieve state-of-the-art performance in various mathematical reasoning tasks, with code and data made open-source. The paper also explores a new approach to solving mathematical problems with the programming language LEAN within a multi-task learning setup, showcasing the potential of LLMs in formalized and code-assisted reasoning.