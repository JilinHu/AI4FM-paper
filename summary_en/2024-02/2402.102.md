#### Background
- **Background**
The paper discusses how current research aiming to enhance the reasoning abilities of large language models (LLMs) mostly focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) prompting. These methods are effective but often require extensive manual prompt engineering.
  
- **Existing Work**
Existing studies indicate LLMs are constrained in reasoning capabilities, primarily employing manual prompting and model training or instruction tuning using substantial amounts of CoT data. However, these methods fail to demonstrate LLMs' inherent reasoning abilities without external prompts.

#### Core Contributions
  - **A novel method of adjusting the decoding process to elicit CoT reasoning paths from LLMs naturally, without the need for prompting**
    - **Challenge 1: How to elicit CoT reasoning from LLMs without using prompting techniques**
      The paper introduces a new way of simply altering the decoding process to naturally elicit CoT paths from the model. It was found that examining alternative top-k tokens beyond standard greedy decoding reveals CoT paths. This approach not only avoids the complexities of manual prompting but also assesses the intrinsic reasoning abilities of LLMs.
    
    - **Challenge 2: How to differentiate between CoT and non-CoT paths and how to leverage this to increase the reliability of decisions**
      The study observes that the model displays increased confidence when decoding the final answer if a CoT reasoning path is present. Utilizing this phenomenon, a method called CoT-decoding is developed to filter more reliable decoding paths, significantly improving performance over greedy decoding on various reasoning benchmarks.

#### Implementation and Deployment
Extensive experiments across different reasoning tasks showed that the proposed CoT-decoding automatically reveals CoT reasoning paths during decoding, significantly enhancing the model's reasoning capabilities over greedy decoding. Experiments were carried out using the PaLM-2 large model, comparing standard greedy decoding paths with alternative paths that involve different top-k token choices. The CoT-decoding method not only improves the reasoning abilities of pre-trained LLMs but also allows for better performance across various reasoning benchmarks.

#### Summary
This work uncovers that by changing the decoding strategy, one can naturally elicit reasoning from pre-trained LLMs, with CoT paths being more prevalent in tasks frequently represented in the pre-training data. The introduced CoT-decoding method significantly enhances model performance on various reasoning benchmarks without the need for manual prompts.