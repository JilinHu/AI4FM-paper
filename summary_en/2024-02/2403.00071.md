#### Background
- **Background**
The paper aims to improve the capacity of large language models (LLMs) to process long texts. Current models demonstrate a lack in generalization capabilities, especially in terms of position embedding when dealing with long contexts.

- **Existing Work**
Existing research attempts to adapt position embeddings for longer sequences often require significant computational resources and have limited effectiveness. For instance, methods such as compressing position indices to fit pre-training ranges, or modifying rotary bases, assisted with fine-tuning strategies, for extended sequences.

#### Core Contributions
  - **Introduced a new technique named Resonance Rope**
    - **Challenge 1: Enhancing model's generalization capacity for long text**
      Resonance Rope is proposed to narrow the generalization gap by deeply analyzing the wavelengths of RoPE features, which improves the capability to handle long texts for RoPE and RoPE-based scaling techniques without additional computational resource requirements during runtime.

    - **Challenge 2: Creating a Suitable Benchmark**
      The paper introduces POSGEN, a new synthetic benchmark designed specifically for long-text scenarios to disentangle the complexities associated with generating longer context from challenges of recognizing new positions or position embedding values.

#### Implementation and Deployment
Resonance Rope proved to enhance performance on out-of-distribution (OOD) positions within the POSGEN benchmark, surpassing existing methods that do not include Resonance Rope. Particularly, it showed lower perplexity in upstream long sequence modeling and better outcomes in downstream tasks involving lengthy contexts. Moreover, when applied to YaRN, Resonance Rope further improved LLM's length extrapolation abilities.

#### Summary
This paper presents Resonance Rope, an improved model that enhances performance in dealing with long texts based on the analysis of RoPE position embedding feature wavelengths. It also introduces the POSGEN benchmark to assist in the study and evaluation of position embeddings in long-text tasks.