#### Background
- **Background**
Large Language Models (LLMs) have shown great potential in complex reasoning tasks by generating step-by-step rationales. However, concerns exist about their susceptibility to hallucination and flaws in the reasoning process, prompting significant efforts to enhance the reliability and faithfulness of the rationales generated.

- **Existing Work**
Current works model the reasoning process as planning or focus on annotating for process supervision. Nevertheless, planning-based search processes tend to have high latency due to the frequent assessment of intermediate reasoning states and extensive exploration space. Additionally, supervising the reasoning process with human annotations is costly and challenging to scale.

#### Core Contributions
  - **Introduced a novel offline training framework**
      - **Challenge 1: Enhancing model accuracy and faithfulness**
      The paper proposes learning planning-based reasoning through direct preference optimization (DPO) on collected trajectories ranked according to synthesized process rewards. This approach aims to improve reliability without relying on real-time assessments during the planning process.

  - **Challenge 2: Synthesizing process rewards without teacher models or human annotation**
      The paper suggests a method for synthesizing coarse-grained process rewards, developing a strategy based on partial trajectory exploration to learn a better policy for generating reliable rationales through Direct Preference Optimization.

#### Implementation and Deployment
The proposed framework trains through trajectory collection and Direct Preference Optimization based on synthesized process rewards without the need for teacher models or human annotation. The methods were evaluated on two challenging logical reasoning benchmarks, showing significant improvements over strong baseline models and verifying the effectiveness of the proposed method.

#### Summary
The paper proposes a novel offline training framework focused on improving the reliability and accuracy of Large Language Models in complex reasoning tasks through trajectory collection and direct preference optimization based on outcome supervision, without the need for teacher models or human annotations. The results on two logical reasoning benchmarks prove the effectiveness of the proposed method.