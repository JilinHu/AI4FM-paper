#### Background
- **Background**
    Multi-agent interactions among Large Language Models (LLMs) have led to substantial improvements in a variety of reasoning tasks. However, these interactions include extended dialogues over multiple rounds across several models, making them costly. Furthermore, current multi-agent approaches are unable to offer a single, efficient model for inference.

- **Existing Work**
    Existing multi-agent frameworks are usually based on proprietary models such as GPT-4, Bard, Claude, etc., capable of acting as general conversation agents and handling long contexts in accordance with instructions. Nevertheless, they are computationally and financially demanding, particularly in multi-round interactions which call for repeated long-token inference requests to the LLMs. Besides, these frameworks lack a joint final model for direct inference and require invoking all interacting LLMs during test time.

#### Core Contributions
  - **Introduced the MAGDI method**
    - **Challenge 1: Structured Distillation**
        To overcome the challenges of high costs and the absence of an efficient single model, the authors introduced MAGDI, a new structured distillation method that distills the reasoning interactions among multiple LLMs into smaller Language Models (LMs). MAGDI enhances the base student model with a graph encoder by representing multi-agent interactions as graphs and distills knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure.

    - **Challenge 2: Improved Efficiency and Generalizability**
        In experiments on seven widely adopted commonsense and mathematical reasoning benchmarks, MAGDI has proven to increase the reasoning abilities of smaller models, outperforming various methods that distill knowledge from a single or multiple teachers. Additionally, MAGDI shows an order of magnitude greater efficiency compared to its larger teacher models. Extensive analyses demonstrate that MAGDI enhances generalizability to tasks outside of its training domain, scales positively with the size and robustness of the base student model, and attains greater improvements (via multi-teacher training) when applying self-consistencyâ€”an inference technique that depends on model diversity.

#### Implementation and Deployment
The researchers developed a structured distillation method called MAGDI, whereby reasoning knowledge from multi-agent interaction graphs (MAG) generated by multiple teacher-LLMs engaging in a multi-round discussion is distilled into a base student model. The results indicate that by employing the MAGDI method, the graph-augmented student model improved by 10.71% on a math reasoning problem compared to the base model. MAGDI not only enhanced the reasoning abilities of the model but also significantly reduced computational and monetary costs, presenting an efficiency gain by an order of magnitude over the original large models, and improved generalization to new tasks.

#### Summary
This paper introduces a new method called MAGDI, which significantly enhances the reasoning abilities and generalization capacity of smaller models through structured distillation of reasoning interactions between multiple LLMs, while reducing costs.