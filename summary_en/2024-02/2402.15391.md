#### Background
- **Background**       
    The paper surveys the rise of generative AI models in recent years, capable of creating innovative and creative content. Significant advancements have been achieved, especially in language and image generation. However, despite early signs of progress in video generation, there is still a gap in interactivity and engagement levels between video generation models and language tools such as ChatGPT, or even more immersive experiences.
    
- **Existing Work**
    Current models fail to provide the same level of interactivity and engagement in video generation as existing text generation tools, nor can they generate complete interactive experiences.

#### Core Contributions
- **Introduced a generative interactive environment named Genie**
    - **Challenge 1: How to achieve controllable video generation?**
        It's costly and often infeasible to obtain traditional video annotation or action labels. Genie overcomes this by inferring latent actions in an unsupervised manner, conditioning future frame predictions on the previous frame's action. It uses a VQ-VAE-based objective to limit the predicted actions to a small, discrete set, ensuring human playability and enhancing control.

    - **Challenge 2: How to compress videos for higher quality video generation?**
        Genie utilizes VQ-VAE to compress videos into discrete tokens to reduce dimensionality. This method involves training an encoder to generate continuous latent actions, where the encoder can take all previous frames and latent actions as input to predict the next frame.

#### Implementation and Deployment
Genie's model includes three key components: 1) a Latent Action Model (LAM) for inferring latent actions between each pair of frames; 2) a video tokenizer that converts raw video frames into discrete tokens; 3) a dynamics model that predicts the next frame of the video using the latent action and past frame tokens. The model utilizes the ST-transformer architecture and uses cross-entropy loss for predicting subsequent frames during training. At inference, video generation can be controlled by consecutively specifying discrete latent actions, enabling the creation of entirely new videos (or trajectories). Furthermore, Genie was trained on a large-scale dataset collected from publicly available internet videos of 2D platformer games, containing 6.8M 16-second video clips.

#### Summary
Genie is an interactive environment model capable of generating new videos and controlling the content of the videos through user inputs, bridging the gap between traditional video generation technologies and interactive experiences.