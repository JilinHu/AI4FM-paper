#### Background
- **Background**
The paper highlights the struggle of LLMs with code generation tasks in specific applications, despite their significant advancements. The difficulty arises from the limited training data available for adapting the LLMs to particular needs, leading to subpar code generation performance. 

- **Existing Work**
Existing methods mostly rely on using large numbers of samples collected in specific scenarios for training, which is not efficient for LLMs as it requires them to relearn entire code data for new scenarios. Moreover, with insufficient data, traditional fine-tuning may lead to undesirable features affecting the model's performance.

#### Core Contributions
  - **SEED: A novel adaptation approach**
      - **Challenge 1: Insufficient samples for specific scenario adaptation**
      SEED overcomes the drawbacks of traditional fine-tuning with limited data by leveraging the errors made by LLMs as learning opportunities. Through steps like Error Code Collection, Automatic Code Revision, Model Optimization, and Iterative Adaptation, it enables efficient learning and adaptation with fewer samples.

      - **Challenge 2: Sustaining model optimization and generalizability**
      Experiments demonstrate SEED's superior performance over traditional fine-tuning methods with fewer samples, showing a relative improvement of 27.2%-325.0%. It maintains strong performance across various LLMs, highlighting its sustained improvement and generalizability.

#### Implementation and Deployment
Experimentation on the MBPP dataset proved SEED to offer remarkable improvements in performance, with a relative increase of 27.2%-325.0% in Pass@1 compared to traditional fine-tuning methods, even with fewer training samples. Additionally, SEED outperformed these benchmark methods on two public benchmarks and exhibited consistently strong performance across varied LLMs, verifying SEED's generalized efficacy.

#### Summary
This paper introduces SEED, an adaptation method using error-driven learning, enabling LLMs to learn efficiently with fewer samples for code generation tasks, achieving better performance and generalization.