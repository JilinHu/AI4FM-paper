#### Background
- **Background**
The paper identifies the costly process of training large language models (LLMs) and explores data-efficient pre-training techniques that aim to optimize the Pareto frontier between model quality and training resource/data consumption.

- **Existing Work**
Existing LLM training approaches do not focus on optimizing data selection, leading to diminishing returns with a linear increase in data and model size and facing boundaries of scaling due to high computational costs.

#### Core Contributions
- **Introduced ASK-LLM and DENSITY Sampling Techniques**
  - **Challenge 1: Costly Data Selection and Quality Estimation**
      The paper introduces the ASK-LLM technique, which leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to assess the quality of training examples directly. This method circumvents the costly data quality assessments by relying on the reasoning capabilities of LLMs rather than other expensive methods.

  - **Challenge 2: Coverage and Diversity Optimization**
      For targeting coverage, the paper proposes the DENSITY sampling technique, which models the data distribution to select diverse samples, addressing the issue of ensuring broad coverage and diversity in training data.

#### Implementation and Deployment
The comparison of 19 samplers involving hundreds of evaluation tasks and pre-training runs showed that ASK-LLM and DENSITY emerged as the best methods in their respective categories. Coverage sampling could recover the performance of full data, and models trained with ASK-LLM data consistently performed better than full-data training, even when rejecting 90% of the original dataset and converging up to 70% faster.

#### Summary
The ASK-LLM and DENSITY techniques proposed in the paper optimize the data efficiency of large language models, effectively enhancing the speed and quality of model training and performing well under resource constraints.