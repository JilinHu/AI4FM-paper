#### Background
- **Background**
Current Large Language Models (LLMs) are limited by maximum context length constraints and struggle to robustly process long inputs. In contrast, humans can read and comprehend very long texts and interactively refer back to original texts when needed.

- **Existing Work**
Efforts to improve LLMs for handling longer contexts include training or fine-tuning with extended context windows and exploring new architectures to reduce the need for such fine-tuning. However, LLMs still tend to underperform with long inputs due to sensitivity to distractive information, resulting in an effective context length thatâ€™s sometimes shorter than the explicit context window.

#### Core Contributions
  - **Introduced a Human-Inspired Reading Agent: ReadAgent**
      - **Challenge 1: Emulate human reading approach for dealing with long texts**
        By emulating how humans abstract and interact with long texts, ReadAgent creates "gist memories" that compress the essence of each page, linking it to the corresponding context, allowing the model to use global context when deciding which pages to review.
      - **Challenge 2: Effectively extending the ability to process long texts**
        ReadAgent surpasses baseline methods on three challenging long-document comprehension tasks, extending effective context lengths by 3 to 20 times with reasonable computational overhead, significantly outperforming existing LLMs in these scenarios.

#### Implementation and Deployment
Compared against methods using only the gist memory without interactive look-up, full text for datasets within the context window, and retrieval methods for page look-up, ReadAgent outperformed all baselines on the tasks while significantly increasing the effective context length compared to the original LLM and maintaining reasonable computational overhead. For instance, on the NarrativeQA Gutenberg test set, ReadAgent enhanced the LLM rating by 12.97% and the ROUGE-L by 31.98% over the best retrieval baseline, while extending the effective context length by approximately 20 times.

#### Summary
ReadAgent is an LLM agent system inspired by human reading processes, which significantly enhances performance and scalability by generating gist memories and retrieving information as needed for tasks involving long contexts.