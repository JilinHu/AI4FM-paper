#### Background
- **Background**       
The article discusses the capabilities of Large Language Models (LLMs) in understanding contextual features, emphasizing that existing evaluation methods have not fully probed the LLMs' ability to understand discourse, which involves linguistic features and structures beyond individual sentences, especially contextual understanding within different tasks.

- **Existing Work**
Most current LLM evaluation methods focus on general benchmarks and datasets but often overlook the evaluation of context understanding tasks. In addition, existing models are mostly evaluated using a limited set of benchmarks that do not include discourse-related datasets. For example, recently released models such as OPT, LLaMA, and GPT-4 have shown outstanding performance across various benchmarks and datasets, but their language understanding abilities have not been comprehensively assessed.

#### Core Contributions
- **Introduced a context understanding benchmark**
    - **Challenge 1: Design tasks, datasets, and evaluation schemes suitable for generative models**
        In order to evaluate LLMs' understanding of context, the authors collected nine existing datasets and designed four tasks. Each task is specifically designed for particular discourse understanding abilities, including coreference resolution and dialogue state tracking. These tasks provide a new perspective for evaluating the ability of LLMs to understand context.

    - **Challenge 2: Evaluating the performance of quantized models on context understanding tasks**
        Due to the exponential increase in the computational and storage costs of large models, model compression has become an important topic in research and real-world applications. The paper evaluates post-training quantized models with 3-bit quantization on context understanding tasks and for the first time, compares the performance of dense and quantized models, revealing that quantization affects the models' ability to understand context to varying degrees across tasks.

#### Implementation and Deployment
In the experiments, the authors evaluated three LLM families. The results show that in the context understanding benchmark, pre-trained dense models struggle to understand more nuanced contextual features, showing inconsistencies with other benchmarks that emphasize other aspects of language. Quantized models demonstrated varying levels of performance degradation in context understanding tasks.

#### Summary
This paper introduces a context understanding benchmark to assess the contextual understanding abilities of Large Language Models (LLMs). The benchmark encompasses the elements required for understanding context both in documents and dialogue bases, and uses innovative testing methods and experimental analysis to showcase the abilities and limitations of LLMs in understanding context.