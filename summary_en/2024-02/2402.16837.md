#### Background
- **Background**
The paper investigates whether Large Language Models (LLMs) can latently engage in multi-hop reasoning with complex prompts that do not directly provide information demands, highlighting the question whether LLMs can deduce entities like "the singer of ‘Superstition’" refers to Stevie Wonder and employ knowledge of Stevie Wonder's mother to finish a prompt.

- **Existing Work**
The current inquiry is into whether LLMs can retrieve factual information stored in their parameters to perform latent multi-hop reasoning when not explicitly fed the information within the input prompt, while earlier works demonstrated that LLMs store and retrieve simple prompt information.

#### Core Contributions
- **Proposed a framework to study latent multi-hop reasoning**
  - **Challenge 1: Designing an experimental framework**
The study introduces a framework to probe whether LLMs are engaging in latent multi-hop reasoning, analyzing if LLMs can internally recall a 'bridge entity' by indirectly mentioning it in a prompt and then examining whether enhancing this recall improves the LLM’s ability to utilize knowledge about the bridge entity.

  - **Challenge 2: Measuring the reasoning effect**
The research introduces two novel metrics, internal entity recall score and consistency score, as proxies for the degree of an LLM's recall of an entity. These metrics help evaluate the multi-hop reasoning at each step – the first hop’s entity recall and the second hop’s knowledge utilization.
#### Implementation and Deployment
Through the creation of the TWOHOPFACT dataset based on Wikidata, consisting of 45,595 dual-hop prompts of 52 types of fact composition, the study evaluates using LLaMA-2 models of sizes 7B, 13B, and 70B. Results indicated substantial evidence for first-hop reasoning in certain contextual scenarios and a clear scaling trend with model size for first-hop reasoning, but not for the second hop. Approximately 70% of experiments displayed increased internal recall of the bridge entity when the prompt was altered to indirectly mention it, while overall reasoning pathways showed moderate evidence, particularly noticeable only in the first hop.
#### Summary
This research examines LLMs’ potential for latent multi-hop reasoning, proposing new methods for evaluating latent multi-hop reasoning capabilities and indicating strong evidence of multi-hop reasoning for certain types of relational prompts in LLMs, though highly context-dependent.