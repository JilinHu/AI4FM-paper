#### Background
- **Background**
The paper discusses that Large Language Models (LLMs) store vast amounts of factual knowledge through large-scale pre-training but this knowledge is static and can become outdated. To keep LLMs up-to-date, continued pre-training on fresh documents is common.

- **Existing Work**
The first part of the paper presents extensive experiments with the Llama-2 model, exploring to what extent modern LLMs can augment their stored knowledge through continued pre-training on new documents with/without subsequent instruction-tuning. They found that even though the document perplexity is minimized, the correct answer rate to related questions by LLMs remains limited, referring to this as the "perplexity curse."

#### Core Contributions
  - **Introduced a method called pre-instruction-tuning (PIT)**
      - **Challenge 1: Perplexity Curse**
      The researchers observed that QA pairs are direct while documents are complex and cluttered. The hypothesis is that exposing LLMs to QA data before pre-training on documents would benefit encoding knowledge considering how questions access knowledge, thus countering the perplexity curse with PIT by introducing a new training stage into LLMs.

      - **Challenge 2: Continual Knowledge Acquisition**
      PIT improves the LLMs' ability to absorb information-rich document knowledge by initially training on QA pairs and then training on QA pairs associated with documents. The researchers created a dataset named Wiki2023, which showed that PIT significantly enhances the ability of LLMs to absorb new document knowledge, boosting QA accuracy by 17.8% in Llama-2 7B and 16.3% in Llama-2 70B.

#### Implementation and Deployment
PIT essentially alters the conventional process of continuing pre-training and instruction-tuning by training the model to first grasp how knowledge is accessed through questions and then learn to encode knowledge from documents. Extensive experiments on the Wiki2023 dataset reveal that PIT led models to prioritize learning access to knowledge before encoding it from documents. This new approach significantly boosted the performance of LLMs and showed promise in cross-domain generalization, providing practical evidence for further scaling up to a wider variety of documents and instructions.

#### Summary
The paper introduces a method called pre-instruction-tuning (PIT), which effectively improves the ability of LLMs to absorb knowledge from documents, addresses the "perplexity curse," and makes significant strides in multi-domain knowledge acquisition.