#### Background
- **Background**
The paper discusses how to align the confidence LLMs have in their generated answers with the actual quality of those responses. Existing approaches involve asking LLMs to produce top k predictions for a query, each prediction paired with a probability value reflecting the model's confidence in that prediction. However, these methods lack a ground-truth standard for confidence assessment, which prevents accurate reflection of the model's confidence in the quality of its answers.

- **Existing Work**
Existing work does not overcome the challenge of lacking an objective real-world standard for appraising model confidence levels, leading to ineffective alignment between confidence and answer quality.

#### Core Contributions
- **Introduced a confidence-quality alignment approach (CONQORD)**
    - **Challenge 1: Lack of an objective standard to evaluate model confidence**
        To address this challenge, the paper adopted a Reinforcement Learning (RL) framework and established a simple preliminary alignment strategy as well as the more advanced CONQORD method to optimize the alignment between confidence and answer quality. This approach reduces dependency on labeled data and enables the model to optimize its confidence levels through self-assessment in the absence of an objective standard.

    - **Challenge 2: Existing methods may introduce bias**
        By introducing a new dual-component reward strategy, the paper overcomes potential biases that could arise from manually assigned confidence scores, increasing the model's accuracy and neutrality in assessing its own confidence.

#### Implementation and Deployment
The evaluation results show that the CONQORD method exhibited strong confidence alignment performance on the TruthfulQA and NQ datasets, outperforming existing methods such as Vanilla and Top-k on metrics like Expected Calibration Error (ECE), Pearson Correlation Coefficient, and Spearman's Rank Correlation Coefficient. This indicates that the method allows the model’s predictions to be closely aligned with their actual quality, while maintaining the baseline model’s performance and significantly improving calibration. However, compared to the CoT prompt which enhances performance, the method still has room for improvement, which will be a focus for future research efforts.

#### Summary
This paper presents a method for aligning confidence and answer quality through reinforcement learning (CONQORD). It optimizes confidence levels through self-assessment in the absence of an objective standard, reducing bias and improving the accuracy and alignment of model predictions, though further improvements are needed to match the performance of more effective methods.