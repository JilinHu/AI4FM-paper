#### Background
- **Background**
Large Language Models (LLMs) face strict requirements concerning memory, latency, and power. Dynamic sparsity has been proposed to reduce computation on an input-by-input basis by skipping entire layers' computation. Dynamic layer sparsity allows networks to learn the function residual of the main branch, integrates skip-connections in design and training, enabling efficiency gains. Researchers propose Radial Networks that utilize dynamic sparsity and are guided by a well-trained router module to support arbitrary layer routing.

- **Existing Work**
Prior work focused on dynamic sparsity along network width or depth sparsity using early exit, which involves special training and can only skip later layers that contribute most to the network output. The new opportunity comes with the realization that in large models, each layer contributes less to the output, paving the way for dynamic layer sparsity.

#### Core Contributions
  - **Introduced a new neural network structure**
    - **Challenge 1: How to exploit the property of layer sparsity for efficiency**
        Modern large language models contribute little to output per layer, and the proposed Radial Networks can employ token-level routing between layers guided by a trained router module to reduce the overall resources required for generating entire sequences, lowering compute and serving costs.

    - **Challenge 2: Designing a router that can adaptively route**
        The router is learned through a small multi-layer perceptron (MLP) model, mapping intermediate embeddings to output router logits processed by a softmax function into probabilities, and then the layer with the highest probability is chosen as the next layer for the forward pass. This dynamically adjusts the network depth without increasing the maximum number of layers.

#### Implementation and Deployment
Experimental outcomes show that for the OPT-13B model, most layers skipped during token generation are in the earlier part of the network. The median residual ratio is only 20% for OPT-125M and drops to 5.9% for OPT-66B, indicating that layer sparsity opportunities broaden with model size. Furthermore, the data suggest that the residual ratio is more closely related to the number of model parameters, not just layers. Radial Networks, by varying the compute per token, provide an efficient methodology when generating the entire sequence.

#### Summary
The paper introduces Radial Networks, a novel neural network architecture that uses dynamic layer sparsity and a trained router module for token-level inter-layer routing. This not only enhances model performance but also significantly reduces computational and serving costs, facilitating further scaling of large language models.