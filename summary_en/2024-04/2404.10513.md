#### Background
- **Background**
The paper discusses the performance of Large Language Models (LLMs) in question-answering (QA) tasks, noting that although these models achieve state-of-the-art performance, they tend to "hallucinate" information in their responses. To address this issue, the research team introduces a Chain-of-Thought Attribution Reasoning approach (CoTAR), aiming to enhance the accuracy of attributions. This approach focuses on generating an attribution-centric output.

- **Existing Work**
Existing work has attempted to enhance the credibility of models by pairing the generated text with supporting evidence, facilitating trustworthiness and easier error detection. This pairing process can be done at various levels, such as pairing each sentence in the response with a set of passage identifiers or correlating individual spans in the answer with specific passages. However, these methods sometimes fail to cite accurately, focusing too much on specific parts of the input or citing too many passages which may not be relevant to the answer.

#### Core Contributions
  - **Introduced the CoTAR Method**
    - **Challenge 1: Improving the quality of answers and the accuracy of attributions**
      The new method involves reasoning over input passages before generating the output, extracting relevant information to specify the form of attribution being generated. This can be done at the span, sentence, or passage level. The multi-step CoT reasoning scheme enhances the LLMs' ability to generate answers of better quality and more precise and faithful attributions from the source.
    - **Challenge 2: Evaluating the model's answer and citation quality**
      For answer quality assessment, the study uses the n-gram-based ROUGE-L method and the semantic BERTScore, with the HEM1 model fine-tuned on NLI datasets for hallucination evaluation. It also proposes specific metrics for each citation level to measure the quality of cited content. The proposed quantitative indicators evaluate not only the quality of the answer but also the quality of the citation and even specific performance for different citation levels.

#### Implementation and Deployment
The approach proposed in the paper was evaluated on two context-enhanced question-answering datasets: QuoteSum (QSUM) and MS MARCO (MS). The experiments utilizing GPT-4 show improvements in answer quality and attribution correctness with CoTAR, and the results indicate that smaller LLMs can achieve or surpass GPT-4's performance in some instances with fine-tuning.

#### Summary
The CoTAR method proposed in this paper addresses the issue of LLMs tending to produce inaccurately attributed content in question-answering tasks. By reasoning prior to output generation and guiding the model at different levels of attribution granularity, the method significantly improves the model's performance in terms of answer quality and attribution accuracy.