#### Background
- **Background**
Large Language Models (LLMs) have been applied in diverse fields, yet their deployment on GPU servers is constrained by significant financial and energy costs due to high compute and memory demands. Although acceleration solutions exist enabling deployment on consumer GPUs, they sacrifice accuracy. Advancing these models to work efficiently on mobile or edge devices is still a focus of ongoing research.

- **Existing Work**
A large body of work in accelerating LLMs is centered around reducing non-zero weights, bits per weight, heads per layer, often requiring specialized hardware or software. Fewer studies concentrate on reducing the number of layers needed during inference by early exiting, an approach that doesn't need specialized hardware.

#### Core Contributions
  - **Introduced LayerSkip, an end-to-end solution**
    - **Challenge 1: Accelerating LLMs without losing accuracy**
      LayerSkip informs a robust training strategy to early exit at earlier model layers, creating different-sized sub-models within the same model, improving early exit accuracy without additional layers or modules as shown in experiments.

    - **Challenge 2: Improving inference speed and reducing memory footprint**
      LayerSkip proposes a novel self-speculative decoding technique that exits early and verifies with the remaining layers, achieving lower memory usage and benefiting from shared computation and activations between the draft and verification stages. This approach has been demonstrated to accelerate tasks between 1.34× to 2.16×.

#### Implementation and Deployment
Experimental findings reveal that implementing LayerSkip leads to significant inference speedups across various Llama model sizes and training regimes, including pre-training from scratch, continual pre-training, domain-specific fine-tuning, and task-specific fine-tuning. It achieves a speedup of up to 2.16× on summarization tasks for CNN/DM documents, 1.82× on coding tasks, and 2.0× on the TOPv2 semantic parsing task, indicating the efficacy of LayerSkip in boosting the efficiency of LLMs.

#### Summary
LayerSkip presents a novel, practical solution that significantly accelerates inference in LLMs without compromising on accuracy, showcasing its potential in real-world applications.