#### Background
- **Background**
The paper discusses that while recent pointwise Large Language Model (LLM) rankers have achieved notable ranking results, they suffer from two significant drawbacks: a lack of standardized comparison guidance and difficulty in making comprehensive considerations for complex passages. Thus, a solution is proposed to improve ranking by generating criteria from various perspectives.

- **Existing Work**
LLM rankers, especially pointwise rankers despite their scalability and interpretability, face consistency issues and fail to assess passages comprehensively.

#### Core Contributions
  - **Introduced MCRanker**
    - **Challenge 1: Generating Consistent Evaluation Criteria**
The model simulates a professional human annotation process, constructing a virtual team with diverse domain expertise to generate multiple evaluative perspectives and guide scoring.

    - **Challenge 2: Improving Ranking Performance**
MCRanker uses the original system of evaluative criteria to provide consistent and comprehensive assessments, whose superior performance was demonstrated across 8 datasets in the BEIR benchmark.

#### Implementation and Deployment
The paper tested MCRanker on eight datasets from the BEIR benchmark, showing that the multi-perspective criteria ensemble approach significantly enhanced the performance of pointwise LLM rankers.

#### Summary
The paper presents the MCRanker model, which improves consistency and comprehensiveness of LLM rankers by creating a virtual professional annotator team and generating evaluative criteria from multiple perspectives, capable of adapting to various datasets and improving ranking performance.