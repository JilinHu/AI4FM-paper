#### Background
- **Background**
The paper highlights the importance of event understanding, which involves inferring and predicting future events based on certain relations and is vital to various NLP applications. It also points out that current smaller instruction-tuned models lack the ability to handle such tasks efficiently due to a lack of explicit modeling of events and their interconnections within their instruction data.

- **Existing Work**
Recent research has been focused on instruction-tuning language models to enable abilities like zero-shot inference, but they did not explicitly model the events and their inter-relations during training, leading to poor performance in most event reasoning tasks. These models have a limited understanding of event structures, semantics, and relations between events.

#### Core Contributions
  - **Introduced the EVIT model**
      - **Challenge 1: Understanding of event structures and semantics**
      Existing models had limited event reasoning ability due to inadequate understanding of event structures and semantics and a gap between the model’s interpretations and human comprehension of events. EVIT improves understanding of events by proposing a new event-centric structure named event quadruple and developing event-relation learning based on the quadruples, addressing this shortfall.

      - **Challenge 2: Insufficient understanding of relations between events**
      Existing models lacked effective reasoning and integration of event knowledge due to poor comprehension of relations between events. EVIT enhances the model’s grasp on event relations and event reasoning abilities by integrating event-relation learning into the instruction-tuning process.

#### Implementation and Deployment
EVIT employs a novel structure "event quadruple" and "event-relation learning" to enhance the understanding of event structures and semantics. An event quadruple includes two events, their relation, and the background information where the facts hold. The researchers trained the EVIT model to predict tail events of event quadruples in both generative and discriminative manners. The training process was encapsulated into instruction tuning with generated instruction templates. For the training implementation, they constructed event quadruples from a large-scale textual corpus, and they designed a heuristic negative events mining algorithm to construct candidate events for discriminative event-relation training. Extensive experimentation across 8 datasets demonstrated EVIT to outperform other instruction-tuned models on unseen event reasoning tasks.

#### Summary
EVIT addresses the shortcomings of current smaller instruction-tuned models in event reasoning tasks by introducing Event-Oriented Instruction Tuning and the concept of event quadruples. The experimental results show that EVIT performs better on event reasoning tasks compared to other models.