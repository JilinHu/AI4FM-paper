#### Background
- **Background**
The paper investigates the self-play training process of LLMs in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate regarding a target word known only to the attacker. The attacker's goal is to prompt the defender to say the target word without realizing it, while the defender attempts to deduce the target word from the attacker's statements. To win, both players must possess ample knowledge about the target word and a high level of reasoning ability to infer and express in this conversation with limited information.

- **Existing Work**
Previous work addressing the challenge of reasoning in LLMs includes efforts in Chain-of-Thought prompt engineering, the use of auxiliary reasoning tools, and approaches based on post-pretraining and fine-tuning. These methods often require additional prompt designs, which can be inconsistent and sensitive to differences in prompt patterns and LLM checkpoints. Additionally, they depend on high-quality textual data and the labor-intensive process of human annotation. Self-improvement methods that utilize model-generated synthetic data have been gaining attention, but they typically need a vast range of high-quality question queries to prevent overfitting and may lack objectivity in the self-improvement process, potentially reinforcing LLM's cognitive biases if they start with incorrect or biased concepts.

#### Core Contributions
  - **Introducing a novel training scheme named SPAG**
    - **Challenge 1: Enhancing the reasoning ability of LLMs**
      The paper proposes a novel approach of enhancing reasoning capabilities in LLMs through self-play of an adversarial language game. The method circumvents the need for additional prompt designs and human data annotation.

    - **Challenge 2: Objectivity and generalization of self-improvement methods**
      The exploration of enhancing reasoning abilities through self-play in adversarial language games, like Adversarial Taboo, which autonomously generate game outcomes, thus overcoming the subjective pitfall that could afflict self-improvement methods.

#### Implementation and Deployment
The self-play training used here leveraged open-source LLMs, LLaMA-2-7B and Baichuan-2-13B, and selected target words from a top-50K frequency words list. Reinforcement learning applied to the game outcomes revealed substantial improvements in LLM performance across a wide array of reasoning benchmarks. Importantly, iterating through the self-play process allowed continuous enhancements in LLM reasoning capabilities.

#### Summary
This paper proposes an innovative training scheme named SPAG that effectively enhances the reasoning capabilities of LLMs through self-play in adversarial language games and demonstrates that these improvements can persist and amplify through the iterative process.