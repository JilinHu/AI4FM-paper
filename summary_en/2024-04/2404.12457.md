#### Background
- **Background**
The article introduces the significant advancements of Retrieval-Augmented Generation (RAG) in combining the strengths of large language models (LLMs) with external knowledge databases for various natural language processing tasks. However, RAG leads to long sequence generation, resulting in high computational and memory costs.

- **Existing Work**
Current works have focused mainly on optimizing LLM inference systems, achieving significant progress by sharing intermediate states between inferences to reduce recomputation costs. However, they have not adequately addressed the specific features of RAG nor the increased sequence in augmented requests, which affects performance due to the additional computation required for these requests.

#### Core Contributions
  - **Introduced RAGCache**
    - **Challenge 1: Long sequence generation caused by knowledge injection**
        RAGCache addresses this challenge by organizing the intermediate states of retrieved knowledge into a knowledge tree, caching them in the GPU, and hierarchy of host memory.

    - **Challenge 2: Increased processing speed for multiple requests**
        RAG systems are sensitive to the order in which documents are retrieved. RAGCache introduces a Prefix-aware Greedy-Dual-Size-Frequency (PGDSF) replacement strategy and cache-aware request scheduling to maximize cache hit rates and minimize end-to-end latency.

#### Implementation and Deployment
RAGCache's evaluations on various datasets and representative LLMs demonstrate its superior performance over the state-of-the-art vLLM integrated with Faiss, increasing the Time to First Token (TTFT) by 4 times and throughput by 2.1 times. Additionally, compared to SGLang, which optimizes the reuse of intermediate states in GPU memory, RAGCache reduces TTFT by up to 3.5 times and increases throughput by up to 1.8 times.

#### Summary
RAGCache enhances the performance of the RAG process by designing a targeted caching system and sharing intermediate states, significantly improving processing speed and reducing computational resource overhead.