#### Background
- **Background**
The paper introduces a mechanism for self-improvement of Large Language Models (LLMs) through imagination, searching, and criticizing their outputs, without the need for additional annotated data.

- **Existing Work**
Existing research struggles with challenges in applying AlphaGoâ€™s self-learning algorithms to LLMs, which include data scarcity, complexity of search spaces, and the nuanced nature of feedback. Current studies have not addressed how LLMs can effectively self-improve without additional annotations.

#### Core Contributions
  - **Introduced the ALPHALLM framework**
      - **Challenge 1: How to enhance LLMs without additional annotations?**
      The introduced ALPHALLM framework integrates Monte Carlo Tree Search (MCTS) with LLMs, enabling models to self-improve by imagining, searching, and criticizing their outputs without extra annotations.
      
      - **Challenge 2: Effective searching and self-improvement**
      The paper designs multiple critique signals to effectively guide the MCTS search process and leverages the feedback from MCTS outcomes to continue training the LLM, achieving efficient self-improvement.

#### Implementation and Deployment
According to the experimental results, employing the ALPHALLM framework significantly enhanced the performance of LLaMA-2 70B on mathematical reasoning tasks, reaching levels comparable to GPT-4. The paper innovatively uses MCTS to guide the training data and its outputs have much better quality than standard nucleus sampling, ensuring that the LLM can self-improve.

#### Summary
The paper presents a novel framework called ALPHALLM that, by integrating Monte Carlo Tree Search (MCTS) with Large Language Models (LLMs), facilitates the self-improvement of LLMs without the need for additional annotated data.