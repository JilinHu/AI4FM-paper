#### Background
- **Background**
Large Language Models (LLMs) are widely used in machine learning and have a wide range of applications. However, their training on vast datasets containing toxic content makes them susceptible to learning and generating toxic behavior, like generating harmful or offensive content. To cope with this, LLMs undergo a process called safety alignment, which adjusts them to generate more beneficial and safer responses. Despite this, LLMs remain vulnerable to adversarial jailbreaking attacks that can bypass these safety mechanisms.

- **Existing Work**
Manual red teaming for detecting these vulnerabilities is inefficient and time-consuming. Additionally, the automatic generation of adversarial prompts commonly produces prompts that are semantically meaningless and can be easily spotted by perplexity-based filters. These automatic methods may also demand gradient information from the Target LLM or are non-scalable due to the long processing times required.

#### Core Contributions
  - **Introduced an LLM named AdvPrompter**
    - **Challenge 1: Fast generation of readable adversarial prompts**
      Challenge Description: Automating the creation of semantically meaningful adversarial prompts to effectively deceive LLMs is difficult. AdvPrompter is trained with a new algorithm that enables the generation of human-readable adversarial prompts in seconds, which is about 800 times faster than existing optimization-based approaches, effectively addressing the challenge of speed and semantic coherence.

    - **Challenge 2: Does not require gradient information from the Target LLM**
      Challenge Description: Existing methods for generating adversarial prompts might need access to the gradient information of the Target LLM, which is not practical for many closed-source or commercial applications. The training algorithm used by AdvPrompter does not need gradient information from the Target LLM, making it more applicable to a wider range of scenarios.

#### Implementation and Deployment
The training of AdvPrompter involves alternating between (1) optimizing AdvPrompter's predictions to generate high-quality target adversarial suffixes and (2) low-rank fine-tuning of AdvPrompter with the generated adversarial suffixes. The well-trained AdvPrompter produces suffixes that mask the input instruction without changing its meaning, enticing the Target LLM to provide a harmful response. The experimental results on open source Target LLMs show that this method achieves state-of-the-art results on the AdvBench dataset and transfers well to closed-source black-box LLM APIs. Moreover, it is demonstrated that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more resistant to jailbreaking attacks while maintaining performance, that is, high MMLU scores.

#### Summary
This paper presents a novel LLM called AdvPrompter that uses an innovative algorithm to rapidly generate human-readable adversarial prompts without the need for gradient information from the Target LLM. It significantly accelerates prompt generation while maintaining semantic coherence, and additionally through training with AdvPrompter, it can enhance the robustness of LLMs against jailbreaking attacks without sacrificing performance.