#### Background
- **Background**
The paper highlights the problem faced by existing Large Language Models (LLMs) in efficiently processing extremely long input sequences with limited memory and computational resources. Current Transformer models can only handle sequences of finite length, which becomes a bottleneck for tasks that contain complex structures or require long-term dependencies.

- **Existing Work**
There have been attempts to overcome the sequence length limitations by using compressive memory systems, but effective, practical compressive memory techniques that strike a balance between simplicity and quality have yet to be seen in the current state of LLMs.

#### Core Contributions
  - **Introduced an attention mechanism called Infini-attention**
    - **Challenge 1: Handling infinitely long inputs**
      The Infini-attention mechanism integrates compressive memory into the standard attention mechanism, supporting masked local attention and long-term linear attention mechanisms within a single Transformer block. This enables LLMs to naturally extend to infinite contexts through continuous pre-training and fine-tuning.

    - **Challenge 2: Limitations of memory and computational resources**
      The approach allows Transformer models to process extremely long inputs with bounded memory and compute resources in a streaming fashion. By reusing key-value states, Infini-attention is able to build both global compressive and local fine-grained states, providing an efficient attention mechanism for each attention layer.

#### Implementation and Deployment
In experiments, this work demonstrated that their approach outperforms baseline models on long-context language modeling benchmarks with a 114x compression ratio in terms of memory size. The model achieved better perplexity when trained with 100K sequence length. A 1B LLM scaled naturally to 1M sequence length and solved the passkey retrieval task when Infini-attention was injected. Finally, an 8B model with Infini-attention reached a new state-of-the-art result on a 500K length book summarization task after continual pre-training and task fine-tuning.

#### Summary
This research proposed a novel attention mechanism, Infini-attention, which combines compressive memory with standard dot-product attention and, by design, supports plug-and-play continuous pre-training and long-context adaptation, enabling LLMs to handle infinitely long contexts with bounded memory and computational resources.