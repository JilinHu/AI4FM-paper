#### Background
- **Background**
The paper discusses the capability of Large Language Models (LLMs) to perform a wide range of tasks using In-context Learning (ICL) without updating millions of parameters. The precise contributions of demonstrations to improving the end-task performance have not been thoroughly investigated in recent analytical studies.

- **Existing Work**
Existing work attempts to unravel the mechanisms behind ICL characteristics, such as how models can "recall" latent knowledge acquired during pre-training through ICL. However, these studies mostly focus on the correctness of input-label mapping within the demonstrations and do not provide definitive answers about the specific factors that lead to performance improvements.

#### Core Contributions
  - **Introduced a method to answer key factors contributing to ICL's performance:**
    - **Challenge 1: Decomposing ICL's performance contribution**
      The authors identify all responses (with and without ICL) and track the change in categories for all instances. They propose decomposing the overall performance enhancement facilitated by ICL into three contributing factors: label space, label format, and discrimination. By comparing the outputs with and without ICL, they propose methodologies to assess these contributing factors.

    - **Challenge 2: Understanding the mechanism for retrieving good demonstrations**
      The study finds that using semantically similar examples significantly enhances ICL performance. Hence, the authors dig deeper into how retrieval aids ICL and how to select the best demonstrations using semantically meaningful sentence embeddings and similarity retrieval.

#### Implementation and Deployment
The study utilized four general-purpose and instruction-tuned LLMs and measured the three contributing factors across multiple classification, sequence labeling, and generation datasets. The findings revealed that ICL is notably effective in regulating label space and format but yields the least improvement in eliciting discriminative knowledge within semantically-rich contexts. Furthermore, the analysis of retrieving good demonstrations underscores the importance of choosing diverse and semantically relevant demonstrations to boost ICL performance.

#### Summary
The paper investigates the mechanisms by which ICL improves task performance, identifying label space regulation and format refinement as significant contributors to performance enhancement while emphasizing the importance of selecting appropriate demonstrations.