#### Background
- **Background**
The paper discusses challenges that large language models (LLMs) face in long-context learning and argues that existing long text task evaluations and metrics do not accurately reflect the models' capabilities in handling realistic long-sequence tasks.

- **Existing Work**
The existing work focuses on reasoning or information retrieval tasks, typically only requiring models to understand a very long input sequence and produce a relatively short segment of text, such as an answer or summary. These tasks fail to fully demonstrate the models' understanding and reasoning over the entire input sequence.

#### Core Contributions
  - **Developed a benchmark named LongICLBench**
    - **Challenge 1: Assessing long-context learning ability**
      To address this challenge, the researchers propose to apply extreme-label classification tasks to LLMs to evaluate their long-context abilities. Such tasks require LLMs to scan the entire input sequence to recognize the task and understand the label space, thus pushing LLMs to demonstrate their comprehension of the entire input sequence.
    
    - **Challenge 2: Evaluating the performance of existing long-context models**
      The researchers compiled the LongICLBench benchmark, which contains six tasks of varying difficulty levels (in terms of context length and label space) to evaluate the performance of 13 recent long-context LLMs. They discovered that as the difficulty of the task increased (requiring longer demonstrations), the performance of the models uniformly decreased. This evaluation provides insights that can inform the design of better long-context LLMs.

#### Implementation and Deployment
The research team evaluated the performance of 13 different long-context LLMs on LongICLBench. The results showed that as the difficulty of the tasks increased (requiring longer demonstrations), all models' performance declined, with some models even showing a linear degradation with input length. Further analysis of label position distribution was conducted, revealing that this significantly impacts the long-context learning capability of the models. For example, the performance of the GPT4-turbo model was greatly affected by the position distribution of instances in the prompts.

#### Summary
This study introduces a new benchmark, LongICLBench, for evaluating the ability of large language models to process long-context tasks and indicates that as the difficulty of the tasks increases, LLMs' performance generally decreases, with the models' long-context learning ability being affected by the distribution of label positions in prompts.