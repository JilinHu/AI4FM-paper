#### Background
- **Background**
The paper has highlighted the importance of understanding non-literal meanings for Large Language Models (LLMs) to become human-like communicators. There is a noticeable gap in evaluating LLMs for pragmatic understanding in languages other than English, especially in contexts involving extensive dialogue turns that are commonplace in daily communications.
  
- **Existing Work**
The existing approaches predominantly utilize binary or multiple-choice questions to assess LLMs, which does not exploit the full capabilities of current LLMs that can generate explanatory texts to directly evaluate their quality of understanding.

#### Core Contributions
  - **Introduced SwordsmanImp Dataset**
      - **Challenge 1: Understanding of Conversational Implicature in Chinese Multi-Turn Dialogues**
      The SwordsmanImp dataset emphasizes the comprehensibility of conversational implicature in multi-turn dialogues in Chinese. It focuses on a sentence with non-literal meaning within a context and provides one correct pragmatic interpretation among other possible misinterpretations. The challenge is whether LLMs can recognize and explain this non-literal meaning, and the researchers used expertly chosen dialogues to test the comprehension capabilities of LLMs.
      - **Challenge 2: LLMs' Handling of Implicatures Derived from Different Gricean Maxims**
      The researchers meticulously annotated which of the Gricean maxims were violated in each conversation. The study investigates whether there are significant differences in how LLMs handle implicatures from different maxims, and what the difficulties are in understanding and explaining them.

#### Implementation and Deployment
Upon testing several closed-source and open-source LLMs, the results indicated that GPT-4 achieved human-level accuracy (94%) on multiple-choice questions. CausalLM follows with an accuracy of 78.5%. The accuracy for other models ranged from 20% to 60%. Additionally, human raters were tasked to rate the explanations generated by LLMs in terms of reasonability, logic, and fluency. While all models were capable of generating fluent and self-consistent text, their explanations were scored lower on reasonability, except for GPT-4, suggesting most LLMs cannot provide satisfactory explanations for conversational implicatures.

#### Summary
The study introduces a novel Chinese multi-turn dialogue dataset, SwordsmanImp, for evaluating the capabilities of LLMs in understanding implicatures within dialogues involving a lot of context and turn-taking, revealing the challenges and limitations of LLMs in understanding and explaining non-literal meanings.