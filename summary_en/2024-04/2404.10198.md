#### Background
- **Background**
The paper discusses a significant problem faced by large language models (LLMs): they are prone to hallucinations and typically can only respond with knowledge contained within their training corpus, leaving them unable to address questions about recent events or information restricted to the public eye. To address this, Retrieval Augmented Generation (RAG) is commonly used to provide relevant retrieved content and improve LLMs’ accuracy.

- **Existing Work**
Despite the usage of RAG to address hallucinations in LLMs and provide timely knowledge, most evaluations of LLMs’ capabilities continue to be performed in non-RAG settings, overlooking the potential for drastic differences between a model's default and RAG-enabled responses. Furthermore, considering the ever-changing nature of web results, which may contain outdated or incorrect information, there is a need for objective evaluation of RAG-enabled LLM behavior, especially as RAG systems are increasingly relied upon to provide factual information in various domains.

#### Core Contributions
- **Introduced a method to quantify the tension between RAG and LLMs' internal knowledge**
    - **Challenge 1: How mismatched information affects LLMs**
        When LLMs' internal knowledge and provided retrieval information disagree, the study evaluates how the model decides. The analysis indicates that the likelihood of the LLM to rely on retrieval information presented in the context (RAG preference rate) is inversely correlated with the model's confidence in its response without context (its prior probability). LLMs tend to revert back to their priors when the original context is progressively modified with unrealistic values.

    - **Challenge 2: Lack of evaluation frameworks**
        There is currently a lack of systematic exploration of a model's confidence (evaluated via log probabilities) and its preference for RAG-provided information. This study, by testing LLMs on answering questions and measuring token probabilities while introducing different levels of perturbations in reference documents, aims to tease apart these two competing forces.

#### Implementation and Deployment
The research evaluates GPT-4's ability to manage question-answering, particularly when RAG documents involved in the QA are introduced with varying levels of perturbations. With a core dataset comprising 1,294 questions across six different domains, the study demonstrates that models are more likely to repeat incorrect, modified information when their internal prior is weaker, and more resistant when their prior is stronger, showing an inherent tension between LLMs’ pre-trained knowledge and the content provided by RAG in the context.

#### Summary
This paper analyses the tension between LLMs’ internal knowledge and retrieved information in RAG settings, finding that LLMs’ tendency to follow RAG information is inversely correlated with the model's confidence in its response without context. The research, which spans six domain datasets with over 1200 questions, reveals the inherent conflict between the model's pre-trained knowledge and the retrieved information.