#### Background
- **Background**
Current benchmarks fail to represent the diverse and complex nature of real-world computer use, limiting the scope of tasks and scalability of agents. These benchmarks offer datasets without executable environments and their non-execution-based evaluation wrongly penalizes agents for not following a single solution approach.
- **Existing Work**
Existing benchmarks lack interactivity and are restricted to specific applications or domains, limiting agents' capability to handle the diversity of real-world tasks. These benchmarks miss opportunities to fully evaluate agents on essential interactions and performance across various tasks due to their restrictive assumptions and evaluation methods.

#### Core Contributions
- **Introduced OSWORLD, a new evaluation environment**
    - **Challenge 1: Effective Evaluation**
       OSWORLD supports task initialization, execution-based evaluation, and interactive agent learning for various real operating systems like Ubuntu, Windows, and macOS. It uses virtual machine technology for a safe, isolated environment and incorporates snapshots for efficient environment resets. The setup configuration and custom evaluation scripts enable reliable and reproducible task assessments.

    - **Challenge 2: Real and Open-Ended Tasks**
      The researchers constructed a benchmark consisting of 369 computer tasks involving real web and desktop applications, OS file I/O, and workflows across multiple applications. These tasks, derived from real-world use cases, include comprehensive initial state setup configurations and tailored execution-based evaluation scripts.
   
#### Implementation and Deployment
Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWORLD uncovered significant challenges in serving as computer assistants. While humans could accomplish over 72.36% of the tasks, the best-performing model only succeeded in 12.24%, primarily struggling with GUI grounding and operational knowledge. OSWORLD's analyses provide valuable insights for the development of multimodal generalist agents not possible with previous benchmarks.

#### Summary
OSWORLD offers a novel evaluation environment addressing the limitations of existing benchmarks, laying the groundwork for the development of multimodal agents capable of performing open-ended tasks in real computer environments.