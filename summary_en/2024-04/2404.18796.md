#### Background
- **Background**
   The paper introduces how Large Language Models (LLMs) have advanced beyond our capability to accurately assess their quality. Not only is it difficult to find data that probes specific model properties, but evaluating the correctness of free-form generation is challenging.

- **Existing Work**
   Using a single large model like GPT-4 as a judge manifests biases and cost issues, especially since such models tend to favor their own outputs. Evaluator models tend to favor universally capable models, which are slow and expensive, limiting their applicability and accessibility.

#### Core Contributions
  - **Introduced the Panel of LLm evaluators (PoLL)**
      - **Challenge 1: Assessing LLM Quality**
          The paper proposes the PoLL methodology, which consists of a larger number of smaller models from different families to evaluate LLM outputs, showing better performance compared to using a single large judge, reducing intra-model bias and cost up to sevenfold in terms of efficiency.

      - **Challenge 2: Applicability Across Various Evaluation Settings and Model Families**
          The paper tested the performance of PoLL in various settings, including single-hop, multi-hop QA tasks, and Chatbot Arena, focusing on whether the method is applicable to a broad array of contexts and model families, verifying PoLL as an effective alternative for evaluating large models in these settings.

#### Implementation and Deployment
Experiments show that using PoLL for ranking has a higher correlation with human judgment on QA datasets and Chatbot Arena. For example, in various single-hop QA tasks on the KILT dataset, PoLL had the strongest agreement with human judges as measured by Cohen’s kappa. Additionally, PoLL demonstrated consistently strong performance across multiple tasks and datasets while reducing scoring bias between models. Moreover, the cost of using a specific instance of PoLL is significantly lower than that of a single large model, with costs reported as $1.25 per input + $4.25 per output for PoLL compared to $10 per input + $30 per output for GPT-4 Turbo.

#### Summary
The paper develops a new method for evaluating LLM generations, called PoLL, which consists of a “jury” of smaller models from different families, showing applicability in varying tasks, cost-efficiency, and reduced bias of LLMs as judges.