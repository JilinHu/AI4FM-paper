#### Background
- **Background**
The paper sets the stage by underlining the significance of scientific publishing for foundational science, highlighting issues arising from an increasing speculation about the use of large language models (LLMs) like ChatGPT in academic writing, but in the face of absent precise measures for the level of academic writing significantly modified or produced by LLMs.

- **Existing Work**
While modified content has obvious tells, detection at the individual level is near impossible for less obvious cases. Existing work lacks systemic analysis on a large scale to analyze the proportion of text modified by LLMs, and also lacks a robust statistical framework for estimating the ubiquity of LLM use.

#### Core Contributions
- **Introduced a large-scale analytical framework**
  - **Challenge 1: Measuring the extensiveness of LLM usage in academic publications**
      The challenge of accurately measuring the extent of LLM usage in academic publications has historically been difficult due to the near impossibility of detecting less obvious cases on an individual level. The method applied in this paper estimates on a corpus level through statistical modelling, providing more robust data than inference from individual instances.

  - **Challenge 2: Understanding the connection between LLM-modified content and structural circumstances**
      By systemically analyzing the use of LLMs in academic publishing, the paper not only quantifies the modified content but also reveals the structural circumstances that might motivate its use, such as pressure to publish or issues of linguistic discrimination. This approach captures nuanced epistemic and linguistic shifts on a collective level.

#### Implementation and Deployment
The research applies the distributional GPT quantification framework to analyze a total of 950,965 papers published between January 2020 and February 2024 across arXiv, bioRxiv, and Nature portfolio journals, revealing a steady uptick in LLM usage, with Computer Science papers showing the largest and fastest growth of up to 17.5%. In comparison, Mathematics papers and the Nature portfolio exhibited the least increase in LLM modification, reaching up to 6.3%. It further highlights a correlation between higher degrees of LLM modification and factors such as first authors posting preprints more often, research in crowded fields, and shorter paper lengths.

#### Summary
This paper presents the first large-scale, systematic examination across articles published on arXiv, bioRxiv, and Nature portfolio, with a statistical estimation method that measures the prevalence of LLM-modified content at the population level, providing valuable insights into the application of LLMs in scientific writing.