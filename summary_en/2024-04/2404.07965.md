#### Background
- **Background**
The paper challenges the norm that all tokens in a corpus are equally important for language model training, pointing out that traditional language model (LLM) pre-training methods that apply a uniform next-token prediction loss to all training tokens may not be optimal.

- **Existing Work**
Previous research has focused on filtering out noise through various heuristic methods and classifiers to improve data quality. Nevertheless, even after extensive filtering, many noisy tokens that do not affect training remain. The challenge is that excessive filtering may exclude useful data, altering the text's meaning or causing biases, especially when the distribution of web data does not align with the ideal distribution for downstream applications.

#### Core Contributions
  - **Introduced a new language model called RHO-1**
    - **Challenge 1: Token-level learning dynamics**
    Analyzing token-level training dynamics of language models, the paper discovers that substantial loss reductions are confined to a small subset of tokens during training. RHO-1 addresses this by utilizing Selective Language Modeling (SLM), training only on useful tokens that are aligned with the desired distribution, thus increasing the efficiency of pre-training.

    - **Challenge 2: Improving efficiency and performance**
    RHO-1 significantly enhances token efficiency during pre-training and improves performance on downstream tasks by selectively training tokens that benefit applications downstream. This method scores pre-training tokens using a reference model and trains the language model by focusing on tokens with higher excess loss, effectively identifying tokens relevant to the target distribution.

#### Implementation and Deployment
RHO-1, when continually pre-training on the 15 billion OpenWebMath corpus, achieves an absolute improvement of up to 30% in few-shot accuracy on 9 math tasks, compared to baselines using traditional causal language modeling. After fine-tuning, RHO-1-1B and 7B models achieve state-of-the-art results of 40.6% and 51.8% on the MATH dataset, respectively, using only 3% of the pre-training tokens compared to DeepSeekMath. In addition, when pre-training on 80 billion general tokens, RHO-1 achieves a 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.

#### Summary
This study introduces RHO-1, a novel language model that employs Selective Language Modeling (SLM). This model focuses on training useful tokens during pre-training, demonstrating superior performance in the continuous pre-training in the mathematical domain, reaching baseline performances faster, and attaining state-of-the-art results with a fraction of the tokens.