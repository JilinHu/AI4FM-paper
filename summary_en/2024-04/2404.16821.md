#### Background
- **Background**
The paper discusses the significant advances of multimodal large language models (MLLMs) in bridging the gap between textual and visual information. Despite these advances, there is a significant gap between open-source models and proprietary commercial models in multimodal understanding.

- **Existing Work**
Current commercial MLLMs typically have a scale of not less than 100 billion parameters, while open-source models tend to use much smaller vision foundation models (VFMs), which are combined with LLMs of 7 or 13 billion parameters. This results in a significant capability gap in understanding detailed scenes and documents. Moreover, even though open-source models rely on LLMs' zero-shot capabilities to support other languages, they commonly use only English data for training, resulting in suboptimal performance in non-English scene understanding and OCR tasks.

#### Core Contributions
  - **Presenting InternVL 1.5 model**
  - **Challenge 1: Generality and Flexibility Issues**
      The paper uses a continuous learning strategy to refine a large-scale VFM, InternViT-6B, which not only improves the model's understanding of visual content but also enhances its adaptability across various LLMs.

  - **Challenge 2: Resolution and Task Suitability for Multimodal Inputs**
      A dynamic high-resolution strategy is adopted, which segments images into 448Ã—448 pixel tiles based on images' aspect ratio and resolution, capable of supporting inputs up to 4K resolution, and incorporates a thumbnail view to capture the global context, thus offering flexible resolution options and detailed cross-resolution applicability.

  - **Challenge 3: Language Capability Discrepancies**
      A wide range of public datasets covering high-quality natural scenes, charts, documents, and conversations has been collected and annotated in both English and Chinese. A data translation pipeline using open-source LLMs has been developed, making it easy to extend to more languages, greatly enhancing performance in bilingual OCR and related tasks.

#### Implementation and Deployment
Evaluation results show that InternVL 1.5 has comparable competitive performance when compared to other open-source and proprietary models, achieving state-of-the-art results in 8 out of 18 benchmarks. Notably, the model generally outperforms the leading commercial model GPT-4V in tasks related to Chinese.

#### Summary
InternVL 1.5 is a robust open-source multimodal language model aimed at closing the performance gap between open-source and commercial models in multimodal understanding. Its strengths include enhanced visual understanding, handling dynamic high-resolution images, and the use of a high-quality bilingual dataset, making it perform well across various tasks.