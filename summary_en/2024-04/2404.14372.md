#### Background
- **Background**
This paper points out that large language models (LLMs) have become the default choice for scaling in many language tasks. However, in certain scenarios like patent approval prediction, simple domain-specific graph methods proved to be more effective due to the inherent dependencies in these tasks.
  
- **Existing Work**
Existing work has tried to scale up the backbone model of open-source LLMs and explore prompt-based methods to harness the potential of LLMs for patent approval prediction. However, the best results of these methods were close to random guessing, highlighting the ineffectiveness of simply scaling up models.

#### Core Contributions
- **Introduced a Fine-grained cLAim depeNdency (FLAN) Graph**
  - **Challenge 1: How to capture the inherent dependencies between parts of the patent text**
      The authors proposed the FLAN Graph through meticulous patent data analysis, capturing the inherent dependencies between segments of the patent text. This method is model-agnostic and applies cost-effective graph models to obtain representations for approval prediction.

  - **Challenge 2: How to improve the current state of patent approval prediction**
      The introduced FLAN Graph, when combined with various graph models, consistently outperformed all LLM baselines in most experiments, greatly surpassing the status quo.

#### Implementation and Deployment
Researchers employed graph models such as GraphSage with the FLAN Graph for patent approval prediction. The experimental results showed that all models applying the FLAN Graph outperformed the previous benchmarks, where GraphSage achieved a 7.4% increase in AUC and a 7.8% increase in Macro-F1 score, reaching absolute scores of 66.04 and 58.22, respectively.

#### Summary
The researchers presented a novel algorithm for constructing a fine-grained claim dependency graph (FLAN Graph) that significantly improves the state of the art at scale and conducted comprehensive experiments and analyses of modern LLMs on patent approval prediction, identifying limitations of LLMs and providing valuable references for the development of future LLM-based solutions. The source code and dataset have been publicly released to facilitate future research.