#### Background
- **Background**
The paper investigates the use of Large Language Models (LLMs) for multi-hop question-answering over Knowledge Graphs (KGs). Previous studies commonly use a unified approach for different KG datasets, but this paper suggests that different strategies should be adopted for different KG datasets.
  
- **Existing Work**
The paper indicates that there are limitations to using a unified method for all KG datasets, due to the specific characteristics and challenges of different datasets, which make it difficult to achieve optimal results with the same treatment.

#### Core Contributions
  - **A strategy-selecting method based on dataset characteristics**
    - **Challenge 1: Dealing with KG datasets without available subgraphs**
      The paper introduces SP-LLM strategy to tackle the challenge of datasets without available subgraphs, using three skills: identification of relevant nodes, identification of edges/predicates, and generation of SPARQL queries for question-answering tasks.
  
    - **Challenge 2: Handling KG datasets with available subgraph information**
      The paper utilizes an IR-LLM strategy for datasets with subgraph information. This approach includes multiple steps such as identifying relevant one-hop relations, filtering and returning top-ranked relations, and determining if nodes can serve as answers.

#### Implementation and Deployment
The paper compares the performance of the proposed strategy with existing methods for question answering on various KG datasets such as WebQSP, MetaQA, ComplexWebQuestions, LC-QuAD V1, and V2. The results show significant improvements over other baseline methods on both WebQSP and MetaQA datasets. For instance, using a GPT-4 model (few-shot) achieved an accuracy of 85.32% on the WebQSP, and on the MetaQA dataset, using ChatGPT (with 5 examples) achieved an accuracy of 98.68%. These results demonstrate that the method is more effective for multi-hop question-answering tasks compared to existing approaches.

#### Summary
The paper presents different strategies for multi-hop question-answering tasks across various KG datasets, demonstrating the potent capabilities of large pre-trained language models in complex QA tasks. Through experiments, the paper validates the superiority of the proposed approach over current technologies.