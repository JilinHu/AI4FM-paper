#### Background
- **Background**
The paper discusses the application of transformer-based large language models (LLMs) across various fields and the enormous computational cost associated with their deployment. Methods such as pruning or building mixtures of experts (MoEs) aim to exploit sparsity in transformer feedforward (FF) blocks to increase speed and reduce memory requirements. However, these techniques can be costly and inflexible as they often require training or are limited to specific architectures.

- **Existing Work**
Current methods, while making use of sparsity in LLMs to enhance efficiency, generally do not translate to real-world speed improvements unless pruning is done in a hardware-friendly manner, which often leads to performance decreases. MoEs retain original performance by adaptively selecting model parts per input but also come with drawbacks including the need for training a gating function (expert selection mechanism) and sometimes full fine-tuning. Many of these methods only work on FF blocks with ReLU activations and cannot effectively carry over to pre-trained LLMs with non-ReLU activations.

#### Core Contributions
- **Introduced GRIFFIN**
  - **Challenge 1: How to optimize LLM's FF blocks without training?**
    Existing pruning and MoE methods require training or depend on specific activation functions, which is inflexible and costly in practice. GRIFFIN, inspired by a phenomenon called "flocking" observed in the activation patterns of LLM's FF blocks, creates a training-free MoE system that is effective across various activation functions.

#### Implementation and Deployment
The paper introduces the GRIFFIN method and demonstrates it as a simple yet powerful way to convert any LLM into an MoE. Experimental results show that with a 50% reduction in FF parameters, GRIFFIN still maintains the original model's performance on a variety of classification and generation tasks, while also speeding up the process (e.g., achieving a 1.25Ã— speed-up on Llama 2 13B on an NVIDIA L40). Additionally, GRIFFIN has proven to be effective across different models and activation functions, including ReLU, SwiGLU, GEGLU, and ReGLU.

#### Summary
GRIFFIN is a training-free MoE system that boosts the efficiency of LLMs by leveraging the phenomenon of flocking observed within FF blocks of LLMs across different activation functions while preserving performance and reducing computational costs.