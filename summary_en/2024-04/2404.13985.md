#### Background
- **Background**
    The paper discusses the difficulties large language models (LLMs) face when dealing with complex reasoning tasks that involve contextual understanding. Recent research has proposed various methods to enhance the reasoning capabilities of LLMs, such as Chain-of-Thought (CoT) and Tree of Thoughts (ToT), but these methods often overlook the importance of identifying logical relationships in the context, leading to superficial interaction with and understanding of the context.

- **Existing Work**
    Existing work focuses on improving the reasoning process of LLMs to yield an accurate final answer but often neglects the necessity to first identify logical relationships in the context when addressing contextually aware tasks, possibly leading to unreliable and lower quality reasoning outcomes.

#### Core Contributions
  - **Proposed an Information Re-Organization (InfoRE) method**
    - **Challenge 1: Identifying logical relationships in the context**
        The existing methods often neglect this key step, but through our information re-organization process, we re-organize the contextual content into a MindMap structure, explicitly displaying the logical relationships that are often implicit within the text. The re-organized information enables LLMs to understand the context more deeply, therefore improving the quality and reliability of reasoning.

    - **Challenge 2: Improving LLMs' ability to handle context-aware multi-hop reasoning tasks**
        Our method can be integrated with existing prompt methods like CoT to further enhance LLMs' reasoning abilities. We have verified our method across different context-aware multi-hop reasoning tasks, showing an average improvement of 3% across all tasks in a zero-shot setting, thereby demonstrating the effectiveness of our approach.

#### Implementation and Deployment
We tested the InfoRE method on multi-hop reasoning tasks that require contextual understanding, including claim verification, question answering, and reading comprehension tasks. Using a zero-shot setup, our method achieved an average improvement of 3% on models such as Llama2-70B, GPT-3.5, and GPT-4, exceeding the performance of existing works and highlighting the potential of our approach to improve the reasoning performance of LLMs.

#### Summary
This paper introduced a novel Information Re-Organization (InfoRE) method that enhances the reasoning capabilities of LLMs by re-organizing contextual content to reveal logical relationships. The method was significantly effective when tested on LLMs for context-aware multi-hop reasoning tasks in a zero-shot setting.