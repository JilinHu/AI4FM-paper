#### Background
- **Background**       
The paper discusses how the size of Large Language Models (LLMs), which consist of billions of parameters, presents challenges for enterprises in fine-tuning and deploying them on edge devices with limited computing and memory resources.

- **Existing Work**
Existing research has focused on compressing LLMs using methods such as pruning, low-rank approximation, and quantization, as well as knowledge distillation to produce a single compressed model. Neural Architecture Search (NAS), based on reinforcement learning and evolutionary algorithms, has evolved into weight-sharing NAS to address scaling issues, but these methods still produce only single compressed models, unsuitable for edge scenarios requiring a spectrum of computational capabilities across different devices.

#### Core Contributions
- **Proposed a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS)**
    - **Challenge 1: How to achieve high-quality encoder models for edge devices while maintaining parameter efficiency**
        The MLFS method successfully accomplishes precise, cost-effective supernet training that produces high-quality encoder models suitable for commercial edge applications by training only low-rank matrices and freezing the pre-trained model weights.

    - **Challenge 2: Overcoming the resistance of decoder-only models to compression and significantly reducing training time**
        Although decoder-only models are resistant to compression, MLFS significantly cuts down on training time through effective slicing operations on decoders.

#### Implementation and Deployment
The paper proposed a distillation-based, parameter-efficient approach for supernet training of LLMs, enabling the efficient training of multiple smaller models at once. MLFS achieves parameter efficiency by training only the low-rank decomposition matrices while keeping the pre-trained model weights frozen. Additionally, while decoder models are challenging to compress, the paper introduced "slicing" to substantially reduce training time.

#### Summary
This paper provided a new method for distilling LLMs for edge devices, allowing LPFT while significantly reducing both the size of models and the cost of training, especially addressing the resistance to compression and training duration of decoder models.