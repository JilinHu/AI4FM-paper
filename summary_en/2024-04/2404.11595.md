#### Background
- **Background**
The paper points out that Large Language Models (LLMs) have demonstrated significant effectiveness in software engineering tasks, especially Automated Program Repair (APR). Most existing Deep Learning (DL)-based APR methods assume known bug locations, rely on line-level localization tools, or address bug prediction and repair in a single step. This study uniquely leverages LLMs to predict bug locations at the token level and subsequently utilizes them for bug fixing.

- **Existing Work**
Although recent LLM-based APR techniques have shown significant improvements over DL-based techniques, they still depend on line granularity and lack comprehensive exploration into the varied applications of LLMs for bug fixing. Current methods have not effectively addressed the potential of LLMs for bug localization without reliance on external assumptions or tools, the comparison of the effectiveness of token-level bug location against line-granular localization, and the impact of different types of prompts on the effectiveness of bug fixing.

#### Core Contributions
- **Introduced a novel approach called Toggle**
  - **Challenge 1: Enhancing Bug Location Accuracy**
      The proposed Toggle approach, unlike existing line-granular methods, ensures that no shared prefix or suffix between buggy and fixed functions will be generated. It performs bug localization on a token-granular level to avoid regeneration of non-buggy code, reduce input redundancies, and through designing different prompts, compares and comprehends the effectiveness of each in varying scenarios. By preventing LLMs from generating the shared prefix and suffix, it deeply injects strong inductive bias, significantly improving bug fixing accuracy.

  - **Challenge 2: Integration and Optimization of Bug Fixing Models**
      By integrating a bug localization model, an adjustment unit, and a bug fixing model, a comprehensive program repair framework is constructed. Toggle uses several different generative LLMs fine-tuned for the bug fixing task, significantly improving the repair accuracy and achieving state-of-the-art (SOTA) performance on the CodeXGLUE code refinement benchmark. It also exhibits better or comparable performance on several other widely-used APR datasets, including Defects4J.

#### Implementation and Deployment
In the study, Toggle achieved state-of-the-art performance on the CodeXGLUE code refinement benchmark and showed better or comparable results on several other heavily-used APR datasets, including Defects4J. The research demonstrated superior performance on the Defects4J benchmark, consistently ranking above other methods and excelling in the Top-10, Top-30, Top-50, and Top-100 metrics. The study also examined Toggle's generalizability to unseen data, evaluated the effectiveness of various prompts, investigated the impact of additional contextual information such as buggy lines and code comments on bug localization, and explored the importance of the adjustment unit.

#### Summary
This paper introduces a new approach named Toggle, which utilizes token-level bug localization and repair to overcome the limitations of existing line-granular methods. By designing inputs and fine-tuning LLMs, it significantly enhances the accuracy of bug fixes and delivers outstanding performance on multiple datasets, marking a new progression in the APR field.