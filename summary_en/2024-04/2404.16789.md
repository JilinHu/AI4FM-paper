#### Background
- **Background**
This survey paper addresses the limitations of Large Language Models (LLMs) trained on static datasets and discusses the significant challenges in integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. The main challenge entails balancing model adaptability with knowledge retention, with a specific focus on the issue of performance degradation in pre-trained LLMs known as "catastrophic forgetting" when they are fine-tuned for specific needs.
- **Existing Work**
The current approach often involves the costly and impractical solution of re-collecting training data and re-training models to meet specific needs. To efficiently adapt LLMs to downstream tasks while preserving performance in previous knowledge domains, researchers turn to the methodology of continual learning. The problem of catastrophic forgetting in continual learning has always been a central issue, despite various techniques being explored to mitigate forgetting in machine learning models.

#### Core Contributions
  - **A Comprehensive Survey on Continual Learning of LLMs**
     - **Challenge 1: Vertical Continuity (VCL)**
        The survey describes how LLMs continually adapt from general to specific domains, leveraging examples such as healthcare-focused LLMs while maintaining their general reasoning and question answering capabilities.

     - **Challenge 2: Horizontal Continuity (HCL)**
        It addresses the challenge of continual adaptation across time and domains, detailing the multi-stage training susceptible to catastrophic forgetting, such as the updates of LLMs on social media platforms to reflect current trends.

#### Implementation and Deployment
The paper thoroughly discusses how to evaluate the continual learning processes of LLMs, offering current data sources and evaluation protocols. It also delves into interesting issues related to continual learning LLMs, including the new properties of continual learning, the shifting roles of traditional incremental learning types and memory constraints within the context of LLMs, and prospective research directions for this field.

#### Summary
The survey provides a comprehensive view on the continual learning of LLMs, with a particular emphasis on the under-explored research areas of continual pre-training (CPT) and domain-adaptive pre-training (DAP). It highlights the need for greater attention from the community, especially in the development of practical, accessible, and widely accepted evaluation benchmarks, as well as methodologies tailored for the emerging learning paradigms of large language models.