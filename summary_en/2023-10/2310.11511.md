#### Background
- **Background**
    The paper discusses how large language models (LLMs) often produce responses with factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG) is an ad hoc approach that augments LMs with retrieval of relevant knowledge to decrease such issues. However, these methods may hinder the versatility of LLMs or introduce unnecessary or off-topic passages, leading to responses of poor quality because they indiscriminately retrieve passages regardless of whether that retrieval is helpful for factual grounding.

- **Existing Work**
    Existing RAG methods may diminish the versatility of LLMs or lead to the generation of low-quality responses. The models in existing RAG methods are not explicitly trained to better leverage and follow facts from the provided passages, and their outputs are not guaranteed to be consistent with the retrieved relevant passages.

#### Core Contributions
  - **Introduced a Self-Reflective Retrieval-Augmented Generation (SELF-RAG) framework**
    - **Challenge 1: Improving factual accuracy without compromising versatility**
        SELF-RAG improves the generation quality of LLMs, including their factual accuracy, through on-demand retrieval and self-reflection. The framework trains an arbitrary LM to reflect on its own generation process by generating both task outputs and intermittent special tokens, known as reflection tokens. This method differs from traditional RAG methods that always retrieve a fixed number of documents for generation, regardless of whether the retrieval is necessary, and never reconsider the generation's quality.

    - **Challenge 2: Customizable decoding algorithm**
        SELF-RAG further enables a customizable decoding algorithm to satisfy hard or soft constraints defined by the predictions of reflection tokens. The inference-time algorithm allows for flexible adjustment of retrieval frequency for different downstream applications and customization of the model's behavior to user preferences by leveraging reflection tokens through segment-level beam search, using the weighted linear sum of the reflection token probabilities as the segment score.

#### Implementation and Deployment
Empirical results show that SELF-RAG significantly outperforms pre-trained and instruction-tuned LLMs with more parameters and widely adopted RAG approaches in terms of higher citation accuracy across six tasks, including reasoning and long-form generation. Specifically, SELF-RAG surpasses retrieval-augmented ChatGPT on four tasks and outperforms Llama2-chat and Alpaca on all tasks. The analysis demonstrates the effectiveness of using reflection tokens for training and inference for overall performance improvements and test-time model customizations, such as balancing the trade-off between citation precision and completeness.

#### Summary
The paper introduces SELF-RAG, a new framework that enhances LLM quality and factual accuracy through on-demand retrieval and self-reflection. It makes the LM controllable during the inference phase to suit diverse task requirements and significantly outperforms existing LLMs and RAG models in various tasks. SELF-RAG offers a novel approach to model self-assessment and customization through its decoding algorithm and reflection tokens.