#### Background
- **Background**
This paper introduces OpsEval, a comprehensive task-oriented AIOps benchmark designed for Large Language Models (LLMs). It evaluates LLMs' proficiency across three pivotal scenarios - Wired Network Operation, 5G Communication Operation, and Database Operation - while considering varying ability levels that encompass knowledge recall, analytical thinking, and practical application. The benchmark consists of 7200 questions in both multiple-choice and question-answer (QA) formats, presented in English and Chinese.

- **Existing Work**
Existing frameworks and methods for evaluation do not adequately measure LLMs' comprehensive performance in the AIOps domain, particularly in a task-oriented benchmark context. Additionally, widely used metrics like Bleu and Rouge are not always reflective of model performance.

#### Core Contributions
- **Introduced a comprehensive task-oriented AIOps benchmark (OpsEval)**
  - **Challenge 1: How to comprehensively evaluate LLMs' performance across different scenarios in AIOps?**
      OpsEval has designed 7200 questions encompassing different scenarios and ability levels, including multiple-choice and QA formats, available in both English and Chinese. This allows the measurement of a model's knowledge recall, analytical thinking, and practical application skills.

  - **Challenge 2: There is a lack of a reliable automatic evaluation metric for large-scale qualitative assessment.**
      OpsEval experiments have revealed that the GPT4 score, as a performance metric, may be more reliable than traditional Bleu and Rouge metrics, suggesting its potential to replace existing automatic evaluation metrics.

#### Implementation and Deployment
With the support of quantitative and qualitative results, the study exposes the subtle impact of various LLM techniques such as zero-shot, chain-of-thought, and few-shot in-context learning on AIOps performance. Notably, the GPT4 score has proven to be more reliable compared to widely used metrics like Bleu and Rouge, indicating its potential to become a new standard for automatic metrics in large-scale qualitative evaluations.

#### Summary
OpsEval, as a comprehensive task-oriented AIOps benchmark, not only assesses the comprehensive performance, reasoning, and practical application capabilities of LLMs but also has the potential to change the evaluation metrics used in future large-scale quality assessments. It provides a solid foundation for ongoing research and optimization of LLMs tailored for AIOps.