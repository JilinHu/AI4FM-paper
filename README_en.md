<h2 align='center'>llm-paper-daily Daily Paper Selection</h2>
<div align='center'>

[![Status](https://img.shields.io/badge/status-Update_03.11_18:49-success.svg)]() [![Simplified Chinese badge](https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-Simplified%20Chinese-blue)](./README.md) [![English badge](https://img.shields.io/badge/%E8%8B%B1%E6%96%87-English-blue)](./README_en.md) 

</div>

**Each paper comes with related resources:**
- arXiv link &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)
- GitHub link&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   ![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)
- Summary of GPT-4  ![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)
- Related blogs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)

<details>
  <summary>Click to view latest updates. &nbsp;&nbsp;<sub>Update time: 03-11 18:49</sub></summary>
<br>

- Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context 
- Yi: Open Foundation Models by 01.AI 
- Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference 
- Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering 
- Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation 
</details>

## 2024-03

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>03-08</span> | **Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation**<br><sub>The paper presents Adversarial Policy Optimization (AdvPO), a novel approach to tackling reward over-optimization issues within the RLHF process, especially in LLMs aimed at aligning with human preferences. AdvPO effectively alleviates the problem of reward over-optimization without incurring high computational costs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2403.05171v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-03/2403.05171.md)  |
| <span style='display: inline-block; width: 42px;'>03-08</span> | **Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering**<br><sub>Institution: Gaoling School of Artificial Intelligence Renmin University of China, Nankai University, Beijing Academy of Artificial Intelligence<br>LLMQA is a novel generalized framework that combines strengths of retrieval- and generation-based evidence collection. By enabling LLMs to take on multiple roles within the framework, the paper significantly improves the overall performance of ODQA systems, with experimental results demonstrating its effectiveness over existing methods.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2403.05217v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-03/2403.05217.md)  |
| <span style='display: inline-block; width: 42px;'>03-08</span> | **Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context**<br><sub>Institution: Google<br>Gemini 1.5 Pro achieved a significant breakthrough in memory and reasoning capabilities for vast amounts of long-context information, particularly in processing extended texts, videos, and audio. The model not only outperforms in effectiveness but also shows improved computational efficiency.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2403.05530v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-03/2403.0553.md)  |
| <span style='display: inline-block; width: 42px;'>03-07</span> | **Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference**<br><sub>Institution: UC Berkeley, Stanford, UCSD<br>Chatbot Arena is an open platform for evaluating LLMs based on human preferences. It employs a crowdsourced approach to collect questions for anonymous randomized battles, addressing the limitations of static dataset benchmarks, and uses carefully designed statistical methods to ensure the credibility and efficiency of evaluations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2403.04132v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-03/2403.04132.md)  |
| <span style='display: inline-block; width: 42px;'>03-07</span> | **Yi: Open Foundation Models by 01.AI**<br><sub>Institution: 01.AI<br>The paper successfully introduces the Yi-34B model, performing comparably to GPT-3.5 in both performance and efficiency, and provides detailed descriptions of innovative approaches to pre-training large language models and their instruction fine-tuning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2403.04652v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-03/2403.04652.md)  |
| <span style='display: inline-block; width: 42px;'>03-05</span> | **ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary**<br><sub>Institution: Tsinghua University<br>The ChatCite system is designed to overcome the challenges faced by LLMs in generating literature reviews. It enables an LLM agent to more effectively understand, summarize, and compare different research works, thus producing organized and comparative literature reviews.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2403.02574v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-03/2403.02574.md)  |
| <span style='display: inline-block; width: 42px;'>03-05</span> | **MathScale: Scaling Instruction Tuning for Mathematical Reasoning**<br><sub>Institution: The Chinese University of Hong Kong Shenzhen, China; Microsoft Research Asia, Beijing, China; Shenzhen Research Institute of Big Data, Shenzhen, China<br>MathScale proposes a scalable approach to creating high-quality mathematical reasoning data and introduces a new comprehensive benchmark, MWPBENCH, to fully evaluate the mathematical reasoning capabilities of LLMs, thereby significantly enhancing the models' performance in solving mathematical problems.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2403.02884v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-03/2403.02884.md)  |
| <span style='display: inline-block; width: 42px;'>03-05</span> | **Design2Code: How Far Are We From Automating Front-End Engineering?**<br><sub>Institution: Stanford University, Georgia Tech, Microsoft<br>The paper formalizes and benchmarks the Design2Code task to assess the capability of current multimodal LLMs in converting visual designs into code, finding that GPT-4V performs best, offering a new paradigm for automating front-end development.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2403.03163v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-03/2403.03163.md)  |

---

## 2024-02

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>02-29</span> | **SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation**<br><sub>Institution: Peking University<br>This paper introduces SEED, an adaptation method using error-driven learning, enabling LLMs to learn efficiently with fewer samples for code generation tasks, achieving better performance and generalization.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2403.00046v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2403.00046.md)  |
| <span style='display: inline-block; width: 42px;'>02-29</span> | **StarCoder 2 and The Stack v2: The Next Generation**<br><sub>Institution: ServiceNow, Hugging Face  <br>The paper presented the development process of The Stack v2 and StarCoder2, a work focused on large-scale pre-training and instruction fine-tuning for code. Researchers significantly enhanced the performance of code LLMs, especially in handling low-resource programming languages and tasks requiring code reasoning, by integrating diverse data sources and a meticulously designed training process.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.19173v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.19173.md)  |
| <span style='display: inline-block; width: 42px;'>02-29</span> | **Beyond Language Models: Byte Models are Digital World Simulators**<br><sub>Institution: Microsoft Research Asia<br>The paper showcases the potential of bGPT in handling challenging byte-level data simulation tasks, particularly highlighting its capabilities in cross-modal knowledge transfer and digital world simulation. This reveals the broad applicability and flexibility of byte models in digital media data processing and understanding.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.19155v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.19155.md)  |
| <span style='display: inline-block; width: 42px;'>02-29</span> | **Resonance RoPE: Improving Context Length Generalization of Large Language Models**<br><sub>Institution: 1DIRO Université de Montréal, Mila - Quebec AI Institute, Huawei Noah’s Ark Lab<br>This paper presents Resonance Rope, an improved model that enhances performance in dealing with long texts based on the analysis of RoPE position embedding feature wavelengths. It also introduces the POSGEN benchmark to assist in the study and evaluation of position embeddings in long-text tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2403.00071v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2403.00071.md)  |
| <span style='display: inline-block; width: 42px;'>02-27</span> | **The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits**<br><sub>Institution: Microsoft, University of Chinese Academy of Sciences<br>The paper presents the BitNet b1.58 model, which is a 1.58-bit quantized Large Language Model that is comparable in performance to traditional full-precision LLMs while being more efficient and energy-saving.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.17764v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.17764.md)  |
| <span style='display: inline-block; width: 42px;'>02-27</span> | **EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions**<br><sub>Institution: Alibaba Group  <br>The EMO framework enhances the realism and expressiveness of generated videos through a direct audio-to-video synthesis method, significantly surpassing existing technologies and marking a significant advance in the field of video synthesis.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.17485v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.17485.md)  |
| <span style='display: inline-block; width: 42px;'>02-27</span> | **When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method**<br><sub>Institution: Google DeepMind<br>The paper provides significant insights into the impact of factors such as data size, model size, and finetuning methods on the performance of LLMs during the finetuning phase, defining a new framework for evaluation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.17193v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.17193.md)  |
| <span style='display: inline-block; width: 42px;'>02-27</span> | **REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering**<br><sub>Institution: Gaoling School of Artificial Intelligence Renmin University of China, School of Information Renmin University of China<br>The paper presented the REAR framework, which focuses on enhancing the ability of LLMs to utilize external knowledge in QA tasks by adding self-awareness of document relevance and has proven its effectiveness over previous methodologies.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.17497v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.17497.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/RUCAIBox/REAR)</div> |
| <span style='display: inline-block; width: 42px;'>02-27</span> | **Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization**<br><sub>Institution: Zhejiang University, Institute of Software Chinese Academy of Sciences, Nanjing University of Posts and Telecommunications<br>Agent-Pro represents a new type of LLM-based intelligence agent that can learn and develop strategies in interactive environments through policy-level reflection and optimization, addressing the issue of existing works' inability to learn through interaction and adapt.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.17574v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.17574.md)  |
| <span style='display: inline-block; width: 42px;'>02-27</span> | **Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models**<br><sub>Institution: OpenAI<br>This review article provides an insight into Sora—a large vision model, discussing its technological features, innovative aspects, current limitations, and potential opportunities for future applications. Sora's capabilities signify progressive strides made by large vision models, including long video generation and processing of diverse video formats.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.17177v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.17177.md)  |
| <span style='display: inline-block; width: 42px;'>02-26</span> | **LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments**<br><sub>The study introduced the LLMARENA benchmark to assess the capabilities of LLM agents in complex multi-agent settings, highlighting existing issues and advancing future research directions, including capabilities in multimodal dynamic contexts and the potential use of external tools.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.16499v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.16499.md)  |
| <span style='display: inline-block; width: 42px;'>02-26</span> | **Improving LLM-based Machine Translation with Systematic Self-Correction**<br><sub>Institution: Zhejiang University, Tencent, Angelalign Technology Inc.<br>The paper successfully introduced the first LLM-based self-correcting translation framework named TER, and demonstrated its effectiveness in improving translation quality across various language pairs and models. It opened new horizons in the field of machine translation, especially for the use of self-correction in translations between high-resource, low-resource languages, and translations involving different central languages.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.16379v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.16379.md)  |
| <span style='display: inline-block; width: 42px;'>02-26</span> | **Do Large Language Models Latently Perform Multi-Hop Reasoning?**<br><sub>Institution: Google DeepMind, UCL, Google Research<br>This research examines LLMs’ potential for latent multi-hop reasoning, proposing new methods for evaluating latent multi-hop reasoning capabilities and indicating strong evidence of multi-hop reasoning for certain types of relational prompts in LLMs, though highly context-dependent.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.16837v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.16837.md)  |
| <span style='display: inline-block; width: 42px;'>02-25</span> | **ChatMusician: Understanding and Generating Music Intrinsically with LLM**<br><sub>Institution: Hong Kong University of Science and Technology<br>The paper made substantial progress in an under-researched domain by creating the first music pre-training dataset and assessment benchmark for language models, enhancing LLMs' performance in understanding and generating music.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.16153v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.16153.md)  |
| <span style='display: inline-block; width: 42px;'>02-23</span> | **ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.15220v1)</div> |
| <span style='display: inline-block; width: 42px;'>02-23</span> | **Genie: Generative Interactive Environments**<br><sub>Institution: Google DeepMind, University of British Columbia<br>Genie is an interactive environment model capable of generating new videos and controlling the content of the videos through user inputs, bridging the gap between traditional video generation technologies and interactive experiences.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.15391v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.15391.md)  |
| <span style='display: inline-block; width: 42px;'>02-22</span> | **Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.14744v1)</div> |
| <span style='display: inline-block; width: 42px;'>02-22</span> | **CriticBench: Benchmarking LLMs for Critique-Correct Reasoning**<br><sub>Institution: Tsinghua University, University of Hong Kong<br>The paper evaluates LLMs' critique and correction reasoning abilities through CRITICBENCH, exploring key factors influencing these competencies, aiming to foster further research in LLM critique and self-improvement.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.14809v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.14809.md)  |
| <span style='display: inline-block; width: 42px;'>02-22</span> | **OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.14658v1)</div> |
| <span style='display: inline-block; width: 42px;'>02-22</span> | **Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.14672v1)</div> |
| <span style='display: inline-block; width: 42px;'>02-21</span> | **User-LLM: Efficient LLM Contextualization with User Embeddings**<br><sub>USER-LLM is a framework that contextualizes LLMs using user embeddings. It addresses the complexities of user data and the challenges of processing long sequences, improving the usability of LLMs in personalized applications while being computationally efficient.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.13598v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.13598.md)  |
| <span style='display: inline-block; width: 42px;'>02-21</span> | **AgentScope: A Flexible yet Robust Multi-Agent Platform**<br><sub>Institution: Alibaba Group<br>AgentScope is a versatile platform for building multi-agent applications, emphasizing usability and customizability, particularly catered to developers with varying skill levels. By implementing fault tolerance and supporting multimodal data processing, as well as optimizing distributed operations, AgentScope significantly reduces the complexity of developing and deploying multi-agent systems, promoting wider participation and innovation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.14034v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.14034.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/modelscope/agentscope)</div> |
| <span style='display: inline-block; width: 42px;'>02-20</span> | **TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization**<br><sub>Institution: AWS AI Labs, The University of Texas at Austin, KAIST<br>The article introduces TOFUEVAL, a new assessment benchmark for evaluating the factual consistency of LLMs in generating topic-focused dialogue summaries. The study uncovered extensive factual errors in the summaries generated by LLMs of varying sizes within the domain of dialogue.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.13249v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.13249.md)  |
| <span style='display: inline-block; width: 42px;'>02-20</span> | **Instruction-tuned Language Models are Better Knowledge Learners**<br><sub>Institution: FAIR at Meta, Carnegie Mellon University, University of Washington<br>The paper introduces a method called pre-instruction-tuning (PIT), which effectively improves the ability of LLMs to absorb knowledge from documents, addresses the "perplexity curse," and makes significant strides in multi-domain knowledge acquisition.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.12847v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.12847.md)  |
| <span style='display: inline-block; width: 42px;'>02-19</span> | **AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling**<br><sub>Institution: Fudan University, Multimodal Art Projection Research Community, Shanghai AI Laboratory<br>AnyGPT is a multimodal language model architecture that achieves seamless conversion and unified processing across modalities through discrete sequence modeling, delivering the ability to generate from any modality to any other without needing alterations to the current LLM architecture or training paradigms. It efficiently processes and generates high-quality multimodal content, with performance comparable to specialized models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.12226v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.12226.md)  |
| <span style='display: inline-block; width: 42px;'>02-16</span> | **FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models**<br><sub>Institution: The University of British Columbia & Invertible AI<br>The paper presents a multimodal Large Language Model suite named FinTral, optimized for financial analysis. The model's performance was showcased against existing models and demonstrated its advanced capabilities in multi-task contexts within the financial sector, especially in handling zero-shot tasks and reducing hallucination phenomena.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.10986v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.10986.md)  |
| <span style='display: inline-block; width: 42px;'>02-16</span> | **Speculative Streaming: Fast LLM Inference without Auxiliary Models**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.11131v1)</div> |
| <span style='display: inline-block; width: 42px;'>02-16</span> | **SPAR: Personalized Content-Based Recommendation via Long Engagement Attention**<br><sub>Institution: The University of British Columbia, Meta<br>The SPAR framework effectively uses long-term user engagement histories to enhance the accuracy of personalized content recommendations and surpasses the existing state-of-the-art across multiple performance metrics.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.10555v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.10555.md)  |
| <span style='display: inline-block; width: 42px;'>02-15</span> | **How to Train Data-Efficient LLMs**<br><sub>Institution: Google DeepMind, University of California San Diego, Texas A&M University<br>The ASK-LLM and DENSITY techniques proposed in the paper optimize the data efficiency of large language models, effectively enhancing the speed and quality of model training and performing well under resource constraints.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.09668v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.09668.md)  |
| <span style='display: inline-block; width: 42px;'>02-15</span> | **A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts**<br><sub>Institution: Google DeepMind, Google Research<br>ReadAgent is an LLM agent system inspired by human reading processes, which significantly enhances performance and scalability by generating gist memories and retrieving information as needed for tasks involving long contexts.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.09727v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.09727.md)  |
| <span style='display: inline-block; width: 42px;'>02-15</span> | **Chain-of-Thought Reasoning Without Prompting**<br><sub>Institution: Google DeepMind<br>This work uncovers that by changing the decoding strategy, one can naturally elicit reasoning from pre-trained LLMs, with CoT paths being more prevalent in tasks frequently represented in the pre-training data. The introduced CoT-decoding method significantly enhances model performance on various reasoning benchmarks without the need for manual prompts.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.10200v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.102.md)  |
| <span style='display: inline-block; width: 42px;'>02-14</span> | **Premise Order Matters in Reasoning with Large Language Models**<br><sub>Institution: Google DeepMind<br>The paper focuses on the influence that the order of premises has on LLMs when conducting reasoning tasks, and the impact was assessed via the newly created R-GSM benchmark test. It reveals the extreme sensitivity of LLMs to premise ordering, showing a substantial effect on performance.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.08939v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.08939.md)  |
| <span style='display: inline-block; width: 42px;'>02-09</span> | **InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning**<br><sub>Institution: Shanghai AI Laboratory, Tsinghua University, Fudan University School of Computer Science<br>The InternLM-Math model is a mathematical reasoning tool based on LLMs that integrates various capabilities and provides supervised learning to help the model achieve state-of-the-art performance in various mathematical reasoning tasks, with code and data made open-source. The paper also explores a new approach to solving mathematical problems with the programming language LEAN within a multi-task learning setup, showcasing the potential of LLMs in formalized and code-assisted reasoning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.06332v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.06332.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/InternLM/InternLM-Math)</div> |
| <span style='display: inline-block; width: 42px;'>02-02</span> | **AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback**<br><sub>Institution: Tsinghua University, Ant Group<br>The AMOR framework integrates reasoning logic based on a finite state machine (FSM) and a process feedback mechanism, showcasing how an open-source LLM-based knowledge agent can reason and adapt with human oversight, enhancing the model's capabilities in performing knowledge-intensive tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.01469v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.01469.md)  |
| <span style='display: inline-block; width: 42px;'>02-02</span> | **MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models**<br><sub>Institution: UNC Chapel Hill.<br>This paper introduces a new method called MAGDI, which significantly enhances the reasoning abilities and generalization capacity of smaller models through structured distillation of reasoning interactions between multiple LLMs, while reducing costs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.01620v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.0162.md)  |
| <span style='display: inline-block; width: 42px;'>02-02</span> | **K-Level Reasoning with Large Language Models**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.01521v1)</div> |
| <span style='display: inline-block; width: 42px;'>02-02</span> | **Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions**<br><sub>Institution: Megagon Labs, Carnegie Mellon University<br>This paper introduces the concept of reasoning capacity in multi-agent systems to improve optimization and evaluation and explores the potential of human feedback to enhance system reasoning capabilities.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.01108v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.01108.md)  |
| <span style='display: inline-block; width: 42px;'>02-02</span> | **LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving**<br><sub>Institution: Shanghai Artificial Intelligence Laboratory, College of Control Science and Engineering Zhejiang University<br>LimSim++ is the first closed-loop evaluation platform specifically developed for (M)LLM-driven autonomous driving. It overcomes the limitations of current simulation platforms and validates its effectiveness in various complex traffic scenarios through experimentation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.01246v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.01246.md)  |
| <span style='display: inline-block; width: 42px;'>02-01</span> | **HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent**<br><sub>Institution: Amazon, University of Milano-Bicocca<br>This paper introduces a new resource, HR-MultiWOZ, a Task-Oriented Dialogue Dataset for an HR LLM Agent. It tackles the problem of a lack of high-quality training datasets for building and evaluating HR LLM agents while providing a cost-effective data generation methodology that serves as a valuable asset and benchmark for subsequent research in the field.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.01018v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.01018.md)  |
| <span style='display: inline-block; width: 42px;'>02-01</span> | **Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing**<br><sub>Institution: Nanyang Technological University, Institute for Infocomm Research A*STAR, Salesforce Research<br>The paper proposes a novel offline training framework focused on improving the reliability and accuracy of Large Language Models in complex reasoning tasks through trajectory collection and direct preference optimization based on outcome supervision, without the need for teacher models or human annotations. The results on two logical reasoning benchmarks prove the effectiveness of the proposed method.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.00658v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.00658.md)  |
| <span style='display: inline-block; width: 42px;'>02-01</span> | **Can Large Language Models Understand Context?**<br><sub>Institution: Georgetown University, Apple<br>This paper introduces a context understanding benchmark to assess the contextual understanding abilities of Large Language Models (LLMs). The benchmark encompasses the elements required for understanding context both in documents and dialogue bases, and uses innovative testing methods and experimental analysis to showcase the abilities and limitations of LLMs in understanding context.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.00858v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.00858.md)  |
| <span style='display: inline-block; width: 42px;'>02-01</span> | **Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration**<br><sub>Institution: University of Washington, University of California Berkeley, The Hong Kong University of Science and Technology<br>This article focuses on identifying knowledge gaps in large language models (LLMs) and abstaining from answering questions when necessary. The study proposes two novel multi-LLM collaboration methods, which showed through comparative experiments that they can effectively improve the ability of LLMs to abstain from generating outputs with low confidence.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2402.00367v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-02/2402.00367.md)  |

---

## 2024-01

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>01-31</span> | **LongAlign: A Recipe for Long Context Alignment of Large Language Models**<br><sub>Institution: Tsinghua University, Zhipu.AI<br>The paper proposes a novel recipe, LongAlign, for the long context alignment of LLMs, by constructing a long instruction dataset, adopting new training strategies, and introducing evaluation benchmarks, enhancing the LLMs' ability to handle lengthy contexts. The code, data, and long-aligned models are open-sourced.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.18058v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.18058.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/THUDM/LongAlign)</div> |
| <span style='display: inline-block; width: 42px;'>01-30</span> | **Efficient Tool Use with Chain-of-Abstraction Reasoning**<br><sub>Institution: Meta<br>The paper proposes a novel Chain-of-Abstraction reasoning approach that effectively enhances LLMs' capability to use external tools and expedites the reasoning process. Experimental results demonstrate its effectiveness and efficiency in multi-step reasoning tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.17464v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.17464.md)  |
| <span style='display: inline-block; width: 42px;'>01-30</span> | **Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate**<br><sub>Institution: Shanghai Jiao Tong University, Carnegie Mellon University, Shanghai Artificial Intelligence Laboratory<br>SCALEEVAL is an innovative meta-evaluation framework designed to evaluate the trustworthiness and efficiency of LLMs as evaluators. It incorporates multi-agent LLM debate and minimal human supervision into the evaluation process, providing flexibility and scalability, with experimental results showing high consistency with purely human evaluations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.16788v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.16788.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/GAIR-NLP/scaleeval)</div> |
| <span style='display: inline-block; width: 42px;'>01-30</span> | **Recovering Mental Representations from Large Language Models with Markov Chain Monte Carlo**<br><sub>Institution: Princeton University, University of Warwick<br>The article demonstrated an effective increase in efficiency and performance by integrating LLMs into sampling algorithms and using Direct Sampling along with MCMC to extract mental representations, exploring the potential for Bayesian inference with LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.16657v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.16657.md)  |
| <span style='display: inline-block; width: 42px;'>01-30</span> | **Incoherent Probability Judgments in Large Language Models**<br><sub>Institution: Princeton University<br>The paper investigates the coherence of probability judgments made by large language models, finding biases comparable to systemic deviations in human cognition. It quantified incoherence using probabilistic identities and repetition of judgments. The hypothesis presented connects the human-like biases observed when LLMs make probability judgments to their autoregressive training objectives, supported by potential links between the Bayesian Sampler model and autoregressive processes within LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.16646v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.16646.md)  |
| <span style='display: inline-block; width: 42px;'>01-29</span> | **Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation for Automatic Diagnosis**<br><sub>Institution: Harbin Institute of Technology<br>This research introduces an LLM-based automatic diagnostic method—Multi-Specialist Agent Consultation Model (AMSC), which better simulates the diagnostic process in the real world and improves diagnosis accuracy and efficiency by integrating predictions from multiple specialized agents.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.16107v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.16107.md)  |
| <span style='display: inline-block; width: 42px;'>01-29</span> | **SelectLLM: Can LLMs Select Important Instructions to Annotate?**<br><sub>Institution: University of Minnesota, Carnegie Mellon University<br>This work introduces a novel method SELECTLLM for using LLMs to select unlabeled high-quality instructions, challenging traditional selection algorithms and enhancing selection efficiency while maintaining the global structure of the dataset. The experiments demonstrate superior performance on instruction-tuning benchmarks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.16553v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.16553.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/minnesotanlp/select-llm)</div> |
| <span style='display: inline-block; width: 42px;'>01-29</span> | **LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning**<br><sub>Institution: Nanyang Technological University<br>LLM4Vuln is an innovative framework that significantly enhances LLMs' performance in code vulnerability analysis by providing a vector database of vulnerability knowledge, tool invocation capabilities, custom CoT prompt schemes, and structuring outputs using instructionally proficient models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.16185v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.16185.md)  |
| <span style='display: inline-block; width: 42px;'>01-28</span> | **PRE: A Peer Review Based Large Language Model Evaluator**<br><sub>The PRE model presented in this paper provides a novel framework for automatically evaluating LLMs by simulating the peer review system commonly used in academia, significantly lowering costs and exhibiting increased generalizability and reliability.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.15641v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.15641.md)  |
| <span style='display: inline-block; width: 42px;'>01-27</span> | **MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries**<br><sub>Institution: Hong Kong University of Science and Technology<br>The paper developed the MultiHop-RAG dataset to assess and improve the existing limitations of Retrieval-Augmented Generation (RAG) systems in handling multi-hop queries requiring retrieval and reasoning. It also provided experimental results demonstrating current RAG systems' limitations on such tasks and released the dataset to encourage further research and development.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.15391v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.15391.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/yixuantt/MultiHop-RAG)</div> |
| <span style='display: inline-block; width: 42px;'>01-26</span> | **EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty**<br><sub>Institution: Peking University, Microsoft Research, University of Waterloo<br>The paper proposes a new framework named EAGLE to increase the auto-regressive decoding speed of Large Language Models (LLMs) while maintaining the consistency of the generated text distribution with the original LLMs. EAGLE has significantly improved upon speculative sampling methods in reducing time overhead and increasing draft acceptance rate, offering faster acceleration compared to Lookahead and Medusa, with low training cost and ease of deployment.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.15077v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.15077.md)  |
| <span style='display: inline-block; width: 42px;'>01-25</span> | **Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning**<br><sub>Institution: Columbia University, Microsoft Research, University of California Berkeley<br>The EC-Finetuning method has successfully increased the consistency of explanations generated by LLMs and demonstrated its ability to generalize to unseen datasets, showing a 10.0% relative improvement in explanation consistency on fine-tuning datasets and a 4.5% improvement on out-of-distribution datasets, along with moderate improvements in prediction accuracy.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.13986v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.13986.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/yandachen/explanation-consistency-finetuning)</div> |
| <span style='display: inline-block; width: 42px;'>01-25</span> | **ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases**<br><sub>Institution: HKUST<br>ConstraintChecker is an independent plugin tool that effectively enhances the performance of LLMs in CSKB reasoning tasks. It helps LLMs to perform better in reasoning by providing and checking explicit constraints and has shown to outperform other advanced prompting techniques in validated metrics.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.14003v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.14003.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HKUST-KnowComp/ConstraintChecker)</div> |
| <span style='display: inline-block; width: 42px;'>01-25</span> | **True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning**<br><sub>Institution: Nanyang Technological University, Zhejiang University<br>The TWOSOME framework effectively aligns LLMs with embodied environments using RL, improving sample efficiency and task generalization while retaining LLMs' original functionality.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.14151v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.14151.md)  |
| <span style='display: inline-block; width: 42px;'>01-24</span> | **Can AI Assistants Know What They Don't Know?**<br><sub>Institution: Fudan University, Shanghai Artificial Intelligence Laboratory<br>This paper focuses on the capacity of AI assistants to recognize their knowledge boundaries and by constructing an Idk dataset and aligning the assistant accordingly, the paper achieves making AI assistants recognize and admit what they don’t know, reducing factual errors in their responses.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.13275v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.13275.md)  |
| <span style='display: inline-block; width: 42px;'>01-24</span> | **MM-LLMs: Recent Advances in MultiModal Large Language Models**<br><sub>Institution: Tencent AI Lab, Kyoto University, Mohamed Bin Zayed University of Artificial Intelligence<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.13601v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.13601.md)  |
| <span style='display: inline-block; width: 42px;'>01-24</span> | **Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction**<br><sub>Institution: Nanjing University of Science and Technology, Northeastern University, Singapore Institute of Technology<br>The paper presents a new Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework that generates labeled data by Retrieval and Denoizing Knowledge from LLMs and significantly improves the performance of document-level relation triplet extraction through a series of novel methods.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.13598v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.13598.md)  |
| <span style='display: inline-block; width: 42px;'>01-24</span> | **Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption**<br><sub>Institution: Tsinghua University, Zhongguancun Laboratory, XinJiang University<br>The CGPE framework introduced in the paper effectively supports the application of LLMs in question-answering tasks by using a clue-guided path exploration mechanism, lowering the capability requirements for LLMs, and significantly reducing computational resource consumption, which has important practical significance for individuals and organizations with limited computational resources.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.13444v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.13444.md)  |
| <span style='display: inline-block; width: 42px;'>01-24</span> | **AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents**<br><sub>Institution: The University of Hong Kong, Zhejiang University, Shanghai Jiao Tong University<br>Researchers introduced a new benchmark, AGENTBOARD, for evaluating multi-turn capable large language model agents, providing a granular progress rate and interactive analysis tools to deepen the understanding of LLM agent performance.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.13178v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.13178.md)  |
| <span style='display: inline-block; width: 42px;'>01-23</span> | **AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents**<br><sub>Institution: Google DeepMind<br>The paper describes a system named AutoRT that uses large foundation models to control real-world robots to autonomously navigate and perform tasks. It marks the first instance of LLM-controlled robots operating autonomously in real-world settings, proposing their own goals, and taking actions toward those goals. The data collected by AutoRT is not only diverse but can improve the performance of robot learning models and be aligned with human preferences.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.12963v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.12963.md)  |
| <span style='display: inline-block; width: 42px;'>01-23</span> | **KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning**<br><sub>Institution: Samsung R&D Institute India - Bangalore<br>KAM-CoT is a multimodal Chain-of-Thought reasoning framework that integrates CoT reasoning, knowledge graphs, and multiple modalities. It outperforms state-of-the-art approaches with fewer trainable parameters, showcasing superior performance and cost-efficiency.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.12863v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.12863.md)  |
| <span style='display: inline-block; width: 42px;'>01-23</span> | **Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment**<br><sub>Institution: Alibaba Inc.<br>The paper proposes DITTO, a self-alignment method that enhances LLMs' role-play capabilities through knowledge augmentation and dialogue simulation. It also provides a reproducible, explainable, and efficient role-play evaluation method and explores the dissection of role-play through cross-supervision experiments, offering an in-depth understanding and insights into building role-play functions for LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.12474v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.12474.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/OFA-Sys/Ditto)</div> |
| <span style='display: inline-block; width: 42px;'>01-23</span> | **CCA: Collaborative Competitive Agents for Image Editing**<br><sub>The paper presents a new generative model based on multiple Large Language Models (LLMs), capable of handling complex image editing tasks and enhancing the quality and robustness of the results. Encouraging collaborative competition among agents, the model demonstrates capabilities exceeding traditional methods, especially in managing complex tasks and learning from intermediate steps to refine outcomes.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.13011v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.13011.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/TiankaiHang/CCA)</div> |
| <span style='display: inline-block; width: 42px;'>01-22</span> | **Improving Small Language Models' Mathematical Reasoning via Mix Thoughts Distillation**<br><sub>Institution: Institute of Information Engineering, Chinese Academy of Sciences<br>This paper stated that EoTD and MTD show it is possible to distill LLMs' mathematical reasoning capabilities into Small Language Models (SLMs) with fewer than one billion parameters. The methods preserve and enhance SLMs' reasoning abilities, enabling them to achieve state-of-the-art performance on reasoning tasks. This advancement opens the door for broader applications of SLMs in resource-constrained environments, bridging the gap between the demand for powerful reasoning models and computational resource limitations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.11864v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.11864.md)  |
| <span style='display: inline-block; width: 42px;'>01-22</span> | **PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety**<br><sub>Institution: Shanghai Artificial Intelligence Laboratory, Dalian University of Technology<br>The article presents PsySafe, a comprehensive framework for the safety of multi-agent systems, integrating psychological-based approaches for attack, defense, and evaluation. The experimental outcomes provide deeper insights into understanding and researching the safety issues of multi-agent systems.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.11880v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.1188.md)  |
| <span style='display: inline-block; width: 42px;'>01-22</span> | **CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation**<br><sub>Institution: Stanford University, Stability AI  <br>This paper addresses the challenges in automated CXR interpretation by introducing a large dataset specifically designed for CXR interpretation, developing a novel foundation model, and creating a comprehensive evaluation benchmark. It demonstrates the superior performance of CheXagent in various assessment tasks compared to other models and takes an important stride towards transparency by examining potential biases within the model, providing valuable insights for future research and applications.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.12208v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.12208.md)  |
| <span style='display: inline-block; width: 42px;'>01-21</span> | **Interactive AI with Retrieval-Augmented Generation for Next Generation Networking**<br><sub>Institution: Nanyang Technological University, Guangdong University of Technology, Institute for Infocomm Research, Agency for Science Technology and Research<br>This paper explores the integration of interactive AI (IAI) with next-generation networking, using retrieval-augmented generation (RAG) and large language models (LLM) to enhance decision-making capabilities, proved through real network optimization case studies.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.11391v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.11391.md)  |
| <span style='display: inline-block; width: 42px;'>01-20</span> | **BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models**<br><sub>Institution: University of Illinois Urbana-Champaign, University of Washington, Western Washington University<br>This article proposes BadChain, a backdoor attack on LLMs using COT prompting that does not require access to training datasets or model parameters and has low computational overhead. The method effectively reveals the security vulnerabilities under COT prompting in LLMs and emphasizes the importance of carrying out backdoor attacks and designing effective defenses.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.12242v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.12242.md)  |
| <span style='display: inline-block; width: 42px;'>01-19</span> | **Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning**<br><sub>Institution: MIT<br>The paper presents how LLMs can be made more resistant to "jailbreak" attacks from a safety alignment perspective through Wanda pruning without the need for fine-tuning and validates model performance through a constructed dataset and evaluation system.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.10862v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.10862.md)  |
| <span style='display: inline-block; width: 42px;'>01-19</span> | **Tool-LMM: A Large Multi-Modal Model for Tool Agent Learning**<br><sub>Institution: ShanghaiTech University, Meituan, UniDT<br>Tool-LMM stands out as the first system aimed at training a large multi-modal model to learn tool agency, innovatively integrating multi-modal inputs with the correct selection of external tools, overcoming ambiguity in text, and showcasing the ability to automatically select appropriate tools in response to multi-modal instructions.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.10727v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.10727.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Tool-LMM/Tool-LMM)</div> |
| <span style='display: inline-block; width: 42px;'>01-19</span> | **Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment**<br><sub>Institution: Sun Yat-sen University, Tencent AI Lab<br>This paper introduces an innovative KCA method that reduces the inconsistency between external and intrinsic knowledge, thereby mitigating hallucinations in LLMs during alignment. The study offers several insights for future research, notably the excellent performance of the KCA method across various scenarios and the combination of its simplicity and effectiveness.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.10768v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.10768.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/fanqiwan/KCA)</div> |
| <span style='display: inline-block; width: 42px;'>01-19</span> | **Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads**<br><sub>Institution: Princeton University, Together AI, University of Illinois Urbana-Champaign<br>The paper presents Medusa, an efficient method for accelerating LLM inference by adding multiple decoding heads that parallelly predict multiple tokens, thereby substantially reducing the number of decoding steps and significantly improving the inference speed of large models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.10774v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.10774.md)  |
| <span style='display: inline-block; width: 42px;'>01-18</span> | **Self-Rewarding Language Models**<br><sub>Institution: Meta, NYU  <br>This work introduces Self-Rewarding Language Models intended to bypass the bottleneck of human preference data by self-training to enhance the model's self-rewarding and instruction-following capabilities. The experimental results are promising, setting a precursor for models that can continuously improve themselves.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.10020v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.1002.md)  |
| <span style='display: inline-block; width: 42px;'>01-18</span> | **Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation**<br><sub>Institution: The University of Tokyo, RIKEN<br>This research innovatively incorporates an explicit reasoning process and question-generation ability into LMMs, promoting more reliable inferences. By creating a new dataset and leveraging it for model training, it sets a precedent for future advancements in LMMs and enables the model to generate explicit reasoning steps and questions when faced with uncertainty.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.10005v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.10005.md)  |
| <span style='display: inline-block; width: 42px;'>01-18</span> | **A Fast, Performant, Secure Distributed Training Framework For Large Language Model**<br><sub>Institution: Ant Group China<br>This paper presents a secure distributed training framework based on model slicing, which solves the problem of model parameter and data leakage on both server and client sides while ensuring the precision of the model training and high efficiency.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.09796v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.09796.md)  |
| <span style='display: inline-block; width: 42px;'>01-18</span> | **ChatQA: Building GPT-4 Level Conversational QA Models**<br><sub>Institution: NVIDIA<br>The ChatQA model significantly improved the effectiveness of multi-turn conversational QA through a two-stage instruction tuning strategy, particularly in areas of context understanding and information retrieval.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.10225v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.10225.md)  |
| <span style='display: inline-block; width: 42px;'>01-17</span> | **Vlogger: Make Your Dream A Vlog**<br><sub>Institution: Shanghai Jiao Tong University, Shanghai AI Laboratory, Shenzhen Institute of Advanced Technology Chinese Academy of Sciences<br>This paper presents the innovative use of LLMs in the production of video blogs, addressing the challenges of creating minute-scale coherent video content and delivering exceptional experimental results.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.09414v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.09414.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/zhuangshaobin/Vlogger)</div> |
| <span style='display: inline-block; width: 42px;'>01-17</span> | **LLMs for Relational Reasoning: How Far are We?**<br><sub>Institution: Continental-NTU Corporate Lab, Nanyang Technological University, Singapore<br>The paper primarily examines the capacities and constraints of large language models in the area of relational reasoning. Through extensive assessments, including novel testing procedures and an evaluation module, the findings indicate that while LLMs perform reasonably well on certain relational reasoning tasks, they are outperformed by models specifically designed for logical reasoning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.09042v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.09042.md)  |
| <span style='display: inline-block; width: 42px;'>01-17</span> | **ReFT: Reasoning with Reinforced Fine-Tuning**<br><sub>Institution: ByteDance Research<br>ReFT significantly enhances the performance and generalization ability of LLMs in math problem-solving tasks by optimizing non-differentiable objectives through reinforcement learning. It transcends traditional supervised learning methods and shows potential for more complex reasoning tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08967v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.08967.md)  |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture**<br><sub>Institution: Microsoft<br>The paper studies the performance of large language models on agricultural data for Q&A pair generation and presents a new pipeline that efficiently utilizes RAG and fine-tuning techniques to enhance LLM applicability in specific industries, expanding the potential for LLMs' application in targeted sectors.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08406v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.08406.md)  |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation**<br><sub>Institution: Johns Hopkins University, Microsoft<br>This paper introduces CPO, a novel LLM fine-tuning method that effectively overcomes the bottlenecks in SFT for MT tasks and achieves significant performance enhancements in moderate-sized LLM translation models with minimal resource expenditure, competing alongside the most advanced state-of-the-art translation systems.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08417v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.08417.md)  |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models**<br><sub>Institution:  Zhejiang University<br>DoraemonGPT is an LLM-driven agent that employs symbolic memory and a set of tools to understand and answer complex questions involving dynamic videos. It leverages an MCTS planner to optimize the process of generating answers, enabling it to handle more complex tasks in real-world scenarios.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08392v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.08392.md)  |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models**<br><sub>Institution: Tencent AI Lab<br>The article delves into analyzing the domain mismatch problem of LLMs in machine translation tasks and experiments with the impact of varying amounts of parallel data on LLM translation capabilities, showcasing the potential of LLMs in addressing these challenges.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08350v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.0835.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/pangjh3/LLM4MT)</div> |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline**<br><sub>Institution: Alibaba Group  <br>The paper presents a new math reasoning dataset combined with a Python code interpreter, significantly improving LLM performance on math problem-solving tasks through dataset enhancement and specific fine-tuning protocols.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08190v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.0819.md)  |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **SpecGen: Automated Generation of Formal Program Specifications via Large Language Models**<br><sub>Institution: Nanjing University, Nanyang Technological University, Singapore Management University<br>The paper presents SpecGen, an automated formal program specification generation technique that combines Large Language Models with a heuristic selection strategy. By comparison with existing tools and purely LLM-based methods, SpecGen showcases superior efficiency and accuracy in specification generation and offers a dataset to facilitate future research.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08807v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.08807.md)  |
| <span style='display: inline-block; width: 42px;'>01-15</span> | **MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models**<br><sub>Institution: Microsoft Research India<br>This study investigates the performance of large language models on multilingual tasks following parameter-efficient fine-tuning, especially in the context of low-resource languages and English tasks. It demonstrates the potential of PEFT and highlights areas for future work.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07598v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.07598.md)  |
| <span style='display: inline-block; width: 42px;'>01-15</span> | **The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey**<br><sub>Institution: Technology Innovation Institute UAE, Islamic University of Technology Bangladesh, Stanford University, Amazon GenAI, AI Institute University of South Carolina<br>The paper is a detailed survey on context length extension techniques in LLMs. It provides an organized overview of current strategies and challenges for researchers in the field and encourages discussions on future advancements.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07872v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.07872.md)  |
| <span style='display: inline-block; width: 42px;'>01-15</span> | **A Study on Large Language Models' Limitations in Multiple-Choice Question Answering**<br><sub>Institution: David R. Cheriton School of Computer Science<br>The study investigates the limitations of LLMs in MCQ tasks, highlighting poor performance by most models in such tasks. It also finds model answers often depend on the order of options and proposes effective assessment methods to eliminate these biases. The paper recommends exercising caution when using MCQs to evaluate LLMs and testing whether models truly understand the task at hand.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07955v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.07955.md)  |
| <span style='display: inline-block; width: 42px;'>01-14</span> | **Small LLMs Are Weak Tool Learners: A Multi-LLM Agent**<br><sub>Institution: Sun Yat-sen University, Alibaba Group<br>The study reveals the weakness of small LLMs as tool learners and introduces the α-UMi multi-LLM framework, which outperforms the single-LLM approach. It highlights a crucial two-stage fine-tuning strategy and delves into data-scaling laws.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07324v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.07324.md)  |
| <span style='display: inline-block; width: 42px;'>01-13</span> | **Bridging the Preference Gap between Retrievers and LLMs**<br><sub>The paper presents the BGM framework to address the "preference gap" between retrievers and LLMs. Through a seq2seq bridge model and a combined SL and RL training scheme, the framework optimizes the retrieved information to fit LLMs' preferences, improving performance in multiple downstream tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06954v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06954.md)  |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models**<br><sub>Institution: University of Washington Seattle, University of Wisconsin-Madison, Stanford University<br>The paper proposes an experimental design framework intended to improve the label efficiency of large language models during Supervised fine-tuning (SFT). It shows that experimental design techniques can significantly increase label efficiency while maintaining low computational costs, saving up to 50% annotation costs in some tasks compared to random sampling.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06692v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06692.md)  |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation**<br><sub>Institution: Nanyang Technological University, Fudan University<br>This paper successfully presented a novel approach, TOOLGEN, which integrates autocompletion tools into the repository-level code generation process of LLMs, solving dependency issues and boosting both the quality and success rate of code generation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06391v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06391.md)  |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs**<br><sub>Institution: Virginia Tech, Renmin University of China, UC Davis<br>This paper presents a novel perspective on studying AI safety by humanizing LLMs, applying over a decade of social science research to AI safety, establishing a persuasion taxonomy, and creating a tool that automatically generates adversarial prompts. The results demonstrate the effectiveness of persuasion in increasing the likelihood of LLMs performing risky behaviors, while also revealing the insufficiency of current defense measures against such strategies.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06373v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06373.md)  |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape**<br><sub>Institution: Tsinghua University, University of Maryland, Beijing Xicheng Educational Research Institute  <br>This research showcases the potential of large language models in the field of education, especially within AES systems. LLMs not only have the ability to automate scoring processes but also enhance the performance of human graders through generated feedback. This advancement offers valuable insights for the future of AI-assisted education and efficient collaboration between AI and humans.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06431v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06431.md)  |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation**<br><sub>Institution: Tianyu Zheng, Shuyue Guo, Xingwei Qu, Jiawei Guo, Weixu Zhang, Xinrun Du, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu, Ge Zhang<br>The paper presents the Kun strategy, addressing the data consistency issue in Chinese large language model instruction fine-tuning, reducing dependency on manual annotation through the AP process and new data generation methods. The evaluation results indicate that the Kun strategy has a significant advantage in creating high-quality datasets.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06477v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06477.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Zheng0428/COIG-Kun)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **TestSpark: IntelliJ IDEA's Ultimate Test Generation Companion**<br><sub>Institution: JetBrains Research, Delft University of Technology<br>The paper introduces the TestSpark plugin, which integrates search-based software test generation and language model-based methods to enhance the efficiency of generating and integrating unit tests in IntelliJ IDEA, while also addressing the compilability issue of tests generated by LLMs. The open-source nature of the plugin facilitates the bridging between software developers and researchers, contributing to the practical advancement of test generation technologies.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06580v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.0658.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/JetBrains-Research/TestSpark)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding**<br><sub>Institution: Tsinghua University, Zhipu AI<br>The research presents APAR as a method that significantly enhances the decoding efficiency and generation speed of LLMs in both memory-limited and high-throughput scenarios while maintaining generation quality, providing a potent new approach for deploying large language models efficiently.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06761v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06761.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion**<br><sub>Institution: Tsinghua Shenzhen International Graduate School Tsinghua University, School of Computer Science Peking University, Baidu Inc.<br>The paper presents a method for temporal knowledge graph completion utilizing large language models. By implementing efficient fine-tuning methods and historical data augmentation with structural information, the model's reasoning capabilities and performance were improved. Experiments demonstrate that this approach effectively enhances the precision of temporal knowledge graph predictions, achieving state-of-the-art results.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06072v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06072.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase**<br><sub>Institution: LAIR Lab Lehigh University, Huazhong University of Science and Technology  <br>This study defined the mixed text (mixcase) found in mixed scenarios, created the MIXSET dataset, and provided insights and directions for solving the detection problem of mixed text. It revealed that existing detectors have shortcomings in recognizing mixcase, underlining the urgent need for more fine-grained detectors.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05952v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.05952.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Dongping-Chen/MixSet)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint**<br><sub>Institution: Gaoling School of Artificial Intelligence, Renmin University of China; School of Information, Renmin University of China; Kuaishou Technology, Beijing China.<br>This paper presents RLMEC, a novel RL method that employs generative reward models with a minimum editing mechanism, enabling precise supervision and stability in training large language models with RL.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06081v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06081.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/RUCAIBox/RLMEC)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems**<br><sub>Institution: Zhongguancun Laboratory, Tsinghua University, Institute of Information Engineering Chinese Academy of Sciences<br>This paper provides a comprehensive overview of the risk taxonomy, mitigation measures, and assessment benchmarks for large language model systems, offering a new systematic framework to help developers more comprehensively understand and deal with the potential risks of LLM systems.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05778v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.05778.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models**<br><sub>Institution: Google Research, Tel Aviv University<br>The paper presents a framework named Patchscopes, offering a novel approach to interpret the information encoded in the hidden representations of large language models (LLMs) and to correct multi-hop reasoning errors. Patchscopes serves as a general modular framework, unifying existing interpretative tools and addressing their deficiencies, while also paving the way for new research and application opportunities.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06102v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06102.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **TOFU: A Task of Fictitious Unlearning for LLMs**<br><sub>Institution: Carnegie Mellon University<br>The paper provides a new dataset and evaluation mechanisms for the issue of unlearning in LLMs. The TOFU task highlights the deficiencies of current unlearning techniques and encourages further improvements and research.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06121v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06121.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning**<br><sub>Institution: Qatar Computing Research Institute <br>This paper introduced a new single-agent, two-step prompting framework—Evidence to Generate (E2G) —aimed at improving the context reasoning abilities of LLMs. By prompting LLMs to generate evidence and explanations alongside answers, E2G reduces erroneous reasoning and enhances the accuracy of models handling various reasoning tasks. Experimental results showed that the E2G method outperforms CoT in multiple context-intensive language tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05787v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.05787.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models**<br><sub>Institution: Johns Hopkins University<br>The study demonstrates that concise Chain-of-Thought (CCoT) prompting can significantly reduce the length of text outputs in large language models without compromising performance in problem-solving tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05618v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.05618.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction**<br><sub>Institution: Fudan University, Microsoft Research Asia, Zhejiang University<br>This paper proposes EASYTOOL, a method that enhances LLM-based agents' performance in tool usage by simplifying and unifying instructions from tool documentation, addressing the issues of inconsistency, redundancy, and incompleteness.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06201v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.06201.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/microsoft/JARVIS)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks**<br><sub>InfiAgent-DABench offers a novel benchmarking tool that not only aids in measuring the performance of intelligent agents in data analysis tasks but also represents an essential step in exploring how to improve and optimize the application of LLMs in this specific domain.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05507v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.05507.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/InfiAgent/InfiAgent)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security**<br><sub>Institution: Tsinghua University, Xiaomi AI Lab<br>As a survey work, the paper presents the current status, challenges, and future trends of personal LLM agents and proposes a generic system architecture and intelligence level definition.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05459v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.05459.md)  |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **CASA: Causality-driven Argument Sufficiency Assessment**<br><sub>Institution: Peking University<br>This paper introduces a zero-shot Causality-driven Argument Sufficiency Assessment framework (CASA) based on LLMs, which effectively tackles challenges in quantifying and intervening in argument sufficiency without observational data and demonstrates its effectiveness in practical applications.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05249v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.05249.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/xxxiaol/CASA)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing**<br><sub>Institution: Google Research<br>The paper successfully proposes a new memory-based transformer method that effectively reduces memory demands and supports bidirectional attention through storage eviction policies and the ATTENDRE layer, demonstrating performance on par with traditional methods in long-sequence processing.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04881v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.04881.md)  |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis**<br><sub>Institution: Renmin University of China, Beijing Key Laboratory of Big Data Management and Analysis Methods, Meituan Group<br>This work introduced a framework named ProLLM4Rec, offering a systematic analysis of utilizing Large Language Models (LLMs) as foundation models for recommendation systems and tested the impact of different conditions on LLMs through experiments. Empirical findings were summarized, providing insights for future research.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04997v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.04997.md)  |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk**<br><sub>Institution: AWS AI Labs<br>The paper presents a novel approach for generating training data by enabling LLMs to conduct self-talk dialogues, which has the potential to improve the performance of task-oriented dialogue agents. Despite certain limitations, the findings suggest that high-quality dialogues can serve as a strong training signal for LLMs, validating the idea of LLMs' capacity to self-improve when trained on their own generated content, leading to better performance in task-oriented dialogue settings.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05033v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.05033.md)  |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **AUTOACT: Automatic Agent Learning from Scratch via Self-Planning**<br><sub>Institution: Zhejiang University, Alibaba Group<br>This research introduces AUTOACT, a framework for autonomous learning of language agents through self-instruction and self-planning to tackle the challenge of learning new tasks from scratch. The key contributions lie in its effective data augmentation method and the highly efficient automatic agent learning process.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05268v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.05268.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/zjunlp/AutoAct)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Leveraging Print Debugging to Improve Code Generation in Large Language Models**<br><sub>Institution: Zhejiang University, ByteDance<br>The paper proposes a methodology for using print debugging to guide LLMs in code generation and debugging, validating its effectiveness on the Leetcode dataset, especially for easy and medium complexity problems. Despite limited success with hard-level problems, this work represents a significant advancement in the field of LLMs for code debugging.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05319v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.05319.md)  |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Large Language Models for Robotics: Opportunities, Challenges, and Perspectives**<br><sub>Institution: Northwestern Polytechnical University, University of Georgia, Shaanxi Normal University<br>The multimodal GPT-4V framework proposed in the paper, which combines NLP and visual perception, aims to tackle challenges faced by LLMs in robotic task planning. It holds significant implications for advancing human-machine interaction and shaping the future of intelligent AI systems.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04334v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.04334.md)  |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Agent Alignment in Evolving Social Norms**<br><sub>Institution: Fudan University<br>This paper introduces an EvolutionaryAgent framework to assess and enhance the adaptiveness and alignment of large intelligent agents in dynamic and constantly evolving societal norms. The research highlights the significance of agent alignment with societal norms during evolution and validates the framework's efficacy through experiments.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04620v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.0462.md)  |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **The Critique of Critique**<br><sub>Institution: The Hong Kong Polytechnic University, Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory<br>METACRITIQUE is the first framework to evaluate natural language critiques, assessing the quality of critiques using principles of precision and recall, and has achieved a high level of interpretability and transparency.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04518v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.04518.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/GAIR-NLP/MetaCritique)</div> |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search**<br><sub>Institution: Nanyang Technological University Singapore<br>ReCo significantly enhances code search accuracy by utilizing LLMs to rewrite code in the codebase through style normalization and introduces a new metric, CSSim, to quantify stylistic differences, advancing research in code style normalization.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04514v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.04514.md)  |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding**<br><sub>Institution: University of California San Diego, Google Cloud AI Research, Google Research<br>The paper introduces the innovative CHAIN-OF-TABLE framework, which enhances reasoning capabilities of LLMs by explicitly incorporating tabular data into the reasoning chain, dynamically planning and updating the process, thereby increasing accuracy and reliability for table-based reasoning tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04398v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.04398.md)  |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs**<br><sub>Institution: Zhejiang University, Ant Group<br>The paper presents a new method named ARALLM that combines analogical reasoning and multi-task model distillation to effectively enhance LLMs' ability to understand and transform natural language into structured logical expressions. This method allows non-expert marketers to use natural language for user targeting, which potentially changes the practice of user targeting. The improvement in this capability not only has practical value in marketing scenarios but also contributes valuable exploration to the functionality and practicality of large language models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04319v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.04319.md)  |
| <span style='display: inline-block; width: 42px;'>01-08</span> | **MARG: Multi-Agent Review Generation for Scientific Papers**<br><sub>Institution: Northwestern University, The Hebrew University of Jerusalem, Allen Institute for AI<br>This paper presents an innovative multi-agent review generation method (MARG) capable of overcoming the context size limitations of the base model and of generating high-quality peer-review feedback for scientific papers. The quality of feedback generated by MARG significantly surpasses the baselines in user studies and automated metrics, with a 2.2-fold increase in the number of helpful comments and a greater generation of specific comments.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04259v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.04259.md)  |
| <span style='display: inline-block; width: 42px;'>01-08</span> | **TTMs: Fast Multi-level Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting of Multivariate Time Series**<br><sub>Institution: IBM Research<br>TTM demonstrates the effectiveness and transfer learning capabilities of tiny pretrained models that are exclusively trained on diverse time series data for improved multivariate time series forecasting in few/zero-shot scenarios.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03955v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.03955.md)  |
| <span style='display: inline-block; width: 42px;'>01-08</span> | **SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems**<br><sub>Institution: Fudan University<br>The paper proposed a multi-modal large language model-based multi-agent system—SpeechAgents, capable of simulating human communication scenarios involving up to 25 agents, exhibiting exceptional scalability. By utilizing multi-modal signals as the medium for agent communication, the system not only can simulate dialogues with correct content, authentic rhythm, and rich emotions but also can be applied to tasks such as drama creation and the generation of audio novels.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03945v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.03945.md)  |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **Grimoire is All You Need for Enhancing Large Language Models**<br><sub>Institution: Beihang University, Renmin University of China<br>The paper introduces a method named SLEICL that significantly enhances the ICL capability of weak language models by learning and transferring skills from strong language models. The effectiveness of the method is validated through experiments, demonstrating the potential of this technology in enhancing weak language models' context learning abilities.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03385v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.03385.md)  |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback**<br><sub>Institution: University of Louisville, Microsoft<br>This paper explores the efficacy of ChatGPT as a conversational recommendation system. It develops a process around ChatGPT that simulates real-user scenarios and addresses and mitigates popularity bias.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03605v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.03605.md)  |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon**<br><sub>Institution: Beijing Academy of Artificial Intelligence, Renmin University of China, Nankai University<br>The paper introduces Activation Beacon, a new technique to extend the context length of Large Language Models, enabling the perception of extensive context within a limited context window, while fully preserving capability on short contexts. Activation Beacon provides an effective, efficient, compatible, and low-training-cost method for extending LLMs' context length.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03462v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.03462.md)  |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects**<br><sub>Institution: The Chinese University of Hong Kong, DeepWisdom, Peking University<br>The paper presents a framework for guiding future research and development of LLM-based intelligent agent systems, explores different methods of improving their planning capabilities, multimodal information processing, and how to address the challenges faced by LLM agents, offering a clear guide for future research directions.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03428v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.03428.md)  |
| <span style='display: inline-block; width: 42px;'>01-06</span> | **The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models**<br><sub>Institution: Renmin University of China, Université de Montréal<br>The paper provides a systematic empirical study to deeply understand and explore the problem of hallucinations in large language models, identifying the sources of hallucination, detection methods, mitigation strategies, and proposing the new benchmark HaluEval 2.0 and a simple yet effective hallucination detection framework.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03205v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.03205.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/RUCAIBox/HaluEval-2.0)</div> |
| <span style='display: inline-block; width: 42px;'>01-06</span> | **Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification**<br><sub>Institution: Aerospace Information Research Institute Chinese Academy of Sciences, Key Laboratory of Target Cognition and Application Technology, University of Chinese Academy of Sciences<br>This study introduced Quartet Logic: A Four-Step Reasoning (QLFR) framework for short-text classification tasks and a CoT-Driven Multi-task learning (QLFR-CML) method. Both of these approaches use the reasoning chain of large language models to address challenges in the STC field. Experimental results confirm the effectiveness and applicability of these methods.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03158v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.03158.md)  |
| <span style='display: inline-block; width: 42px;'>01-06</span> | **CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models**<br><sub>Institution: Harbin Institute of Technology, Kuaishou Technology<br>CogGPT addresses challenges faced by large language models in emulating human cognitive dynamics by introducing an iterative cognitive mechanism and a memory retention system, showcasing impressive performance in continuous information processing.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08438v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.08438.md)  |
| <span style='display: inline-block; width: 42px;'>01-05</span> | **Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache**<br><sub>Institution: Alibaba Group, Shanghai Jiao Tong University<br>The paper presents an efficient system for cloud services supporting long-context language models. Through the distributed algorithm DistAttention, it optimizes the processing and storage of the attention module, and the DistKV-LLM service system manages and coordinates it. It achieves efficient allocation and management of resources in a distributed environment, demonstrating significant performance improvements.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02669v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.02669.md)  |
| <span style='display: inline-block; width: 42px;'>01-05</span> | **From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models**<br><sub>Institution: Beike Inc.<br>The paper introduces the RAISE framework, which enhances the performance of LLMs in multi-turn dialogues, especially in real estate sales contexts, by incorporating an augmented memory system and a structured agent construction process.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02777v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.02777.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **LLM Augmented LLMs: Expanding Capabilities through Composition**<br><sub>Institution: Google Research, Google DeepMind<br>The paper presents a new framework for model extension - CALM, which successfully integrates two large language models to perform new tasks and demonstrates its effectiveness across multiple experiments.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02412v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.02412.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval**<br><sub>Institution: Columbia University<br>This paper proposes SPEER, a sentence-level planning method through embedded entity retrieval for long document tasks of hospital discharge summaries. It guides large language models (LLMs) to better cover key entities and generate more complete and credible clinical summaries. The research demonstrates that the SPEER method can improve document coverage and accuracy in practical applications, thereby reducing the documentation burden on clinicians.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02369v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.02369.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives**<br><sub>Institution: Zhejiang University, OPPO Research Institute<br>This paper introduces a new strategy called "Self-Contrast" to address issues of stubbornness and inconsistency in reflection and self-correction processes within Large Language Models (LLMs). By creating diverse solving perspectives, contrasting different solutions, and summarizing disparities into a checklist, it enhances the quality of LLM reflection. The approach's effectiveness and broad applicability are validated through experiments.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02009v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.02009.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **Using LLM to select the right SQL Query from candidates**<br><sub>Institution: Peking University<br>This research proposes a method for automatically generating test cases for text-to-SQL using LLMs and presents a three-step re-ranking process. The method significantly improves the performance of existing text-to-SQL models, as evidenced by experiments.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02115v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.02115.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers**<br><sub>Institution: Bytedance Inc.<br>This paper introduces a methodology, ICE-GRT, designed to enhance the depth and accuracy of LLMs in handling domain-specific tasks. By incorporating reinforcement learning from human feedback, ICE-GRT significantly improves domain-specific capabilities without sacrificing general task performance, achieving state-of-the-art in several assessment tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02072v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.02072.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)**<br><sub>Institution: University of South Carolina, New Mexico State University, IBM Research<br>This paper is a survey of the application of Large Language Models in the field of Automated Planning and Scheduling, proposing the prospect of combining leading LLMs like GPT-4 and BERT with classical planning methods and the potential of applying LLMs in eight different planning problem categories, with the aim to develop more advanced and intelligent planning systems.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02500v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.025.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)**<br><sub>Institution: University of South Carolina, New Mexico State University, IBM Research<br>The paper provides insights into the integration prospects of Large Language Models (LLMs) with Automated Planning and Scheduling (APS), breaking through the traditionally limited adaptability to context, and offers a possibility for a more dynamic, context-aware planning pathway, laying a foundation for further application and research.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02500v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.025.md)  |
| <span style='display: inline-block; width: 42px;'>01-03</span> | **Social Media Ready Caption Generation for Brands**<br><sub>Institution: Adobe Research India<br>The paper introduces a new framework designed to aid brands in creating engaging captions on social media that align with their brand image and personality. The framework, which consists of two parts, successfully addresses the challenge of generating socially engaging and relevant captions for brands.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01637v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.01637.md)  |
| <span style='display: inline-block; width: 42px;'>01-03</span> | **MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries**<br><sub>Institution: Indian Institute of Technology Patna, Stanford University, Amazon GenAI<br>MedSumm presents a novel approach for multimodal medical question summarization, integrating textual and visual information to create medically detailed summaries potentially enhancing the quality of healthcare decision-making and deepening the understanding of patient queries.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01596v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.01596.md)  |
| <span style='display: inline-block; width: 42px;'>01-02</span> | **A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models**<br><sub>Institution: Islamic University of Technology Bangladesh, University of South Carolina, Stanford University<br>This paper offers an exhaustive survey on hallucination mitigation techniques in LLMs, proposing a categorization framework and systematic feedback and reasoning methods, and assesses the efficacy and impact of these techniques.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01313v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.01313.md)  |
| <span style='display: inline-block; width: 42px;'>01-02</span> | **LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning**<br><sub>The paper successfully presents a method for extending the context window of LLMs without fine-tuning, which is crucial for improving the capability of large language models to process long texts when computational resources are limited.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01325v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.01325.md)  |
| <span style='display: inline-block; width: 42px;'>01-01</span> | **The Earth is Flat? Unveiling Factual Errors in Large Language Models**<br><sub>Institution: The Chinese University of Hong Kong, Tencent AI Lab<br>The FactChecker introduced in this paper provides a new automated framework for testing factual inaccuracies in large language models and has been shown to uncover and reduce factual errors in these models through the construction of knowledge graphs and the generation of test questions.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00761v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.00761.md)  |
| <span style='display: inline-block; width: 42px;'>01-01</span> | **A & B == B & A: Triggering Logical Reasoning Failures in Large Language Models**<br><sub>Institution: The Chinese University of Hong Kong, Tencent AI Lab<br>This work proposes LogicAsker, addressing the challenge of evaluating and improving the logical reasoning abilities of LLMs through comprehensive assessment and effective enhancement via problem generation and in-context learning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00757v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.00757.md)  |
| <span style='display: inline-block; width: 42px;'>01-01</span> | **From Prompt Engineering to Prompt Science With Human in the Loop**<br><sub>Institution: University of Washington<br>The paper demonstrates how to transition prompt engineering for LLMs into a more scientific and systematic prompt science. By incorporating a qualitative coding method analogous to the human-in-the-loop approach, it ensures the quality and consistency of the responses generated by the LLM while eliminating individual subjectivity and randomness.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04122v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2024-01/2401.04122.md)  |

---

## 2023-12

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>12-31</span> | **BatchEval: Towards Human-like Text Evaluation**<br><sub>Institution: Beijing Institute of Technology, Xiaohongshu Inc  <br>The paper introduces a novel LLM evaluation paradigm—BATCHEVAL—that addresses the issues of robustness and consistency with human judgment in automatic text evaluation. By implementing batch-wise evaluation and iterative processing, BATCHEVAL significantly surpasses existing methods in terms of accuracy and cost-efficiency.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00437v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2401.00437.md)  |
| <span style='display: inline-block; width: 42px;'>12-31</span> | **Improving Text Embeddings with Large Language Models**<br><sub>Institution: Microsoft Corporation<br>The paper introduces an innovative text embedding approach utilizing the latest LLMs and synthetic data to match performance on competitive benchmarks with fewer than 1,000 training steps and no label data, offering strong evidence for further advancements in text embedding technology.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00368v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2401.00368.md)  |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **DB-GPT: Empowering Database Interactions with Private Large Language Models**<br><sub>Institution: Alibaba Group<br>This paper presents DB-GPT, an innovation integrating LLMs and database systems to enhance user experience and accessibility, demonstrating a hierarchical design that effectively addresses concerns such as privacy and security protection, while also elevating the system's overall performance and efficiency through multi-source RAG and adaptive ICL mechanisms.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17449v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17449.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/eosphoros-ai/DB-GPT)</div> |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **Building Efficient Universal Classifiers with Natural Language Inference**<br><sub>Institution: Vrije Universiteit Amsterdam, University of London Royal Holloway, Hugging Face<br>The paper provides a novel approach to universal text classification using natural language inference, complete with detailed steps and tools needed to implement the method, significantly increasing model efficiency without compromising performance.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17543v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17543.md)  |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception**<br><sub>Institution: **Institution:** Shanghai Key Laboratory of Data Science School of Computer Science Fudan University, School of Data Science Fudan University, DataGrand Co. LTD<br>This research has significantly improved LLMs' quantitative reasoning abilities by establishing a dimensional unit knowledge base and a customized benchmark test, providing a new pathway for understanding and reasoning accurately with vital quantitative information in text.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17532v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17532.md)  |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **The Right Prompts for the Job: Repair Code-Review Defects with Large Language Model**<br><sub>Institution: Ant Group, Nanjing University<br>The research explores the application of LLMs in repairing code review defects, introduces an effective semi-automated APR paradigm, analyzes the performance of 9 popular models, and designs effective prompts to guide the code repair process.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17485v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17485.md)  |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17484v1)</div><div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/jongjyh/trfr)</div> |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs**<br><sub>Institution: Chinese University of Hong Kong, Tencent AI Lab<br>This paper presents a new evaluation paradigm that challenges LLMs to engage in meta-reasoning, and it introduces the accompanying open-source benchmark DiagGSM8K, adding a new dimension to the evaluation of LLMs' cognitive abilities.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17080v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1708.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Structured Packing in LLM Training Improves Long Context Utilization**<br><sub>Institution: University of Warsaw, Google DeepMind, Polish Academy of Sciences<br>This paper introduces the SPLICE method to enhance utilization of long-range contexts and validates its effectiveness in improving context utilization and performance on long-context tasks for large-scale language models. SPLICE is especially applicable for constructing training examples in training datasets that lack additional structured information.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17296v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17296.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Improving In-context Learning via Bidirectional Alignment**<br><sub>Institution: Nanyang Technological University, Princeton University, Salesforce Research USA<br>The paper introduced Bidirectional Alignment (BiAlign), which effectively improves the ICL abilities of smaller models by integrating a new ranking loss along with aligning the output distribution.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17055v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17055.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension**<br><sub>Institution: Tsinghua University, Renmin University of China<br>This paper introduces GITAGENT, an autonomous agent that can extend tools from GitHub to meet the varied demands of user queries. By addressing the challenge of non-standardization, GITAGENT autonomously learns human experience from GitHub Issues/PRs to overcome problems during tool extension, showing its effectiveness in autonomously integrating tools for task accomplishment across various domains.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17294v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17294.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos**<br><sub>Institution: Tsinghua University<br>This paper presents the Grounding-Prompter method, addressing the TSG challenge in long videos by combining LLM with temporal reasoning and multimodal information, demonstrating the effectiveness of prompting LLM with multimodal data, and validating its superiority in TSG tasks for long videos through experiments.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17117v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17117.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs**<br><sub>Institution: Chinese University of Hong Kong, Tencent AI Lab<br>This paper presents an innovative evaluation paradigm for LLMs, emphasizing meta-reasoning, which is assessing the reasoning process itself. This approach promises to uncover cognitive deficiencies overlooked by result-oriented evaluation methods, providing a new direction for future LLM assessment and training.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17080v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1708.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **DrugAssist: A Large Language Model for Molecule Optimization**<br><sub>Institution: Tencent AI Lab, Department of Computer Science Hunan University<br>DrugAssist is a model that facilitates molecule optimization through human-machine interaction, overcoming the lack of interactivity limitations in LLM applications for drug discovery and showcasing superior multi-property optimization abilities.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.10334v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2401.10334.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/blazerye/DrugAssist)</div> |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Experiential Co-Learning of Software-Developing Agents**<br><sub>Institution: Tsinghua University,Dalian University of Technology,Beijing University of Posts and Telecommunications<br>The paper proposes a new framework named Experiential Co-Learning, which through the sequential implementation of co-tracking, co-memorizing, and co-reasoning modules, allows LLM-driven intelligent agents to learn more effectively from historical trajectories and use past experiences to reason mutually when solving new tasks. It shows a clear performance improvement over existing technologies.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17025v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17025.md)  |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **Rethinking Tabular Data Understanding with Large Language Models**<br><sub>Institution: UC San Diego, USC, UC Davis  <br>The paper delves into the understanding and reasoning capabilities of LLMs over tabular data, contributing insights into the robustness of table structure, the comparison of textual versus symbolic reasoning, and the impact of aggregating multiple reasoning pathways on model performance. The proposed table structure normalization method and the mix self-consistency mechanism are instrumental in enhancing LLMs' performance in tabular data reasoning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16702v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.16702.md)  |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **Conversational Question Answering with Reformulations over Knowledge Graph**<br><sub>Institution: University of Illinois at Urbana-Champaign, Amazon<br>CoRnNet represents a novel RL model for non-dialogue ConvQA tasks with LLM-generated reformulations, showing superior performance over other advanced models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17269v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17269.md)  |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **How Robust are LLMs to In-Context Majority Label Bias?**<br><sub>Institution: Amazon<br>The article conducts a comprehensive study on the robustness of LLMs when faced with majority label bias in ICL, finding significant stability in certain models in handling such bias.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16549v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.16549.md)  |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges**<br><sub>Institution: Shanghai Jiao Tong University (SJTU)<br>This paper is a survey on how to adapt large language models for the education system. It provides an overview of the development of LLMs in education-related capabilities, explores the potential and challenges in building such systems, and offers insights for future related research.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08664v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2401.08664.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **A Prompt Learning Framework for Source Code Summarization**<br><sub>Institution: Nanyang Technological University, Tencent Inc., Nanjing University<br>This paper introduced a novel PromptCS framework for source code summarization, capable of generating high-quality summaries while reducing training costs and providing open-source code for further research.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16066v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.16066.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Supervised Knowledge Makes Large Language Models Better In-context Learners**<br><sub>Institution: School of Engineering Westlake University, Westlake Institute for Advanced Study, Peking University<br>The SuperContext framework proposed in the paper significantly enhances the generalizability and factuality of LLMs in natural language understanding and question answering tasks by leveraging the supervised knowledge from task-specific fine-tuned SLMs. It represents an innovative approach to incorporating the strengths of small models into LLMs to deal with OOD data and minimize hallucinations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15918v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.15918.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Align on the Fly: Adapting Chatbot Behavior to Established Norms**<br><sub>Institution: Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, The Hong Kong Polytechnic University<br>The research advances a dynamic OPO method that aligns LLMs with the complex and varying landscape of human values in real-time, using collected rules as external memory without further training. Despite limitations in inference efficiency and potential for retrieval model enhancements, extensive experiments across multiple evaluation datasets vouch for the method's effectiveness.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15907v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.15907.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/GAIR-NLP/OPO)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Think and Retrieval: A Hypothesis Knowledge Graph Enhanced Medical Large Language Models**<br><sub>Institution: Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education; School of Computer Science Peking University, Beijing China<br>The HyKGE framework effectively addresses the accuracy and interpretability challenges faced by large language models in dealing with complex problems in the medical field, demonstrating potential for applications in the medical domain and showcasing its superiority in real-world scenarios.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15883v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.15883.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph**<br><sub>Institution: Northeastern University, Neusoft AI Magic Technology Research, Neusoft Institute of Intelligent Medical Research<br>The paper introduces KnowledgeNavigator, a novel framework designed to enhance LLM reasoning over knowledge graphs, addressing LLM's limitations in complex reasoning tasks. The effectiveness demonstrated by the experiments suggests potential for broader application of LLMs in high-risk and sensitive domains.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15880v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1588.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder Models**<br><sub>Institution: University of Waterloo<br>The paper introduces LiT5-Distill and LiT5-Score, two sequence-to-sequence encoder-decoder models for efficient zero-shot listwise reranking. These methods not only offer competitive performance but also address traditional reliance on large LLMs and external relevance labels, showcasing optimization and advancement in this domain.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16098v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.16098.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/castorini/LiT5)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation**<br><sub>Institution: City University of Hong Kong, The Chinese University of Hong Kong, Hangdian University<br>The paper presents a novel framework named RecRanker, which optimizes the performance of LLMs in top-k recommendation tasks through instruction tuning and effectively integrates signals from traditional recommendation systems, improving the model's application performance in recommendation scenarios.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16018v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.16018.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Aligning Large Language Models with Human Preferences through Representation Engineering**<br><sub>Institution: Fudan University  <br>This paper introduces a novel RAHF method, which manipulates internal model representations through representation engineering techniques to align LLMs with human preferences. The method is computationally efficient, easy to implement, and shows potential in managing a spectrum of human preferences or values.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15997v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.15997.md)  |
| <span style='display: inline-block; width: 42px;'>12-25</span> | **ESGReveal: An LLM-based approach for extracting structured data from ESG reports**<br><sub>Institution: Alibaba Cloud, Tsinghua University, Sun Yat-Sen University<br>ESGReveal marks significant progress in ESG data processing, aiming to improve the consistency and accuracy of structured data extraction from corporate reports through large language models and related techniques, and it has driven improvements in ESG practices and transparency.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17264v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.17264.md)  |
| <span style='display: inline-block; width: 42px;'>12-25</span> | **Alleviating Hallucinations of Large Language Models through Induced Hallucinations**<br><sub>Institution: Soochow University, Tencent AI Lab<br>The paper offers a novel method to reduce hallucinations in LLMs by constructing a factually weaker model and subtracting its knowledge in the generation process, improving the generation of factual content.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15710v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1571.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **YAYI 2: Multilingual Open-Source Large Language Models**<br><sub>Institution: Beijing Wenge Technology Co. Ltd., Institute of Automation Chinese Academy of Sciences<br>The paper presents YAYI 2, a large language model optimized for multilingual scenarios, which significantly improves performance on various tasks, especially in Chinese-related tasks, by pre-training on a large corpus and aligning with human values through multiple approaches.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14862v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.14862.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning**<br><sub>Institution: Huawei Noah's Ark Lab, University College London, University of Oxford<br>The paper introduces the Pangu-Agent framework, which addresses the challenges faced by standard RL methods in multi-task environments. By integrating structured reasoning through intrinsic functions and enabling fine-tuning through supervised learning and RL, Pangu-Agent enhances the ability of agents to adapt across various environmental interactions.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14878v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.14878.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes**<br><sub>Institution: University of Michigan, Rutgers University<br>This paper presents a novel method of assessing the reasoning abilities of LLMs through the NPHardEval benchmark. The benchmark covers a broad range of problems from polynomial time complexity to NP-Hard levels, and it features a dynamic data updating mechanism to prevent model overfitting, ensuring reliable and authentic assessment results. The findings significantly advance the understanding of current capabilities of LLMs and pave the way for improving the reasoning abilities of these models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14890v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1489.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/casmlab/NPHardEval)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Plan, Posture and Go: Towards Open-World Text-to-Motion Generation**<br><sub>Institution: Tsinghua University, Microsoft Research Asia<br>The researchers introduced a new framework named PRO-Motion to overcome limitations of traditional text-to-motion generation methods, successfully generating more diverse and realistic motions in open-world scenarios.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14828v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.14828.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Large Language Model (LLM) Bias Index -- LLMBI**<br><sub>Institution: University of Oxford, University Canada West, Amazon Web Services (AWS)<br>The introduction of LLMBI marks a significant step towards creating fairer and more reliable LLMs. It provides a quantifiable measure of bias for system engineers and researchers, guiding them to continuously improve these powerful models and ensuring that they reflect society's diverse and evolving fabric.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14769v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.14769.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Generative AI Beyond LLMs: System Implications of Multi-Modal Generation**<br><sub>The paper is the first to characterize system performance for models that span across text, image, and video generation, revealing unique system properties distinct from traditional LLMs. It also highlights challenges and opportunities where traditional optimizations might need rethinking for TTI/TTV models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14385v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.14385.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **A Survey of Reinforcement Learning from Human Feedback**<br><sub>Institution: LMU Munich, Duke Kunshan University<br>This article is a survey of RLHF, analyzing its applications at the crossroads of artificial intelligence and human-computer interaction and discussing the latest research trends, especially those related to LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14925v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.14925.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Reasons to Reject? Aligning Language Models with Judgments**<br><sub>Institution: Tencent AI Lab, The Chinese University of Hong Kong<br>The paper presents a new framework for aligning LLMs through direct use of language feedback named Contrastive Unlikelihood Training (CUT) and demonstrates its effectiveness in various scenarios including offline and online alignment, as well as further optimizing both unaligned (cold-start) and already aligned (warm-start) models. Research indicates that judgmental feedback holds greater potential than rewards for aligning LLMs, meriting further investigation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14591v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.14591.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation**<br><sub>Institution: University of Waterloo, IN.AI Research<br>The paper presents an evaluation framework called VIEScore aimed at providing explainable evaluations for conditional image generation tasks. VIEScore overcomes the challenge of existing automated metrics' inability to explain their scoring rationale and is adaptable to various task requirements.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14867v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.14867.md)  |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **De novo Drug Design using Reinforcement Learning with Multiple GPT Agents**<br><sub>Institution: Tsinghua University, Microsoft Research AI<br>The paper introduces a reinforcement learning algorithm with multiple GPT agents for drug molecular generation and demonstrates good performance and practicality in GuacaMol benchmark tests and in designing inhibitors for SARS-CoV-2 protein targets.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06155v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2401.06155.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HXYfighter/MolRL-MGPT)</div> |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **AppAgent: Multimodal Agents as Smartphone Users**<br><sub>Institution: Tencent  <br>The study introduces an innovative multimodal agent framework allowing the agent to operate any smartphone application like a human user by learning new apps through autonomous exploration and observing human demonstrations. Findings demonstrate the framework's efficiency and adaptability in performing a variety of advanced tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13771v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.13771.md)  |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction**<br><sub>Institution: MIT, Microsoft Research NYC<br>The paper introduces LASER, a strategy for pruning specific layers of the Transformer model after training to enhance its performance. The authors indicate that this strategy is not only effective, but also the first discovery of enhancing the performance of Transformer models through carefully selected pruning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13558v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.13558.md)  |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning**<br><sub>Institution: Language Technology Lab University of Cambridge<br>This paper offers a comprehensive analysis of the performance and calibration of different learning methods in data-scarce scenarios. It indicates challenges in jointly achieving high performance and good calibration, but demonstrates that self-ensembling techniques can enhance model calibration without sacrificing performance, providing important guidelines for future LLMs applications.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13772v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.13772.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Lampr: Boosting the Effectiveness of Language-Generic Program Reduction via Large Language Models**<br><sub>Institution: University of Waterloo, The Hong Kong University of Science and Technology, Concordia University<br>Lampr represents a pioneering algorithm that integrates LLMs into the program reduction process. It achieves a balance between cross-language generality and particular language semantic awareness through a multi-level prompting method and assistance from LLMs, with superior performance demonstrated in empirical evaluations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13064v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.13064.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation**<br><sub>Institution: The University of Hong Kong, Shanghai Jiao Tong University, King’s College London<br>This paper presents a novel multi-agent-based solution for code generation, AgentCoder, which effectively solves the balance problem between code generation and testing through specific agents focused on code generation, test designing, and test execution, achieving code generation quality that outperforms existing SOTA methods.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13010v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1301.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy**<br><sub>Institution: Ant Group<br>The paper presents the Lookahead inference acceleration framework, which uses a Trie-tree based multi-branch inferencing strategy to improve the inference speed of LLMs while maintaining the accuracy of generation. The framework's performance is validated through extensive experimentation and has been deployed in real-world scenarios at Alipay.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12728v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.12728.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Mini-GPTs: Efficient Large Language Models through Contextual Pruning**<br><sub>Institution: Massachusetts Institute of Technology<br>The paper demonstrates the process and results of developing Mini-GPTs, smaller yet efficient versions of GPT models, through contextual pruning. This method successfully reduced the size of LLMs across various domain-specific datasets while upkeeping performance, proving that pruning techniques are not only theoretically viable but also practically valuable in developing resource-efficient, domain-specific LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12682v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.12682.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation**<br><sub>Institution: The University of Hong Kong, Shanghai Jiao Tong University<br>AgentCoder represents a novel multi-agent framework that significantly improves the quality and accuracy of automated code generation by performing iterative testing and optimization, especially exhibiting its advantages in handling enhanced datasets with more challenging testing requirements.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13010v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1301.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Time is Encoded in the Weights of Finetuned Language Models**<br><sub>The research introduces the concept of time vectors, showing how temporal variations can be encoded to some extent in language model weight space, and how weight interpolation can assist in tailoring models to new time periods.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13401v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.13401.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Generative Multimodal Models are In-Context Learners**<br><sub>Institution: Beijing Academy of Artificial Intelligence, Tsinghua University, Peking University<br>The paper successfully enhances the context learning capabilities of the multimodal generative model Emu2 by scaling up the model and achieves breakthrough results on a spectrum of multimodal understanding tasks, especially in visual question-answering and controllable visual generation after instruction tuning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13286v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.13286.md)  |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes**<br><sub>Institution: University of Cambridge<br>This paper introduces CLLM, a novel methodology that combines the prior knowledge of Large Language Models with a robust data-centric approach to data augmentation, paving the way for the broader application of ML in data-deprived domains and regions.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12112v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.12112.md)  |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT**<br><sub>The paper presented the first publicly available benchmark dataset for fake news detection, ChatGPT-FC, which combines human verification and ChatGPT assistance. Quantitative analysis was conducted to compare human journalists and LLMs in fact-checking, highlighting the potential of LLMs to enhance the objectivity and reliability of news fact-checking processes.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11870v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1187.md)  |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **Active Preference Inference using Language Models and Probabilistic Reasoning**<br><sub>Institution: Cornell University, Cornell Tech<br>This study introduced a real-time algorithm that accelerates LLMs' inference of user preferences by generating informative questions, demonstrated to reduce user interaction and improve task performance in an online shopping scenario.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12009v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.12009.md)  |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **Text-Conditioned Resampler For Long Form Video Understanding**<br><sub>Institution: University of Oxford, Google, Google DeepMind<br>This paper presents TCR, a novel architecture and pre-training method capable of processing long videos conditioned on textual prompts. It effectively bridges pre-trained visual encoders with LLMs, addressing the challenge of long-form video understanding and sets new best performance benchmarks across several evaluation tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11897v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.11897.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Towards Better Serialization of Tabular Data for Few-shot Classification with Large Language Models**<br><sub>Institution: Carnegie Mellon University<br>The paper successfully showcased innovative application of Large Language Models in tabular data classification, with a focus on the new LaTeX serialization framework, introducing novel serialization methods effective for domain-specific datasets. It also explored the LLMs' capability to interpret complex data relationships more deeply. The paper's LaTeX serialization method not only enhanced LLM performance in classification tasks but also significantly improved memory and computational efficiency.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12464v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.12464.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation**<br><sub>Institution: University of Waterloo, Huawei Noah’s Ark Lab, FEEC-Unicamp Brazil<br>This work introduces the NoMIRACL dataset, providing a multilingual tool for assessing robustness in LLMs during retrieval-augmented generation, and showcases challenges that LLMs face in differentiating between relevant and non-relevant retrieval results, highlighting the need for future research to improve LLM robustness.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11361v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.11361.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Generalized Category Discovery with Large Language Models in the Loop**<br><sub>This paper presents an end-to-end active learning framework that incorporates Large Language Models into the training loop, significantly enhancing model performance on the task of generalized category discovery and autonomously generating category names.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10897v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10897.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **From Google Gemini to OpenAI Q-Star: A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape**<br><sub>Institution: Cyberstronomy Pty Ltd, Academies Australasia Polytechnic, Massey University<br>This review extensively analyzes the development of the generative AI field and its reshaping effects on the research landscape, with a special focus on MoE multimodality learning and AGI prospects. The study spans a comprehensive taxonomy from AI model structures and training techniques to application domains and ethical considerations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10868v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10868.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Retrieval-Augmented Generation for Large Language Models: A Survey**<br><sub>Institution: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University, Fudan University<br>The paper offers a thorough and systematic overview of the RAG domain, emphasizing the importance of enhancing the retrieval and generative capabilities of LLMs, highlighting current challenges, and envisioning future research directions.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10997v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10997.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Agent-based Learning of Materials Datasets from Scientific Literature**<br><sub>Institution: University of Toronto  <br>This paper showcases the capability of an intelligence agent based on large language models to autonomously learn and extract material-related datasets from scientific literature. Eunomia demonstrated effectiveness in entity and relation extraction without any fine-tuning and could enhance its ability to avoid errors in complex tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11690v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1169.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/AI4ChemS/Eunomia)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows**<br><sub>Institution: University of Washington, Stanford University, Allen Institute for AI<br>The paper introduces a design space framework and three case studies adapting crowdsourcing workflows to LLM chains, providing practical guidance and theoretical insights for the future design and development of LLM chains.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11681v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.11681.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Social Learning: Towards Collaborative Learning with Large Language Models**<br><sub>Institution: Google, EPFL<br>The paper presents a novel framework for knowledge transfer in LLMs—social learning, and provides solutions for privacy protection. The framework allows for knowledge exchange between models using natural language while preventing the leakage of sensitive information, and it validates its effectiveness and privacy-preserving capabilities through experimentation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11441v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.11441.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model**<br><sub>Institution: Huawei Noah's Ark Lab, The University of Hong Kong, The Hong Kong University of Science and Technology<br>This paper overcomes the limitations of multimodal large language models in solving geometric problems by constructing the Geo170K dataset and developing the G-LLaVA model based on it, achieving better performance than existing state-of-the-art models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11370v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1137.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **"Paraphrasing The Original Text" Makes High Accuracy Long-Context QA**<br><sub>Institution: Tsinghua University  <br>The paper presents a low-cost, effective approach to extending the capability of existing language models to handle long texts, significantly improving accuracy in long-context question answering by theoretical demonstration and experimental validation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11193v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.11193.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **MAC-SQL: Multi-Agent Collaboration for Text-to-SQL**<br><sub>Institution: Beihang University, Tencent Cloud AI<br>Overall, the MAC-SQL framework addresses key challenges in the Text-to-SQL task by collaborating with intelligent agents, tackling issues like managing extensive databases, complex queries, and SQL verification and correction. The release of the open-source SQL-Llama model shows promising results and has the potential to perform comparably to proprietary models like GPT-4.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11242v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.11242.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/wbbeyourself/MAC-SQL)</div> |
| <span style='display: inline-block; width: 42px;'>12-17</span> | **Mixed Distillation Helps Smaller Language Model Better Reasoning**<br><sub>Institution: Zhejiang University, Dalian Medical University<br>The Mixed Distillation framework significantly enhanced smaller models' advanced reasoning capabilities by integrating PoT and CoT abilities from LLMs, specifically showing improved performance in mathematical reasoning tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10730v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1073.md)  |
| <span style='display: inline-block; width: 42px;'>12-17</span> | **Distinguishing Translations by Human, NMT, and ChatGPT: A Linguistic and Statistical Approach**<br><sub>Institution: Shanghai Jiao Tong University<br>The study provides tentative answers to the possibility of ChatGPT being an alternative translation tool apart from NMT and showcases its distinctive properties compared to NMT and HT. These novel insights may inform the future development of more human-like and contextually appropriate translation systems and offer guidance on how to effectively use AI-generated translations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10750v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.1075.md)  |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **RIGHT: Retrieval-augmented Generation for Mainstream Hashtag Recommendation**<br><sub>Institution: CAS Key Lab of Network Data Science and Technology ICT CAS, University of Chinese Academy of Sciences Beijing China<br>The paper presents a new retrieval-augmented generative system for mainstream hashtag recommendation (RIGHT), combining the strengths of retrievers, selectors, and generators to overcome existing methods' limitations in processing new information and identifying mainstream tags, and demonstrates significant experimental results.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10466v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10466.md)  |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **A Survey on Robotic Manipulation of Deformable Objects: Recent Advances, Open Challenges and New Frontiers**<br><sub>Institution: Tongji University, National Natural Science Foundation of China, Shanghai Municipal Science and Technology Major Project<br>This survey compiles recent advances, challenges, and new frontiers in the field of robotic manipulation of deformable objects (DOM). It notably emphasizes the initial progress of Large Language Models (LLMs) in robotic manipulation and points out important directions for further research in this area. While the review covers a broad range of literature and identifies future research directions, actual deployment examples and quantitative evaluations are limited.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10419v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10419.md)  |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **ProTIP: Progressive Tool Retrieval Improves Planning**<br><sub>Institution: Apple  <br>The paper presents ProTIP, an advanced strategy for tool retrieval and use in complex planning tasks for large language models. The key to ProTIP lies in its progressive retrieval, effective use of execution history, and achieving subtask-tool functionality alignment. Experimental results demonstrate that ProTIP significantly outperforms traditional methods, reduces tool hallucination, and increases planning efficiency.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10332v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10332.md)  |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **CoAScore: Chain-of-Aspects Prompting for NLG Evaluation**<br><sub>Institution: GSAI Renmin University of China<br>CoAScore is an innovative evaluation metric that improves the accuracy of NLG task assessments through a "chain of aspects" method, an approach that has been experimentally validated.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10355v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10355.md)  |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large Language Models**<br><sub>Institution: Science Foundation Ireland (SFI), JSPS KAKENHI<br>This paper presents the RecPrompt model, which optimizes news recommendation using LLMs. Through an iterative optimization process with manually and LLM-generated prompt templates, the news recommendation performance is significantly improved, particularly under the LLM-generated prompt templates utilizing GPT-4. However, this approach does not always outperform traditional recommendation methods and is significantly impacted by the choice of LLM.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10463v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10463.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs)**<br><sub>Institution: Luleå University of Technology Sweden<br>This paper introduces the ProCoT method, showing how LLMs can be harnessed to foster students' critical thinking and writing while preventing cheating. This method can help educators to make better use of these technological tools and cultivate students into better critical thinkers.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09801v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.09801.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **Challenges with unsupervised LLM knowledge discovery**<br><sub>Institution: Google DeepMind, Google Research<br>The paper challenges the capacity of existing unsupervised methods to explore latent knowledge in LLMs through theoretical proofs and experimental validations while providing sanity checks to consider for future knowledge elicitation method evaluations. Overall, the authors suspect that future unsupervised methods are likely to face similar issues, having difficulty in accurately distinguishing between model knowledge and other features.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10029v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10029.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION**<br><sub>Institution: OpenAI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openai/weak-to-strong)</div><div style='min-width:85px;'>[![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent**<br><sub>Institution: Google<br>This paper outlines creating an LLM agent capable of reasoning and interacting with external knowledge, along with a self-improvement algorithm that enables smaller models to perform comparably to large models in compositional question-answering benchmarks. The proposed method not only improves reasoning capabilities but also significantly reduces the required parameter count of the models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10003v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10003.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment**<br><sub>Institution: NLP Group Fudan University, Hikvision Inc  <br>The paper introduces a model called LoRAMoE to address the problem of world knowledge forgetting in language models due to massive increases in fine-tuning data and shows potential in multi-task learning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09979v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.09979.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **Generative Context-aware Fine-tuning of Self-supervised Speech Models**<br><sub>Institution: ASAPP, Carnegie Mellon University, Toyota Technological Institute at Chicago<br>The paper presents a new fine-tuning method for self-supervised speech models that leverages text generated by large language models as context to enhance task performance. It provides a way to reduce dependence on extra large models and resource usage during inference without compromising on performance.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09895v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.09895.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **Faithful Persona-based Conversational Dataset Generation with Large Language Models**<br><sub>Institution: University of Southern California, Google, Information Sciences Institute<br>The paper presents an LLM-based framework for generating, expanding, and updating large persona-based conversational datasets. By employing a Generator-Critic architecture and faithfulness criteria, the study successfully established the Synthetic-Persona-Chat dataset with enhanced dialogue quality.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10007v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10007.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models**<br><sub>Institution: Fudan University<br>This paper is the first to systematically study the vulnerability of skimming-based language models from the perspective of efficiency and proposes No-Skim, an effective and general efficiency robustness evaluation framework that generates adversarial inputs to increase computational complexity. Additionally, the framework is modularized to accommodate different plug-in modules, enabling evaluations to be conducted across three different knowledge levels.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09494v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.09494.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know**<br><sub>Institution: Apple<br>The paper introduces KGLens, a new framework for assessing factual knowledge in LLMs. KGLens generates natural language questions using the KG structure for evaluations and is aided by a parameterized KG and a graph-guided QG strategy to improve the quality of natural question generation and the efficiency of the assessment process.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11539v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.11539.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **GSVA: Generalized Segmentation via Multimodal Large Language Models**<br><sub>Institution: Tsinghua University<br>The GSVA method proposed in the paper solves the challenges of multi-target and empty targets in GRES tasks by learning to predict multiple [SEG] tokens and innovatively generating [REJ] tokens, demonstrating significant advantages over existing technologies.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10103v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.10103.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft**<br><sub>Institution: CUHK-SenseTime Joint Laboratory, Shanghai AI Laboratory, Tsinghua University<br>Auto MC-Reward is an advanced learning system that uses LLMs to automatically design dense rewards for Minecraft tasks. By leveraging LLMs' abilities to understand tasks and summarize experience, it effectively improves agents' learning of new behaviors and completion of long-term tasks in complex environments.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09238v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.09238.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation**<br><sub>Institution: Tsinghua University, Stanford University, Nanyang Technological University<br>This paper is the first to thoroughly investigate the robustness of LLMs against factual misinformation in a persuasive conversation setting, revealing the susceptibility of LLMs to persuasive misinformation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09085v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.09085.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Towards Verifiable Text Generation with Evolving Memory and Self-Reflection**<br><sub>Institution: Peking University, Chinese Academy of Sciences, Baidu Inc<br>VTG improves the reliability and verifiability of text generated by LLMs through an evolving memory and self-reflection approach, effectively addressing challenges of complex attention shifting and document retrieval. It has been validated through experiments.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09075v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.09075.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning**<br><sub>Institution: National University of Singapore, University of Illinois Urbana-Champaign, Microsoft  <br>The TAP4LLM framework proposed in this paper significantly enhances the performance of Large Language Models in tabular reasoning tasks. It operates by sampling, augmenting, and packing semi-structured data and can also serve as a plugin to further enhance LLMs' understanding of structured data.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09039v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.09039.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Entity-Augmented Code Generation**<br><sub>Institution: JetBrains<br>The paper proposes an innovative architecture to tackle the task of code generation with external entities. The architecture can scale without sacrificing performance, and with the integration of the entity retriever into the decoder rather than the encoder, the model can inspect all entities at once and directly use them. The new architecture not only resolves the limitations of existing models but also demonstrates its superiority in several experimental scenarios.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08976v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08976.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in Mathematical Reasoning**<br><sub>Institution: Peking University, DeepSeek-AI, The University of Hong Kong<br>MATH-SHEPHERD successfully addresses the issue of costly human annotations by training LLMs with automatically generated supervision data, thereby enhancing the accuracy of LLMs in solving complex mathematical problems and opening up new avenues for the advancement and practical application of LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08935v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08935.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent**<br><sub>Institution: Shanghai Jiao Tong University<br>The paper suggests enhancing LLMs' ability to solve complex mathematical problems through the MathAgent framework, namely Planner-Reasoner-Executor-Reflector (PRER). By breaking down the problems into phases and simulating human-like problem-solving processes, MathAgents significantly improve solving capabilities on challenging mathematical datasets, particularly in areas demanding higher estimation and synthesis skills.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08926v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08926.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Forbidden Facts: An Investigation of Competing Objectives in Llama-2**<br><sub>Institution: MIT<br>The paper provides insights into how the Llama-2-chat model handles competing objectives through the study of its behavior in the 'forbidden fact' task, introducing novel analytical methods in the process.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08793v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08793.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention**<br><sub>Institution: Tencent AI Lab Seattle<br>The Zebra model proposed in this paper effectively lowers computational and memory requirements by utilizing grouped local-global attention layers, exhibiting excellent performance in processing both long and short sequences. The research team validated the model through various experiments, proving the advantages of the Zebra architecture.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08618v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08618.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning**<br><sub>Institution: Hong Kong University of Science and Technology, Microsoft Research<br>This paper presents CoT-Max, a method that enhances LLMs' mathematical reasoning capabilities using a coarse-to-fine pruning technique, effectively improving the effects of few-shot learning in math reasoning tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08901v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08901.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **StemGen: A music generation model that listens**<br><sub>Institution: SAMI, ByteDance Inc.<br>The paper presents a new non-autoregressive language model approach for music generation, which optimizes the processing of multiple channels and the consistency between music and contextual information, and demonstrates, through objective and subjective assessments, the quality of the music generated and its alignment with contextual information.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08723v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08723.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **CogAgent: A Visual Language Model for GUI Agents**<br><sub>Institution: Tsinghua University, Zhipu AI<br>CogAgent breaks the limitation of pure text-based approaches by efficiently tackling the challenge of understanding and navigating GUIs with combined high and low-resolution image encoders and visual language models. The model achieves leading performance on nine visual question-answering benchmarks, propelling the future research and application of AI agents powered by advanced VLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08914v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08914.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/THUDM/CogVLM)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **TinyGSM: achieving >80% on GSM8k with small language models**<br><sub>Institution: Carnegie Mellon University, Microsoft Research  <br>This paper has successfully demonstrated that small language models can exceed an 80% accuracy rate on the GSM8K math problem reasoning benchmark by creating a synthetic dataset of math problems with corresponding Python solutions (TinyGSM), showing the feasibility of significant performance improvement of small models through high-quality datasets and verifier strategies.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09241v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.09241.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Weight subcloning: direct initialization of transformers using larger pretrained ones**<br><sub>Institution: Apple<br>The paper introduces a powerful weight subcloning approach to initialize smaller transformer models using weights from larger pretrained ones, greatly accelerating training speed, and enabling efficient training of the new models even with limited computational resources.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09299v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.09299.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Self-Evaluation Improves Selective Generation in Large Language Models**<br><sub>Institution: Google DeepMind, Google Research<br>The paper presents a new method where LLMs are guided to self-evaluate in order to improve the calibration of the quality of their generative output in selective generation scenarios. Experiments show that this method enhances the accuracy and overall quality of the generated content by LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09300v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.093.md)  |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **LDM$^2$: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement**<br><sub>Institution: University of Chinese Academy of Sciences<br>The paper presents the LDM2 model, which incorporates a dynamic memory mechanism and tree exploration approach to augment the decision-making capabilities of LLMs to adapt to more complex and unknown environments, and to realize dynamic learning abilities.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08402v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08402.md)  |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision**<br><sub>Institution: Peking University<br>This paper proposes a knowledge-aware method for synthesizing images of ancient artifacts with LLM-enhanced prompting and multi-source supervision, overcoming the lack of domain knowledge in existing text-to-image synthesis methods and showing significant improvement in quality and historical knowledge alignment.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08056v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08056.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/danielwusg/artifact_diffusion)</div> |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention**<br><sub>Institution: The Swiss AI Lab IDSIA USI & SUPSI, AI Initiative KAUST, Center for Brain Science Harvard University<br>SwitchHead is a novel approach that optimizes resource usage in the multi-head self-attention structure, resulting in reduced resource consumption while maintaining model performance. The method has practical application potential, especially for researchers and institutions with limited resources.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07987v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.07987.md)  |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **E&V: Prompting Large Language Models to Perform Static Analysis by Pseudo-code Execution and Verification**<br><sub>Institution: UC Riverside, Microsoft Research<br>This paper demonstrates the potential of LLMs in conducting pseudo-code static analysis and self-verification through the E&V method. The approach not only improves the flexibility and precision of static analysis but also reduces the human effort and specialized knowledge required to develop static analysis tools.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08477v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08477.md)  |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models**<br><sub>Institution: University of Southern California, Amazon.com Inc.<br>The paper presents BD-LLM, a new method to enhance the efficiency and transferability of LLMs in toxic content detection tasks, proposing the DToT method and optimizing model compression for more effective production deployment.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08303v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.08303.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **LLMEval: A Preliminary Study on How to Evaluate Large Language Models**<br><sub>Institution: Fudan University, Shanghai Jiaotong University  <br>The paper focuses on how to evaluate Large Language Models (LLMs), comparing various evaluation criteria, types of evaluators, scoring methods, and ranking systems. It introduces a new evaluation dataset, LLMEval, and assesses 20 LLMs, generating a massive amount of manual and automatic evaluation results. The study provides valuable insights and conclusions for the future evaluation of LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07398v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.07398.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection**<br><sub>Institution: Shanghai Jiao Tong University<br>The study explores ICL from the perspective of the inter-demonstration relationship, proposing the minimally edited text construction of Comparable Demonstrations (CDs) to alleviate potential demonstration bias. The experiments confirm the performance gains of CDs in OOD scenarios, emphasizing their particular necessity in simpler tasks and demonstrating their robustness with respect to the number of examples.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07476v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.07476.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **diff History for Long-Context Language Agents**<br><sub>Institution: New York University<br>The paper presents and validates the use of diff history to enhance model processing capabilities of long interaction histories. This method significantly improves model performance in complex decision tasks and effectively extends the length of history models can handle, providing new insights for the design of long-time series decision-making agents.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07540v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.0754.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **VILA: On Pre-training for Visual Language Models**<br><sub>Institution: NVIDIA, MIT  <br>VILA employs an improved pre-training strategy, outperforming benchmarks in various vision-language tasks, and offers practical guidance for the design of future visual language models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07533v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.07533.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **Alignment for Honesty**<br><sub>Institution: Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, Fudan University<br>The paper introduces the concept of alignment for honesty in LLMs and presents challenges and proposed solutions. By formally defining the problem, suggesting new methods, and establishing an evaluation framework, the paper provides a comprehensive solution to alignment for honesty in large language models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07000v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.07.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/GAIR-NLP/alignment-for-honesty)</div> |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **Tell, don't show: Declarative facts influence how LLMs generalize**<br><sub>Institution: Apollo Research, University of Oxford<br>The paper investigates how models generalize when declarative statements in training data conflict with statistical patterns or procedural examples. The findings have important implications for AI safety (regarding the “treacherous turn”) and fairness.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07779v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.07779.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **LLM in a flash: Efficient Large Language Model Inference with Limited Memory**<br><sub>Institution: Apple<br>This research provides a novel and practical solution that effectively reduces the data load and significantly speeds up inference when running large language models on memory-constrained devices, holding substantial significance for practical applications.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11514v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.11514.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **Efficient Few-Shot Clinical Task Adaptation with Large Language Models**<br><sub>The paper contributed to few-shot medical image classification by presenting an efficient fine-tuning approach through partial layer freezing and incorporating large language models for contextualizing labels to offer effective semantic guidance. The approach demonstrated exceptional performance in a challenge, indicating its effectiveness in adapting natural image models to medical image tasks in few-shot scenarios.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07125v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.07125.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **"What's important here?": Opportunities and Challenges of Using LLMs in Retrieving Information from Web Interfaces**<br><sub>Institution: Carnegie Mellon University<br>The paper explores the capabilities and challenges of LLMs in retrieving information from web interfaces, unveiling key factors affecting model performance and their limitations, setting a direction for future work.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06147v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.06147.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples**<br><sub>Institution: Xiamen University, Tencent YouTu Lab<br>The work introduces a new paradigm with MMICT to showcase the use of in-context learning capabilities to enhance fine-tuning performance on large multi-modal language models. By designing the versatile M-Hub module and conducting various context demonstration experiments, the study reveals the potential of in-context learning to improve performance on multi-modal tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06363v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.06363.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Unlocking Anticipatory Text Generation: A Constrained Approach for Faithful Decoding with Large Language Models**<br><sub>Institution: Salesforce AI Research<br>This work introduces a novel approach to improve the decoding methods for large language models by incorporating future constraint satisfaction. The proposed formal approach and scoring mechanism, benchmarked against LLMs, significantly contribute to the improved quality and control of text generation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06149v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.06149.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes**<br><sub>Institution: Zhejiang University, Alibaba Group<br>The paper presents a novel approach, FedKSeed, for federated full-parameter tuning using ZOO with a fixed set of seeds, substantially reducing the communication overhead required for tuning billion-sized LLMs, while achieving higher model accuracy and computational efficiency.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06353v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.06353.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Oracle-based Protocol Testing with Eywa**<br><sub>Institution: Microsoft Research<br>The paper introduced an oracle-based testing method, fully leveraging LLMs to build rich protocol behavior models and enhancing the auto-generation and coverage of network protocol test cases by combining symbolic execution with traditional test generation methods.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06875v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.06875.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **On Meta-Prompting**<br><sub>Institution: Microsoft  <br>This paper presents a theoretical framework based on category theory to generalize and depict automated prompting methods. Through experiments in the fields of ideation and creativity, it demonstrates that meta-prompting generates outputs that are more favorable to users compared to traditional fixed prompts.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06562v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.06562.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Honeybee: Locality-enhanced Projector for Multimodal LLM**<br><sub>Institution: Kakao Brain<br>The paper introduced a new type of locality-enhanced projector design, addressing deficiencies in existing methods in handling visual feature locality, and effectively utilized multifaceted instruction datasets. Consequently, the Honeybee model achieved significant performance improvements across multiple MLLM benchmarks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06742v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.06742.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/kakaobrain/honeybee)</div> |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning**<br><sub>Institution: Microsoft, Microsoft Research<br>The research presents a novel framework utilizing LLMs and ICL to extract self-consistent causal insights from user feedback to support analysis in Microsoft's Feedback Hub. The framework employs innovative self-consistency and prompt ensemble techniques to mitigate hallucinations and incorrect reasonings in LLMs and introduces two heuristic methods to assess the richness of feedback information. The experiments demonstrate the efficacy of the method in extracting causal insights and new bugs, and in assisting Microsoft engineers to prioritize feedback rich in information.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06820v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.0682.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Dense X Retrieval: What Retrieval Granularity Should We Use?**<br><sub>Institution: University of Washington, Tencent AI Lab<br>This paper introduces propositions as a new retrieval unit for dense retrieval, which improves the performance of downstream QA tasks and cross-task generalization capabilities while reducing irrelevant information in the retrieved texts.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06648v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.06648.md)  |
| <span style='display: inline-block; width: 42px;'>12-10</span> | **Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs**<br><sub>Institution: Microsoft Israel<br>The study's core contribution lies in its comparison of fine-tuning and RAG methodologies for knowledge injection into LLMs, finding that RAG demonstrates superior performance in injecting both new and existing knowledge. The research used innovative datasets and assessment methods to ensure the practicality and viability of the theoretical findings.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05934v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.05934.md)  |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Sim-GPT: Text Similarity via GPT Annotated Data**<br><sub>Institution: Shannon.AI, Zhejiang University, Bytedance<br>Sim-GPT is a framework that uses data labeling by GPT-4 to effectively train STS models. It incurs a one-time cost for data generation, is faster, and the model outperforms on multiple STS benchmarks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05603v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.05603.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/ShuheWang1998/Sim-GPT)</div> |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge**<br><sub>Institution: Northeastern University, Oracle<br>This paper introduces the Agile-Quant, an activation-guided quantization framework to accelerate the inference of large language models on edge devices. Agile-Quant overcomes challenges associated with activation value outliers and edge device hardware implementation, achieving task performance comparable to weight-only quantization methods while significantly increasing inference speed on actual devices.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05693v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.05693.md)  |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Context Tuning for Retrieval Augmented Generation**<br><sub>Institution: Apple  <br>The paper presents context tuning as a novel component that enhances RAG-based planning, enabling it to effectively handle incomplete or under-specified queries and reduce hallucinations. It systematically compares various retrieval methods in lightweight models and LLMs, showcasing the effectiveness of context tuning in improving contextual understanding.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05708v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.05708.md)  |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **NLLG Quarterly arXiv Report 09/23: What are the most influential current AI Papers?**<br><sub>Institution: University of Mannheim, University of Bielefeld<br>The paper provides an analysis of the most current trends and influence in AI research by examining the most cited papers on arXiv over a set period, particularly highlighting the significance of LLMs in this context.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05688v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.05688.md)  |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis**<br><sub>Institution: Shanghai Jiao Tong University<br>This research systematically explores the capability boundaries of LLMs within the context of game theory and provides insights for integrating LLMs into social science research from three distinct perspectives.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05488v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.05488.md)  |
| <span style='display: inline-block; width: 42px;'>12-08</span> | **PaperQA: Retrieval-Augmented Generative Agent for Scientific Research**<br><sub>Institution: RAND Corporation, Carnegie Mellon University, LangChain<br>The paper presents PaperQA, a retrieval-augmented generative agent for scientific research capable of answering questions based on up-to-date scientific literature with a performance comparable to human experts, and in some aspects even superior. The effectiveness of PaperQA is demonstrated, and its superiority is affirmed through comparative results with human experts and other commercial tools.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07559v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.07559.md)  |
| <span style='display: inline-block; width: 42px;'>12-08</span> | **Using Program Knowledge Graph to Uncover Software Vulnerabilities**<br><sub>The paper introduces a Program Knowledge Graph by combining program graphs with security data, and leverages prompt tuning of large language models to auto-generate queries for detecting vulnerabilities within software code. The method aims to overcome the limitations of traditional vulnerability detection methods, improving the automation and effectiveness of vulnerability detection, especially in static analysis applications.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04818v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.04818.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Beyond Surface: Probing LLaMA Across Scales and Layers**<br><sub>Institution: Hong Kong University of Science and Technology<br>The core contribution of the study lies in proposing a series of probing tasks to evaluate the higher-order capabilities of large language models, focusing on computation, mathematical reasoning, logical reasoning, and truthfulness detection. It reveals how the performance of LLMs varies with changes in model scale and structural layers.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04333v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.04333.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use**<br><sub>Institution: Gaoling School of Artificial Intelligence, Renmin University of China, Alibaba Group<br>The paper presents the Attention Buckets method to address deficiencies in context awareness of LLMs during tool use, significantly enhancing their performance in such tasks by processing different RoPE angle bases.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04455v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.04455.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Generating Illustrated Instructions**<br><sub>Institution: GenAI Meta, Columbia University<br>The paper presents a novel approach called StackedDiffusion for the task of generating illustrated instructions, a task that combines text and images to describe how to achieve a goal. This method overcomes the limitations of current T2I models that fail to generate visuals from user queries directly and surpasses existing methods in human evaluations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04552v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.04552.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models**<br><sub>Institution: MPI for Intelligent Systems, University of Washington<br>This research introduces the CLADDER dataset and CAUSALCOT chain-of-thought prompting strategy to test and analyze the abilities of large language models (LLMs) in formal causal reasoning, highlighting limitations of LLMs and suggesting future research directions.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04350v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.0435.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/causalNLP/cladder)</div> |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **A Study on the Calibration of In-context Learning**<br><sub>Institution: Harvard University<br>The paper conducts an in-depth study of the calibration accuracy in language models (LMs) for in-context learning (ICL) and presents methods for evaluation and analysis. It reveals the relationship of calibration errors with model size and the changes during finetuning, as well as the reduction in calibration during the generation of reasoning tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04021v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.04021.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration**<br><sub>Institution: Renmin University of China, Beijing Institute of Technology, HKUST (GZ)<br>This paper delivers a comprehensive study aimed at exploring a cost-effective batch prompting approach to entity resolution. The main contributions include the introduction of the BATCHER framework and the proposal of a covering-based demonstration selection strategy.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03987v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03987.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Chain of Code: Reasoning with a Language Model-Augmented Code Emulator**<br><sub>Institution: Google DeepMind, Stanford University, University of California Berkeley  <br>Chain of Code (CoC) adds a new dimension to language models by improving reasoning capabilities through code writing and code execution emulation. It achieves breakthrough performance in both numerical and semantic reasoning tasks, expands the application scope of LLMs, and has the potential to be applied to a broader range of problems.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04474v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.04474.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **An LLM Compiler for Parallel Function Calling**<br><sub>Institution: UC Berkeley, ICSI, LBNL<br>The paper introduces a system named LLMCompiler that addresses high latency costs and inefficiencies in executing multi-function calls by LLMs. It enhances speed, reduces costs, and improves accuracy through parallelized function calling and optimized orchestration.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04511v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.04511.md)  |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **OneLLM: One Framework to Align All Modalities with Language**<br><sub>Institution: MMLab The Chinese University of Hong Kong, Shanghai Artificial Intelligence Laboratory<br>OneLLM showcases strong multimodal understanding and processing capabilities through its unified multimodal encoding framework and progressive alignment pipeline, addressing the challenge of expanding multimodal LLMs in the area of reasoning and utilization.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03700v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.037.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/csuhan/OneLLM)</div> |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia**<br><sub>Institution: Google DeepMind, Google Research<br>This paper proposes a method for enhancing agent-based models with generative large language models, using the Concordia library to simulate interactions of agents in social, physical, and digital spaces. The model aims to provide life-like social simulations and explore the effectiveness of model validation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03664v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03664.md)  |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment**<br><sub>Institution: Zhejiang Lab<br>The paper successfully introduces Holmes, a framework that facilitates training LLMs in heterogeneous NIC environments. Empirical studies confirm that Holmes can achieve performance levels in these environments comparable to those possible with homogeneous RDMA NICs. This significant advancement makes LLM training more accessible and expands the potential for efficient scaling within the broader research community.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03549v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03549.md)  |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Efficient Large Language Models: A Survey**<br><sub>Institution: The Ohio State University, Google Research, Amazon AWS AI<br>The paper is a survey of the recent advancements in large language models concerning sparse activation methods, especially the Mixture-of-Experts system (MoE) and its application in long-context processing. It synthesizes various optimization methods for MoE models, including algorithmic improvements and system-level acceleration frameworks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03863v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03863.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/AIoT-MLSys-Lab/EfficientLLMs)</div> |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **AnimateZero: Video Diffusion Models are Zero-Shot Image Animators**<br><sub>Institution: Peking University, Tencent AI Lab, HKUST<br>AnimateZero provides decoupled and precise control of appearance and motion for T2V generation, realizing step-by-step video generation from T2I to I2V, while maintaining good domain consistency through spatial appearance control and temporal consistency control.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03793v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03793.md)  |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Controllable Human-Object Interaction Synthesis**<br><sub>Institution: Stanford University, FAIR Meta<br>The paper proposes a novel interaction synthesis method, CHOIS, which is capable of generating synchronized human and object motions under the guidance of language descriptions, adhering to the geometric constraints of 3D scenes. Integrated into a system, it demonstrates its efficacy in synthesizing continuous, realistic, and context-aware human-object interactions.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03913v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03913.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education**<br><sub>Institution: Carnegie Mellon University  <br>The main contribution of this paper is the development of an automated MCQ generation system based on GPT-4, which, through a specialized flexible architecture and precise LO alignment mechanism, successfully generates MCQs consistent with higher education Python courses LOs. The findings show that the automatically generated MCQs maintain good alignment with the LOs and are close in quality to human-generated MCQs, but fall short on having a single correct answer and high-quality distractors, suggesting future work should focus on alleviating these issues.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03173v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03173.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models**<br><sub>Institution: University of Waterloo, 2Cohere, Comcast Applied AI<br>The paper's key achievement is demonstrating how to construct an effective listwise reranker without dependence on GPT models, significantly surpassing existing GPT-based rerankers, and calling for the development of higher-quality listwise training datasets to enhance model performance.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02969v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.02969.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Inherent limitations of LLMs regarding spatial information**<br><sub>Institution: ProtagoLabs, International Monetary Fund, NetMind.ai  <br>The paper provides a new evaluation framework and specially designed dataset for the capabilities of large language models like GPT-4 in handling spatial information, and analyzes the abilities and limitations of GPT-4 in dealing with spatial information.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03042v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03042.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction**<br><sub>Institution: Zhejiang Lab, Ant Group<br>By introducing a multi-agent cooperation approach within KGC, the cooperKGC framework improves the precision with which agents solve tasks involving entity, relation, and event extraction, and potentially lays the foundation for a future of collaboration-aware AI.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03022v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03022.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!**<br><sub>Institution: University of Waterloo<br>RankZephyr is a new type of open-source LLM specifically optimized for zero-shot list reranking tasks. It offers reranking effects comparable or superior to those of large proprietary models, while emphasizing the importance of data augmentation for enhanced model robustness, and has proven its effectiveness and application potential in real-world scenarios.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02724v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.02724.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/castorini/rank_llm)</div> |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Prompt Optimization via Adversarial In-Context Learning**<br><sub>Institution: National University of Singapore, Hong Kong University of Science and Technology, Institute for Infocomm Research (I2R) A*STAR<br>The paper introduces a novel Adversarial In-Context Learning (adv-ICL) method for optimizing prompt selection in large models to enhance their performance. It achieves adversarial training objectives, overcoming data and computational resource constraints by improving performance through prompt optimization instead of model parameters, with experimental results significantly outperforming existing techniques across multiple tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02614v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.02614.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation**<br><sub>Institution: Sea AI Lab, Sun Yat-sen University, Harvard University  <br>The paper introduced a Creative Leap-of-Thought (CLoT) paradigm for enhancing the creative thinking abilities of large language models, demonstrating its effectiveness and generalizability across various tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02439v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.02439.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/sail-sg/CLoT)</div> |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Large Knowledge Model: Perspectives and Challenges**<br><sub>Institution: Zhejiang University<br>The paper proposes the concept of a Large Knowledge Model (LKM), aimed at more effectively managing and interpreting the diversity of knowledge representation. The study outlines the challenges in transitioning from current LLMs to LKMs, underlines the importance of structured knowledge in pre-training, and introduces a set of design principles for LKMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02706v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.02706.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **A Hardware Evaluation Framework for Large Language Model Inference**<br><sub>Institution: Princeton University<br>LLMCompass, as a hardware evaluation framework, effectively addresses the challenges in designing hardware for LLM inference. It is not only fast and accurate but also architecturally descriptive and cost-aware, and it has been validated on commercial hardware with exceptional performance.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03134v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03134.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **How should the advent of large language models affect the practice of science?**<br><sub>Institution: Max Planck Institute for Biological Cybernetics, University of Tübingen, University of Washington  <br>The paper discusses the implications of LLMs on scientific practices and recommends a cautious approach to their usage, emphasizing the importance of protecting the normative and epistemic aspects of science. Although LLMs may improve the efficiency of certain research tasks, they should be used judiciously as tools that abide by scientific norms and standards.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03759v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.03759.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication**<br><sub>Institution: Fudan University, National University of Singapore, Shanghai AI Laboratory  <br>The Exchange-of-Thought (EoT) framework introduced in this paper enhances the reasoning capabilities of LLMs through cross-model communication, leveraging four communication paradigms and a confidence evaluation mechanism, yielding significant improvements in various reasoning tasks and proving the role of external insights in enhancing model performance.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01823v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01823.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning**<br><sub>Institution: Allen Institute for Artificial Intelligence, University of Washington<br>The paper introduces a simple, tuning-free method (URIAL) for aligning LLMs through in-context learning, which demonstrates performance on par with or superior to traditional tuning alignment methods. The findings significantly contribute to future LLM research, highlighting the importance of deeper analysis and theoretical understanding in LLM alignment.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01552v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01552.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **On the Effectiveness of Large Language Models in Domain-Specific Code Generation**<br><sub>Institution: Shanghai Jiao Tong University, Chongqing University, East China Normal University<br>The study demonstrates that LLMs' capabilities in domain-specific code generation can be significantly enhanced by effectively integrating domain knowledge into the code generation process. The DomCoder approach exemplifies the incorporation of different strategies to blend domain knowledge and boost the actual effectiveness of code generation within certain contexts.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01639v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01639.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions**<br><sub>Institution: Nanyang Technological University, National University of Singapore<br>The study presents the first comprehensive evaluation of the potential of leveraging ChatGPT in the generation of pre-university math questions. It explores question generation in two main scenarios: with and without given context and aims to provide practical insights for educators. The findings from this study may promote the usage of modern AI technologies in education, enhancing the practicability and efficiency of automated math question generation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01661v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01661.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **LLMs Accelerate Annotation for Medical Information Extraction**<br><sub>Institution: Google Research<br>The paper presents a method that uses large language models, specifically Google's PaLM 2, to enhance the speed of annotation in medical information extraction tasks. The LLM-based annotation workflow increases efficiency without complex model parameter adjustment, making it a promising tool for accelerating data annotation work in the medical field.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02296v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.02296.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Data Management For Large Language Models: A Survey**<br><sub>Institution: Peking University, Huawei Noah’s Ark Lab<br>This survey studies the current state of research in data management at both the pretraining and supervised fine-tuning stages of LLMs and the design of data management strategies.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01700v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.017.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/ZigeW/data_management_LLM)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models**<br><sub>Institution: Xiamen University, MBZUAI, Tencent AI Lab<br>The paper successfully elevated the CoT reasoning capabilities of LLMs in multi-modal tasks by introducing a dynamic automatic retrieval mechanism and stratified sampling method. The proposed approach not only improved model performance but also refined the reasoning process through diverse example selection, setting a new performance benchmark in the field of multi-modal reasoning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01714v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01714.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Competition-Level Problems are Effective LLM Evaluators**<br><sub>Institution: Microsoft Research Asia, Xiamen University, Microsoft Azure AI<br>The study has revealed inadequacies in LLMs like GPT-4 when assessing their real-world reasoning capabilities using competition-level programming questions, suggested methods for improvement, and highlighted the significance of such problems as efficient evaluators of LLMs, thus fostering further research into enhancing complex reasoning abilities in LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02143v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.02143.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly**<br><sub>Institution: Elsevier<br>This paper summarizes the applications and associated challenges of Large Language Models (LLMs) in security and privacy, highlighting the good, the bad, and the ugly aspects while emphasizing the potential for data protection in these domains.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02003v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.02003.md)  |
| <span style='display: inline-block; width: 42px;'>12-03</span> | **Running cognitive evaluations on large language models: The do's and the don'ts**<br><sub>Institution: Massachusetts Institute of Technology<br>This paper provides instructive recommendations on the methodological approach for conducting cognitive assessments of large language models, exploring how to avoid potential issues during the evaluation process. The goal of the paper is to contribute to the broader discussion of best practices in the field of AI Psychology.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01276v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01276.md)  |
| <span style='display: inline-block; width: 42px;'>12-03</span> | **TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents**<br><sub>Institution: University of Southern California, Google Cloud AI<br>The paper introduces TextGenSHAP, an efficient post-hoc explanation method designed for large language models. The method improves the speed of explanation generation and demonstrates how to leverage these explanations to enhance long-document question answering and document retrieval systems.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01279v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01279.md)  |
| <span style='display: inline-block; width: 42px;'>12-03</span> | **D-Bot: Database Diagnosis System using Large Language Models**<br><sub>Institution: Tsinghua University, Pigsty, ModelBest<br>D-Bot is a database diagnosis system based on large language models designed to improve the efficiency and accuracy of database diagnosis by extracting knowledge from documents and generating effective diagnosis reports, addressing challenges faced by domain experts in the field of database diagnosis.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01454v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01454.md)  |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Large Language Models Are Zero-Shot Text Classifiers**<br><sub>Institution: Florida Atlantic University<br>The paper demonstrates that LLMs are effective as zero-shot text classifiers, which is particularly beneficial for small teams or businesses that need to quickly deploy text classifiers. The research results suggest that GPT-4 consistently surpasses traditional ML algorithms in all four datasets. The article also suggests future research directions that include optimizing prompts for higher accuracy or introducing a critic agent to evaluate and improve the outcomes of LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01044v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01044.md)  |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Just-in-Time Security Patch Detection -- LLM At the Rescue for Data Augmentation**<br><sub>Institution: University of Luxembourg, Windows Copilot Microsoft, Singapore Management University<br>The paper presents an innovative security patch detection framework, LLMDA, utilizing Large Language Models for patch analysis and data augmentation, and aligning multimodal inputs. This enables the system to extract more extensive information from the combined context of patches and code, thereby enhancing detection accuracy.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01241v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01241.md)  |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Exploring and Improving the Spatial Reasoning Abilities of Large Language Models**<br><sub>Institution: Stanford University  <br>The paper advances our understanding of LLMs' capabilities in spatial reasoning and sequence labeling, proposing a method to improve LLMs' performance in 3D trajectory recognition tasks with significant performance improvements.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01054v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.01054.md)  |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Axiomatic Preference Modeling for Longform Question Answering**<br><sub>The axiomatic framework proposed in this paper offers a new method for preference modeling in long-form question-answering, closely examining human preferences and optimizing the accuracy and efficiency of preference scoring.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02206v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.02206.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs**<br><sub>Institution: Singapore Management University, National Sun Yat-sen University<br>The study demonstrates the capability of LLMs to successfully work through knowledge graph reasoning tasks using their internal knowledge graph and to infer knowledge graph relations from context, showcasing the potential and applicative value of LLMs in knowledge graph reasoning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00353v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.00353.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models**<br><sub>Institution: University of Wisconsin - Madison<br>This paper presented an extensive study on the impact of compression techniques (pruning and quantization) on parametric knowledge retention in large language models (LLMs), providing valuable insights for practitioners on model compression.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00960v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.0096.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses**<br><sub>Institution: Google<br>The paper introduced ExploreLLM, a new interaction pattern between users and LLM-powered assistants by combining a prompt-based task decomposition method with a novel schema-like GUI. The system aims to reduce the cognitive burden of completing complex tasks and to enhance the level of personalized responses.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00763v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.00763.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Instruction-tuning Aligns LLMs to the Human Brain**<br><sub>Institution: EPFL<br>The study shows that large language models trained through instruction-tuning exhibit better representation of world knowledge and alignment with human brain activity. This provides a crucial perspective for the future development of LLMs to incorporate world knowledge into the models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00575v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.00575.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games**<br><sub>Institution: Quebec AI Institute<br>The paper provides a new evaluation method adapted to the complexity and new challenges of JuBensha games and establishes a new framework, ThinkThrice, for assessing the capabilities of LLM agents in an interactive gaming environment, advancing AI applications in multiplayer role-playing games.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00746v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.00746.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Nash Learning from Human Feedback**<br><sub>Institution: Google DeepMind<br>The paper introduces a novel method to fine-tune LLMs for alignment with human preferences through Nash equilibrium, demonstrating its potential in complex tasks and verifying its effectiveness through empirical evidence.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00886v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.00886.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Leveraging Large Language Models to Improve REST API Testing**<br><sub>Institution: Georgia Institute of Technology, IBM Research<br>RESTGPT addresses the limitations of existing methods in extracting rules from natural language descriptions and generating effective values by leveraging the accuracy and efficiency of LLMs, especially GPT-3.5 Turbo, significantly enhancing the quality and accuracy of REST API testing.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00894v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.00894.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models**<br><sub>Institution: University of Wisconsin - Madison<br>This research represents one of the first large-scale investigations into the impact of compression techniques on the parametric knowledge of LLMs, offering significant insights for practitioners, especially regarding decisions related to pruning and quantization techniques.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00960v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.0096.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Improve Supervised Representation Learning with Masked Image Modeling**<br><sub>Institution: Google Research, OpenAI  <br>This paper proposed a new training setup that integrates supervised representation learning with MIM, significantly enhancing the quality of representation learning for downstream tasks such as classification, image retrieval, and semantic segmentation without introducing significant training or inference overhead.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00950v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.0095.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Learning from One Continuous Video Stream**<br><sub>The paper presents a framework for online learning from a single continuous video stream focused on evaluating adaptability and generalizability, proposing a sequence of future prediction tasks for pre-training. The study demonstrates that optimization strategies in such learning environments need to be adjusted, with reductions in momentum and frequency of weight updates leading to improved adaptability and generalization of models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00598v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.00598.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback**<br><sub>RLHF-V presents a novel strategy to rectify MLLM behavior via fine-grained, correctional human feedback. It collects quality data to align MLLM learning with human preferences, effectively improving the models' reliability and practicality in various tasks. This study represents a significant advancement in enhancing the robustness of large multimodal language models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00849v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-12/2312.00849.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/RLHF-V/RLHF-V)</div> |

---

## 2023-11

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **Applying Large Language Models and Chain-of-Thought for Automatic Scoring**<br><sub>Institution: University of Georgia<br>The study showcases the potential of LLMs in facilitating automatic scoring, highlighting that CoT significantly enhances scoring accuracy when used with item stems and scoring rubrics. The combined approach of LLMs with CoT can reduce complexity and manpower cost in building automatic scoring models and potentially offer a closer alignment with human scoring results.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03748v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2312.03748.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text**<br><sub>Institution: The University of Tokyo<br>The research showcased GPT-4's robust capabilities in managing scrambled texts, set forth new metrics RR and RPG, and validated GPT-4's stable performance across various scramble scenarios and rates.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18805v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.18805.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations**<br><sub>Institution: Comcast Applied AI, University of Waterloo<br>The authors proposed a novel probe to detect implicit association biases in LLMs representations and demonstrated state-of-the-art performance in preference detection. The research additionally uncovered significant biases in multiple instruction-following and "classic" LLMs related to nationality, politics, religion, and gender, despite the explicit safety calibration of the LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18812v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.18812.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/castorini/biasprobe)</div> |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation**<br><sub>Institution: UC Berkeley, Microsoft Azure AI, ZOOM<br>CoDi-2 is an advanced multimodal generation model capable of processing complex multimodal inputs, guiding generation in-context, interacting with users through multi-round conversations, and achieving outstanding zero-shot and few-shot performance.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18775v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.18775.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **PoseGPT: Chatting about 3D Human Pose**<br><sub>Institution: Max Planck Institute for Intelligent Systems, Meshcapade<br>PoseGPT is a novel framework that enables models to directly generate 3D human poses from textual and visual inputs by embedding SMPL pose tokens within LLMs, achieving some innovation in interpreting 3D human pose.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18836v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.18836.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **Autonomous Agents in Software Development: A Vision Paper**<br><sub>Institution: Tampere University<br>This paper proposes a vision of using multiple GPT agents for automating SE tasks and showcases preliminary success in simple software tasks. This work has the potential to fundamentally change the way software development is conducted and to shorten development time.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18440v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.1844.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions**<br><sub>Institution: Huawei Poisson Lab<br>The IAG framework, with its inductive prompting method for strengthening the factuality of knowledge statements and optimized knowledge fusion mechanism and student inductor model, addresses the shortcomings of existing retrieval-based methods in answering QA tasks involving implicit reasoning. The research findings indicate that IAG performs better in answering QA tasks that involve implicit reasoning.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18397v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.18397.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation**<br><sub>Institution: University of Science and Technology of China, Microsoft Research Asia<br>MicroCinema, with its innovative two-phase process for text-to-video generation and effective Appearance Injection Network and Appearance Noise Prior mechanisms, has achieved a breakthrough in video generation quality, serving as a reference model for subsequent work.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18829v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.18829.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **TaskBench: Benchmarking Large Language Models for Task Automation**<br><sub>Institution: Zhejiang University<br>This paper presented TaskBench, a new benchmark test, and TASKEVAL, an evaluation system, which together effectively address the assessment challenges of LLMs in task automation through data generation and quantitative evaluation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18760v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.1876.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Zero-shot Conversational Summarization Evaluations with small Large Language Models**<br><sub>Institution: Intel labs<br>The paper focuses on the application of Large Language Models in the conversational summarization task, deeply examining the impact of different instructions on model performance and researching optimization techniques for using compressed models under hardware limitations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18041v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.18041.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **TaskWeaver: A Code-First Agent Framework**<br><sub>Institution: Microsoft<br>TaskWeaver is a code-first designed framework to build LLM-powered autonomous agents, achieving efficient handling of complex data structures, flexible plugin usage, and the successful integration of domain-specific knowledge into the system.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17541v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.17541.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/microsoft/TaskWeaver)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering**<br><sub>Institution: Sun Yat-Sen University<br>This work innovatively combines three agents to simulate the top-down reasoning process in human cognition, and introduces the concept of a Multi-view Knowledge Base, significantly enhancing the expressiveness and interpretability of VQA models.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17331v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.17331.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Are Large Language Models Good Fact Checkers: A Preliminary Study**<br><sub>Institution: Chinese Academy of Sciences<br>The paper systematically evaluates the potential of LLMs in the entire fact-checking process, revealing that while they show promise in certain aspects, considerably more research and trials are needed to improve their performance in fact-checking tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17355v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.17355.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Large Language Models for Networking: Applications, Enabling Techniques, and Challenges**<br><sub>Institution: BUPT<br>The paper proposes a new framework, ChatNet, that integrates Large Language Models with network technologies, exploring its application in network planning. The study demonstrates that ChatNet can effectively promote the automation and intelligence level of network tasks, though challenges such as multimodal data integration and plugin development must be addressed prior to deployment.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17474v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.17474.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models**<br><sub>Institution: Harbin Institute of Technology<br>The introduction of the TIMEBENCH benchmark marks an important step in the comprehensive assessment of temporal reasoning abilities in Large Language Models, showcasing the current gap between models and humans in this area and providing guidance for future research.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17667v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.17667.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/zchuz/TimeBench)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Understanding and Improving In-Context Learning on Vision-language Models**<br><sub>Institution: LMU Munich, University of Oxford<br>This paper proposed a novel method, MMICES, for selecting demonstrations in in-context learning for vision-language models, demonstrating its effective performance across different models and datasets.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18021v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.18021.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation**<br><sub>Institution: The Education University of Hong Kong<br>This paper represents an innovative attempt to build an AI tutor system that can adapt to any course subject and provide customized high-quality educational support, potentially progressing the application of AI technology in education and forging a new path for the development of AI tutoring systems.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17696v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.17696.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation**<br><sub>Institution: Alibaba Group<br>The paper presents a new framework "Animate Anyone" using diffusion models for character animation. The framework maintains appearance consistency through ReferenceNet and ensures controllability and continuity of animations via a pose guider and temporal layer, achieving advanced results in character animation generation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17117v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.17117.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HumanAIGC/AnimateAnyone)</div><div style='min-width:85px;'>[![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)](https://humanaigc.github.io/animate-anyone/)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?**<br><sub>Institution: Nanyang Technological University<br>This survey paper provides an assessment of the performance of open-source LLMs across multiple task domains compared to ChatGPT, highlighting the strengths and potential problems of current open-source LLMs, and offers insights for future research and development. Furthermore, it summarizes numerous best practices and challenges, indicating that the open-source field could potentially close the gap with commercial models to some extent.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16989v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.16989.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **LLaFS: When Large-Language Models Meet Few-Shot Segmentation**<br><sub>Institution: Singapore University of Technology and Design, Zhejiang University <br>This paper presents an LLM-based framework for few-shot image segmentation, addressing the core challenges of enabling LLMs to understand and execute visual tasks. A combination of customized guidance and fine-grained in-context instructions facilitates high-quality few-shot segmentation.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16926v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.16926.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/lanyunzhu99/LLaFS)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **RELIC: Investigating Large Language Model Responses using Self-Consistency**<br><sub>Institution: ETH Zurich<br>RELIC is an interactive system that, through the factual consistency investigation of multiple samples, helps users verify and direct texts generated by LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16842v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.16842.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **RankingGPT: Empowering Large Language Models in Text Ranking with Progressive Enhancement**<br><sub>Institution: Alibaba Group<br>This study presents a two-stage training model for text ranking that combines weakly supervised pre-training and supervised fine-tuning. It smoothly transitions from pre-training to fine-tuning without sacrificing pre-training benefits, enhancing fine-tuning performance. The experiments have shown significant superiority over existing techniques.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16720v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.1672.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond**<br><sub>The research introduces a novel, integrated AvatarGPT framework for handling high-level and low-level tasks related to understanding, planning, and generating human motions, showcasing the potential for extended duration motion synthesis and reduced manual intervention.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16468v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.16468.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Training Chain-of-Thought via Latent-Variable Inference**<br><sub>Institution: Google<br>This paper develops an MCMC-EM based fine-tuning strategy that, by averaging over rationales, helps LLMs generate the correct answers, holding potential for wide applicability.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02179v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2312.02179.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization**<br><sub>Institution: Shanghai AI Laboratory<br>The article proposes an innovative strategy for optimizing LVLMs to reduce hallucinations and introduces a new evaluation method to more comprehensively measure hallucinations. The effectiveness of the proposed method is validated through experiments.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16839v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.16839.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Graph Prompt Learning: A Comprehensive Survey and Beyond**<br><sub>Institution: The Chinese University of Hong Kong, Hong Kong University of Science and Technology, Fudan University  <br>This paper provides a thorough survey on graph prompt learning, covering the AGI challenges with graph data handling and how graph prompt learning can facilitate cross-modality, cross-domain, and cross-task applicability of AGI technologies.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16534v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.16534.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/WxxShirley/Awesome-Graph-Prompt)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine**<br><sub>Institution: Microsoft<br>This paper explores how to guide a generalist foundation model to exhibit expert-level capabilities on specialized tasks without expert supervision, using the medical field as a case study. The proposed Medprompt strategy proves to have a significant advantage in enhancing the specialized abilities of foundation models and shows the possibility of widespread application across multiple disciplines.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16452v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.16452.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Prompting in Autoregressive Large Language Models**<br><sub>Institution: George Mason University<br>This paper provides a succinct literature review in the field of prompting for autoregressive large language models, highlighting unresolved challenges and open problems, thereby offering directions for future research.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03740v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2312.0374.md)  |
| <span style='display: inline-block; width: 42px;'>11-27</span> | **RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks**<br><sub>Institution: Chinese Academy of Sciences, Peking University<br>The paper presents an intelligent agent named RoboGPT that is designed for making embodied long-term decisions for daily instruction tasks. The agent combines the generic knowledge of LLMs with the professional knowledge in the robotics domain and introduces Re-Plan and RoboSkill modules to enhance the rationality and adaptability of task planning. On the ALFRED benchmark tests and generalization tasks, RoboGPT surpasses existing advanced methods.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.15649v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.15649.md)  |
| <span style='display: inline-block; width: 42px;'>11-25</span> | **Faster Minimum Bayes Risk Decoding with Confidence-based Pruning**<br><sub>Institution: University of Cambridge<br>The paper presented an algorithm for MBR decoding that reduces utility function calls by gradually increasing the number of samples in the estimate and using confidence pruning. The algorithm significantly lowers computational costs while maintaining accuracy, and its effectiveness was validated through NMT experiments on three language pairs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.14919v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.14919.md)  |
| <span style='display: inline-block; width: 42px;'>11-24</span> | **Calibrated Language Models Must Hallucinate**<br><sub>Institution: Microsoft Research<br>The paper outlines the statistical root cause of inevitable hallucinations under sufficient calibration in pretrained language models, elucidates the native mechanism of hallucination generation in models with good predictive performance and provides a lower bound estimate for the rate of hallucination. It discusses the likelihood of different types of facts hallucinating and points towards potential future directions for mitigating specific types of hallucinations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.14648v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.14648.md)  |
| <span style='display: inline-block; width: 42px;'>11-24</span> | **Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language**<br><sub>Institution: Amazon<br>The paper presented an effective CnR method capable of efficiently aligning LLMs with human expectations through detailed feedback and response revision using natural language. With relatively less human feedback data, this method significantly improves the quality of responses from even top LLMs such as ChatGPT.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.14543v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.14543.md)  |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach**<br><sub>Institution: Chinese Academy of Sciences<br>The LLaMAC framework demonstrates superior performance of LLM-based multi-agent systems in long-term planning, mathematical reasoning, optimization problems, and spatial reasoning, while also reducing access costs for large-scale multi-agent collaboration. With further enhancement of LLMs and more collaboration frameworks emerging, new opportunities will unfold in the multi-agent collaboration field.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13884v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.13884.md)  |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline**<br><sub>Institution: Sber AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13073v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions**<br><sub>Institution: Tsinghua University<br>The paper introduces Probabilistic Tree-of-thought Reasoning (ProbTree), a novel method that explores LLMs' capabilities to answer complex, knowledge-intensive questions and incorporates uncertainty into the reasoning process, integrating external and parametric knowledge within a unified framework.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13982v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.13982.md)  |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **GAIA: a benchmark for General AI Assistants**<br><sub>Institution: FAIR, Meta<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12983v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs**<br><sub>Institution: Google Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13600v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes**<br><sub>Institution: ASRI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13384v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **Diffusion Model Alignment Using Direct Preference Optimization**<br><sub>Institution: Nikhil Naik, Stanford University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12908v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **XAGen: 3D Expressive Human Avatars Generation**<br><sub>Institution: National University of Singapore, ByteDance<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13574v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms**<br><sub>Institution: Princeton University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13133v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **Enhancing Summarization Performance through Transformer-Based Prompt Engineering in Automated Medical Reporting**<br><sub>Institution: Utrecht University<br>This research validated that applying transformer-based prompt engineering in automated medical reporting can improve summarization performance. Despite some limitations, the proposed approach has shown the effectiveness of including examples and contextual information in prompt formulations and pointed out directions for future work.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13274v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.13274.md)  |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **Visual In-Context Prompting**<br><sub>Institution: HKUST, Microsoft Research<br>The paper presents DINOv, an innovative visual in-context prompting framework effectively handling a variety of visual prompts, utilizing unlabeled data, and performing well across several tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13601v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.13601.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/UX-Decoder/DINOv)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Oasis: Data Curation and Assessment System for Pretraining of Large Language Models**<br><sub>Institution: Chinese Academy of Sciences<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12537v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks**<br><sub>Institution: University of Cambridge<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12786v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey**<br><sub>Institution: Nanjing University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12351v1)</div><div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Strivin0311/long-llms-learning)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **A Survey on Multimodal Large Language Models for Autonomous Driving**<br><sub>Institution: Purdue University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12320v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Latent Lab: Large Language Models for Knowledge Exploration**<br><sub>Institution: Department of Electrical Engineering and Computer Science, MIT<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13051v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **How Capable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks**<br><sub>Institution: University of Pennsylvania, MIT<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12997v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Prompting Frameworks for Large Language Models: A Survey**<br><sub>Institution: Zhejiang University<br>This research delivers a framework that enhances interaction with LLMs by implementing new techniques, including improved compatibility with programming languages, enabling LLMs to utilize external tools, and maintaining historical interaction information, thus guiding future research directions.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12785v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.12785.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/lxx0628/Prompting-Framework-Survey)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?**<br><sub>Institution: University of Auckland<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12337v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **AcademicGPT: Empowering Academic Research**<br><sub>Institution: International Digital Economy Academy<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12315v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents**<br><sub>Institution: Shanghai Jiao Tong University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11797v1)</div><div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Zoeyyao27/CoT-Igniting-Agent)</div> |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **Continual Learning: Applications and the Road Forward**<br><sub>Institution: KU Leuven<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11908v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **Assessing Prompt Injection Risks in 200+ Custom GPTs**<br><sub>Institution: Northwestern University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11538v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **GPQA: A Graduate-Level Google-Proof Q&A Benchmark**<br><sub>Institution: New York University<br>The GPQA dataset offers a benchmark for testing the ability of AI systems to handle complex questions that require deep understanding and reasoning. With rigorous quality control and expert-level difficulty, it has the potential to advance the development of collaborative methods between human experts and AI systems, as well as the advancement of AI system design.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12022v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.12022.md)  |
| <span style='display: inline-block; width: 42px;'>11-19</span> | **TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems**<br><sub>Institution: SenseTime Researc<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11315v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **An Embodied Generalist Agent in 3D World**<br><sub>Institution: Beijing Institute for General Artificial Intelligence <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12871v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **Orca 2: Teaching Small Language Models How to Reason**<br><sub>Institution: Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11045v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning**<br><sub>Institution: Technical University of Darmstadt, University of Cambridge<br>The paper proposes a unified library—Adapters—that integrates and extends parameter-efficient and modular transfer learning methods. It achieves close integration with the Transformers library and demonstrates its effectiveness through comparative experiments on several NLP tasks.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11077v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.11077.md)  |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability**<br><sub>Institution: University of Science and Technology of China<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10947v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-17</span> | **Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2**<br><sub>Institution: Allen Institute for AI <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10702v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-17</span> | **Exploring the Relationship between In-Context Learning and Instruction Tuning**<br><sub>Institution: HKUST<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10367v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **Automatic Engineering of Long Prompts**<br><sub>Institution: Google<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10117v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **MacGyver: Are Large Language Models Creative Problem Solvers?**<br><sub>Institution: University of California, Princeton University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09682v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **Crafting In-context Examples according to LMs' Parametric Knowledge**<br><sub>Institution: The University of Texas at Austin<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09579v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **Predictive Minds: LLMs As Atypical Active Inference Agents**<br><sub>Institution: Charles University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10215v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Contrastive Chain-of-Thought Prompting**<br><sub>Institution: DAMO Academy, Alibaba Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09277v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models**<br><sub>Institution: Tecent AI Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09210v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **ToolTalk: Evaluating Tool-Usage in a Conversational Setting**<br><sub>Institution: Microsoft Corporation<br>ToolTalk is a benchmark designed to evaluate and improve the performance of LLMs in utilizing multi-step external tools within a conversational context. With innovative evaluation methods and realistic scenario simulations, it challenges and expands the boundaries of LLM capabilities and charts a course for future research.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10775v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.10775.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/microsoft/ToolTalk)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Memory Augmented Language Models through Mixture of Word Experts**<br><sub>Institution: Google Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10768v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Exponentially Faster Language Modelling**<br><sub>Institution: ETH Zurich<br>The paper introduces UltraFastBERT, a variant of a large-scale language model that significantly reduces the number of neurons needed during inference and increases computational efficiency through the use of fast feedforward networks. Despite lacking a native efficient implementation, the model provides a CPU code implementation that significantly accelerates the inference process and performs well on standard downstream tasks. This work demonstrates the substantial potential of conditional neural execution in the field of language modeling.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10770v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.1077.md)  |
| <span style='display: inline-block; width: 42px;'>11-14</span> | **KTRL+F: Knowledge-Augmented In-Document Search**<br><sub>Institution: KAIST AI, Samsung Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.08329v3)</div> |
| <span style='display: inline-block; width: 42px;'>11-14</span> | **Learning to Filter Context for Retrieval-Augmented Generation**<br><sub>Institution: Carnegie Mellon University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.08377v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-14</span> | **Instruction-Following Evaluation for Large Language Models**<br><sub>Institution: Google, Yale University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.07911v1)</div><div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/google-research/google-research/tree/master/instruction_following_eval)</div> |
| <span style='display: inline-block; width: 42px;'>11-13</span> | **Can LLMs Patch Security Issues?**<br><sub>Institution: School of Computer Science Atlanta<br>The article introduced a new approach to code refinement named FDSS, which, by integrating with the static code analysis tool, Bandit, significantly enhances the capability of LLMs in solving security issues within code.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00024v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2312.00024.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Kamel773/LLM-code-refine)</div> |
| <span style='display: inline-block; width: 42px;'>11-11</span> | **In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering**<br><sub>Institution: Stanford University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.06668v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-10</span> | **Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking**<br><sub>Institution: Helvia.ai<br>For the first time, this paper presents a comprehensive evaluation of methodologies in a resource-limited industrial context, including cost analysis, RAG method, and data augmentation using GPT-4, offering new avenues for the financial industry to address challenges related to data and budget constraints.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.06102v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.06102.md)  |
| <span style='display: inline-block; width: 42px;'>11-05</span> | **ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs**<br><sub>Institution: Cornell University, Microsoft Research<br>The paper presents a new approach to enhance online educational QA platforms using open-source LLMs, and it has undergone extensive evaluation and testing. By combining technologies like RAG, SFT, and DPO, the study not only ensures a significant improvement in the quality of responses but also protects data privacy, making it significant for the development of intelligent QA assistants.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.02775v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.02775.md)  |
| <span style='display: inline-block; width: 42px;'>11-01</span> | **LLMRec: Large Language Models with Graph Augmentation for Recommendation**<br><sub>Institution: University of Hong Kong, Baidu<br>LLMRec, as a pioneering work, introduces LLMs to enhance graph recommendation systems and successfully addresses the issues of sparsity in interaction data and low-quality side information. It improves the performance of recommendation systems through means such as reinforcing user-item interaction edges, item node attributes, and user profiling, ensuring recommendation quality while reducing the impact of data noise.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.00423v5)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-11/2311.00423.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HKUDS/LLMRec.git)</div> |

---

## 2023-10

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>10-20</span> | **The History and Risks of Reinforcement Learning and Human Feedback**<br><sub>Institution: Berkeley<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.13595v2)</div> |
| <span style='display: inline-block; width: 42px;'>10-17</span> | **Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection**<br><sub>Institution: University of Washington<br>The paper introduces SELF-RAG, a new framework that enhances LLM quality and factual accuracy through on-demand retrieval and self-reflection. It makes the LM controllable during the inference phase to suit diverse task requirements and significantly outperforms existing LLMs and RAG models in various tasks. SELF-RAG offers a novel approach to model self-assessment and customization through its decoding algorithm and reflection tokens.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.11511v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-10/2310.11511.md)  |
| <span style='display: inline-block; width: 42px;'>10-11</span> | **OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models**<br><sub>Institution: Tsinghua University, Chinese Academy of Sciences<br>OpsEval, as a comprehensive task-oriented AIOps benchmark, not only assesses the comprehensive performance, reasoning, and practical application capabilities of LLMs but also has the potential to change the evaluation metrics used in future large-scale quality assessments. It provides a solid foundation for ongoing research and optimization of LLMs tailored for AIOps.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.07637v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-10/2310.07637.md)  |
| <span style='display: inline-block; width: 42px;'>10-10</span> | **GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models**<br><sub>Institution: Microsoft Research<br>This study presents a new approach in employing LLMs for answering questions in the field of agriculture, significantly enhancing LLMs' performance on multiple-choice questions through the Ensemble Refinement strategy, showing the broad potential in handling domain-specific problems.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.06225v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-10/2310.06225.md)  |

---

## 2023-09

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>09-04</span> | **Benchmarking Large Language Models in Retrieval-Augmented Generation**<br><sub>Institution: Chinese Information Processing Laboratory <br>This paper introduces a new benchmark based on real news articles for comprehensive assessment of large language models' capabilities in complex informational environments and illustrates the existing limitations of LLMs through the experimental results.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2309.01431v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-09/2309.01431.md)  |

---

## 2023-08

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>08-18</span> | **Learning Representations on Logs for AIOps**<br><sub>Institution: IBM Research<br>The BERTOps model proposed in this paper leverages general LLM representations and specifically tailored pretraining for AIOps log data, effectively improving the performance of automated log analysis tasks and demonstrating significant enhancements. BERTOps not only surpasses existing models but also exhibits superior performance across multiple downstream tasks, facilitating the practical application of AIOps.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2308.11526v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-08/2308.11526.md)  |

---

## 2023-07

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>07-11</span> | **Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps**<br><sub>Institution: UNIVERSITY OF MARYLAND<br>This study analyzed the internal mechanisms of ICL in LLMs using contrastive demonstrations and saliency map analysis, revealing the differential impacts of label flipping, input changes, and complementary explanations on predictions, providing insights for practitioners on curating demonstrations.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2307.05052v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-07/2307.05052.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/paihengxu/XICL)</div> |

---

## 2023-05

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>05-24</span> | **In-Context Demonstration Selection with Cross Entropy Difference**<br><sub>Institution: Microsoft Cognitive Service Research<br>The paper presents a novel Cross-Entropy Difference (CED) method for in-context demonstration selection, provides a theoretical rationale, and achieves performance improvements on large language models of different sizes and types.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2305.14726v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-05/2305.14726.md)  |
| <span style='display: inline-block; width: 42px;'>05-23</span> | **Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning**<br><sub>Institution: Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun<br>The paper explores the internal mechanism of in-context learning (ICL) by large language models from an information flow perspective, identifying the anchoring phenomenon of label words, proposing a new hypothesis, and experimentally validating its effectiveness. Moreover, the insights were used to propose methods for improving ICL performance, providing a theoretical foundation and practical guidance for future related researches.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2305.14160v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-05/2305.1416.md)  |
| <span style='display: inline-block; width: 42px;'>05-19</span> | **How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings**<br><sub>Institution: The Ohio State University<br>The study revealed the critical database knowledge and optimal representations for effective prompting, offering guidance for the application of LLMs in the text-to-SQL task, and pointed out a "sweet spot" in terms of prompt length in the cross-domain setting. The findings may not always be applicable to a specific database, particularly if the database is significantly different from the Spider databases.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2305.11853v3)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-05/2305.11853.md)  |

---

## 2023-03

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>03-31</span> | **A Survey of Large Language Models**<br><sub>Institution: Renmin University of China<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2303.18223v13)</div> |

---

## 2023-02

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>02-08</span> | **A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity**<br><sub>Institution: Centre for Artificial Intelligence Research<br>The article evaluated ChatGPT's reasoning abilities in a more granular way and identified a key issue in LLMs - the lack in non-text semantic understanding. This finding offers significant directions for future improvements and research into the reasoning capabilities of LLMs.</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2302.04023v4)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary_en/2023-02/2302.04023.md)  |

---

