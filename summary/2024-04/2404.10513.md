#### 背景
- **背景**       
    论文讨论了大型语言模型（LLMs）在问答（QA）任务中的表现，指出虽然这些模型在这些任务上的表现达到了前沿水平，但它们倾向于在回应中“幻觉”（生成与事实不一致的信息）。针对这个问题，研究团队介绍了一种基于归因的思考链推理方法（CoTAR），目的是提高信息归因的准确性。这种方法专注于生成以归因为中心的输出。

- **已有的工作**
    已有工作尝试通过提供支持证据来配对生成的文本，以增强模型的可信度并便于错误检测。这一配对过程可以通过不同级别进行，诸如将响应中的每个句子与一组段落标识符配对，或将回答中的个别跨度与特定段落对应起来。然而，这些方法在准确引用方面面临失败的情况，要么过于关注输入的特定部分，忽略其它相关部分，要么引用了太多与答案无关的段落。

#### 核心贡献
- **提出了哈CoTAR方法**
    - **挑战1：如何提高答案的质量和归因的准确性**
        新方法通过让模型在生成输出之前对输入段落进行推理，抓取相关信息以指定生成的归因形式，来解决上述挑战。可以在跨度、句子或段落级别提取信息。这种多步骤的CoT推理方案提高了LLMs生成更高质量答案的能力，并提高了从源头更精确和忠实的归因。
    - **挑战2：如何评估模型的答案和引用质量**
        研究在评估答案的质量时使用了基于n-gram的ROUGE-L方法和基于语义的BERTScore，为评估幻觉使用了在NLI数据集上fine-tuned的HEM1模型。同时为每个引用级别提出了特定的度量标准，来衡量引用内容的质量。所提出的量化指标不仅评价答案质量，也评价引用质量，甚至针对不同引用级别的特定性能。

#### 实现与部署
论文中提出的方法在两个增强了语境的问答数据集上评估：QuoteSum（QSUM）和MS MARCO（MS）。实验利用GPT-4表明了CoTAR在答案质量和归因正确性方面的改进，并且实验结果表明，通过细微调整，一些较小的LLMs也可以在某些情况下达到或超过GPT-4的性能。

#### 总结
本文提出的CoTAR方法针对LLMs在问答任务中倾向于生成不准确归因的问题。通过在输出生成前进行推理，并在不同的归因粒度级别上引导模型，显著提升了模型在答案质量和归因精确度上的表现。