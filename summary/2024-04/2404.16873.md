#### 背景
- **背景**       
    论文提出，虽然大型语言模型（LLMs）在现代机器学习中无处不在并取得了显著的成功，但它们在面对导致生成不当或有害内容的“越狱”攻击时却十分脆弱。为了解决这一问题，多数LLM通过了所谓的安全配对过程，该过程将模型与反映积极社会价值观的人类偏好进行了微调，以生成更有帮助、适当和安全的回应。尽管如此，即使经过微调的LLM在面对越狱攻击时仍然容易受到攻击。

- **已有的工作**
    尽管手动红队演练通过寻找引发越狱的敌对提示来检测这些漏洞，但它既低效又耗时。另一方面，自动敌对提示生成通常会导致在语义上毫无意义的攻击，这些攻击可以很容易地被基于困惑度的过滤器发现，并可能需要从目标LLM获取梯度信息，或者由于过程耗时，不便于扩展。

#### 核心贡献
- **提出了一个名为AdvPrompter的新LLM**
    - **挑战1：快速生成可读的敌对提示**
        挑战说明：自动化敌对提示生成遇到的主要问题是生成的提示在语义上往往毫无意义，也就无法有效欺骗LLM。AdvPrompter通过新算法训练，在几秒钟内生成人类可读的敌对提示，比现有基于优化的方法快约800倍。有效地解决了快速性和语义合理性的平衡问题。

    - **挑战2：不需要目标LLM的梯度信息**
        挑战说明：现有的自动化敌对提示方法可能需要访问目标LLM的梯度信息，这对许多封闭源或商业应用而言是不切实际的。AdvPrompter采用的训练算法不需要获取目标LLM的梯度信息，这使其更加适用于广泛的场景。

#### 实现与部署
在AdvPrompter的训练过程中，通过以下两个步骤交替进行：(1) 通过优化AdvPrompter的预测来生成高质量的目标敌对后缀；(2) 使用生成的敌对后缀对AdvPrompter进行低秩微调。训练有素的AdvPrompter生成的后缀能够掩饰输入指令而不改变它的含义，从而诱使目标LLM给出有害反应。在公开源的目标LLM上的实验结果显示，我们的方法在AdvBench数据集上显示出领先水平，并且能够转移到封闭源的黑箱LLM API。此外，我们证明，通过对AdvPrompter生成的合成数据集进行微调，LLM可以在保持性能（即高MMLU得分）的同时，对越狱攻击更加稳健。

#### 总结
本文提出了一个新型的LLM，名为AdvPrompter，它利用新颖的算法，无需目标LLM的梯度信息，迅速生成人类可读的敌对提示，显著提升了生成速度并保持了提示的语义连贯性。此外，通过AdvPrompter的训练还能增强LLM面对越狱攻击的稳健性，而不牺牲性能表现。