#### 背景
- **背景**       
    本文探讨了如何让大型语言模型（LLMs）在生成回答时根栈自己的信心水平。现有的方法包括让模型产出前k个预测结果，并为每个预测配对一个概率值以反映模型对该预测的信心级别。然而这些方法在信心评估方面缺乏一个客观的实际标准，导致无法准确反映模型对自身回答品质的信心。

- **已有的工作**
    现有工作无法克服缺乏客观实际标准来评价模型信心水平的挑战，这导致无法达成信心和回答品质之间的有效对齐。

#### 核心贡献
- **提出了一个信心与回答质量对齐的方法（CONQORD）**
    - **挑战1：缺乏客观实际标准来评价模型信心**
        为了应对这个挑战，论文采用了强化学习（RL）框架，并建立了简单的初步对齐策略，以及更进一步的CONQORD方法来优化信心与回答质量之间的对齐。这种方法能够减少对预先标注数据的依赖，让模型在没有客观实际标准的情况下，通过自我评估来优化信心水平。

    - **挑战2：现有方法可能引入偏见**
        通过引入一种新的双组分奖励策略，来分别评估回答质量和信心与质量之间的一致性，从而克服了可能由手工分配信心分数导致的偏见，并增加了模型评估自身信心的准确性和中立性。

#### 实现与部署
论文中的评估结果表明，CONQORD方法在TruthfulQA与NQ数据集上展现了强大的信心对齐性能，与现存方法如Vanilla和Top-k相比，在预期校准误差（ECE）、皮尔森相关系数和斯皮尔曼相关系数等指标上均显示出更优的性能。这表明该方法能够使得模型做出的预测与实际质量紧密对齐，同时保留了基础模型的性能并显著提升了校准水平。然而，与增强性能的CoT提示方法相比，论文的方法仍有改进空间，这是未来研究的重点。

#### 总结
本文提出了一个通过强化学习对齐信心和回答质量的方法（CONQORD）。该方法在没有客观实际标准的情况下通过自我评估来优化信心水平，并能够减少偏见，提升了模型预测的准确性和对齐性，但仍需对比绩效更高的方法进行改进。