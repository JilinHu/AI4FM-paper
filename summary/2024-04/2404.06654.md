#### 背景
- **背景**       
    论文讨论了长上下文语言模型（long-context language models，简称LMs）的评估问题。以往的研究通常采用诸如通过长干扰文本检索特定信息（needle-in-a-haystack）这样的合成任务来测试长上下文LMs的性能，但这些合成任务只能评估模型在浅层次的长上下文理解能力。这些评估手段在不同的工作中使用不一致，且无法衡量LMs在长上下文理解方面的其他能力。

- **已有的工作**
    现有的工作无法充分评估长上下文LMs在处理复杂任务时的性能衰退问题，尤其是当上下文长度增加时。以前的基准测试主要关注信息的检索能力，而忽略了模型在更长范围上下文中进行信息追踪、汇总和多跳推理等行为的表现。

#### 核心贡献
- **提出了一个名为RULER的合成基准测试框架**
    - **挑战1：检验LMs在长上下文理解上的不同能力**
        RULER引入了多种类型和数量的“针”，以及多跳追踪和信息汇总这样的任务分类，用以测试长上下文下LMs除了简单检索之外的行为。它提供了可定制的序列长度和任务复杂度设置，使评估包含多样化的任务类型，减少了对参数化知识的依赖，并为不同序列长度和任务复杂度提供了灵活控制。

    - **挑战2：模型在复杂任务和长上下文长度下的性能衰退**
        尽管所有评估的模型在标准的NIAH测试中取得了几乎完美的准确率，但当上下文长度增加时，它们在RULER中的更复杂任务上表现出显著的性能下降。此外，研究还分析了一个声称支持200K上下文长度的模型Yi-34B，揭示了随着输入长度和任务复杂度增加，该模型有相当大的改进空间。

#### 实现与部署
使用RULER对包括GPT-4在内的十种长上下文LMs进行了评估，发现所有模型随着序列长度增加，在复杂任务上都出现了性能的大幅度衰退。尽管这些模型都声称支持32K或更多的令牌长度，只有四种模型在32K长度下保持了令人满意的性能。在具体模型的分析中，Yi-34B在处理大上下文时，常返回不完整的答案，无法精准定位相关信息。此外，随着上下文规模的增大，多个模型出现了越来越依赖参数化知识和在非检索任务中直接从上下文复制的趋势。额外的消融研究显示，长序列训练并不总是导致在RULER上更好的表现，并且更大的模型规模通常与更好的长上下文能力呈正相关。

#### 总结
本论文为长上下文LMs提出了新的评估工具RULER，并开源，用于测试LMs在复杂任务和长上下文理解能力上的表现，并在各种模型和任务复杂度上进行了分析，推动了长上下文LMs的未来研究。