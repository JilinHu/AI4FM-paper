#### 背景
- **背景**       
    文章介绍了Retrieval-Augmented Generation（RAG）在结合大型语言模型（LLMs）和外部知识库的优势方面，在多种自然语言处理任务上取得了显著的提升。但是由于RAG导入长序列生成问题，导致了高计算和内存消耗。

- **已有的工作**
    现有的工作主要集中于LLM推理系统优化，并取得了显著的进展，例如通过共享LLM推理的中间状态以减少重新计算成本。然而，这些努力没有考虑到整合特定的RAG特征以及只优化了LLM推理（没有考虑到序列的增加），在增强请求中导致了原先请求的额外计算成本提升，从而影响了性能。

#### 核心贡献
- **提出了一个名为RAGCache的新方法**
    - **挑战1：增强请求导致了长序列生成问题**
        RAGCache通过创建一个知识树，在GPU和主机内存层次中组织检索到的知识中间状态，并在不同类型的内存中进行缓存，以此来解决这个挑战。

    - **挑战2：提高了多请求情况下的处理速度**
        对于RAG的文档检索顺序很敏感，RAGCache通过引入一个基于前缀感知的Greedy-Dual-Size-Frequency（PGDSF）替换策略和一个缓存感知的请求调度方法来最大化缓存命中率，进而减少end-to-end的延迟。

#### 实现与部署
RAGCache在不同的数据集和代表性的LLM上进行评估显示其表现优于集成Faiss的vLLM，将第一个标记的生成时间（TTFT）提升了4倍，吞吐量提升了2.1倍。与优化GPU内存中间状态复用的SGLang相比，RAGCache将TTFT降低了3.5倍，吞吐量提升了1.8倍。

#### 总结
通过针对性的缓存系统设计和中间状态共享，RAGCache优化了RAG流程的性能，显著提升了处理速度并减少了计算资源的开销。