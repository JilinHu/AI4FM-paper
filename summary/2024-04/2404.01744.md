#### 背景
- **背景**       
    论文讨论了在边缘设备（如个人电脑或智能手机）上部署大型语言模型的挑战，如受限的存储能力和较低的推理速度，并指出了当前研究如使用小型LLM和LoRA方法来克服这些挑战。

- **已有的工作**
    尽管有尝试部署小型化的LLM到边缘设备，但这些工作主要侧重于提高设备上模型的推理速度，对于语言模型调用函数的能力仍然存在瓶颈。

#### 核心贡献
- **提出了一个高效的方法来增强在设备上的语言模型调用函数的准确性和延迟，实现了行业领先的结果**
    - **挑战1：如何在边缘设备上提高函数调用的精确性和减少延迟**
        通过对2B参数模型进行增强，该研究展示了如何利用标记化核心函数的名称和细粒度训练来解决这一挑战，使得在进行函数调用时比GPT-4表现更佳，可节省超过95%的上下文长度，在iPhone上使用时可实现相同电池下37倍的函数调用量，减少大约35倍的延迟。

    - **挑战2：如何训练语言模型以使其理解与特定功能令牌相关的意义**
        通过将函数描述集成到训练数据集中，模型能够学习这些特殊令牌的重要性。利用特殊令牌<nexa_end>作为早期停止标准，实现了快速准确的函数调用。

#### 实现与部署
论文借助谷歌Gemma-2B模型作为预训练模型并采用两种训练方法：全模型训练和LoRA训练。使用AdamW优化器和线性学习率调度器来进行训练。实验评估采用了详尽的基准测试方法，比较了模型生成函数调用的准确度和反应时间，并与顶尖的GPT-4和GPT-3.5模型进行了比较。通过RAG技术和Meta的FAISS进行语义搜索以增强函数调用描述的检索过程，显著提高了精确度和降低了延迟。评估研究了训练数据集的大小和模型训练方法对性能指标的影响，并发现即使是100个数据点的API也可实现98.095%的准确度，这表明在资源受限的情况下仍然能够保持较高的性能。

#### 总结
这篇论文解决了边缘设备上LLM的部署和功能调用效率问题，通过引入特殊的训练方法和减少推理时需处理的上下文量，显著提高了在设备上进行函数调用的准确率和降低了延迟，实验结果表明其对提升函数调用任务的性能具有显著影响。