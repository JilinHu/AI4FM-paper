#### 背景
- **背景**       
    论文分析了大型语言模型（LLMs）在一个名为“对抗性禁忌”（Adversarial Taboo）的两人对抗性语言游戏中的自我训练过程。在这个游戏中，攻击者和防御者围绕攻击者唯一可见的目标词进行沟通。游戏的目标是攻击者诱导防御者不知不觉地说出目标词，而防御者则尝试从攻击者的话语中推测目标词。换句话说，双方都需要对目标词有足够的了解，并且拥有高级别的推理能力，才能在这种保留信息的对话中推理和表达。

- **已有的工作**
    有关提升LLMs的推理能力，先前的工作包括了基于Chain-of-Thought（思考链）的提示工程设计、辅助推理工具的使用、以及基于后预训练和微调的方法。但这些方法通常需要额外的提示设计，具有敏感性和不一致性。此外，它们还依赖于高质量的文本数据和人类标注工作，增加了成本。而自我改进方法虽然通过模型生成的合成数据来增强LLMs，但这些方法仍然需要广泛的高质量问题查询来防止过度拟合。此外，LLMs的判断并不保证是客观的，并且如果LLM已经对某个概念有了错误或偏见的认识，自我改进过程可能会强化和放大LLM的认知失调。

#### 核心贡献
- **提出了一个名为SPAG的新型培训方案**
    - **挑战1：提升LLMs的推理能力**
        论文提出了一种新颖的方法，即让LLMs通过自我对抗性语言游戏的自我播放来提升推理能力。该方法不依赖于额外的提示设计和人类数据标注，旨在提供一种更一般和客观的自我推理改进方法。

    - **挑战2：自我改进方法是否客观和泛化**
        论文通过自我对抗性语言游戏（如Adversarial Taboo）的自我训练来探索推理能力的提升，这种游戏自动生成游戏结果，无需人工干预，从而解决了自我改进方法可能带来的主观性问题。

#### 实现与部署
本论文采用的自我训练方法使用了开源的LLMs，LLaMA-2-7B和Baichuan-2-13B，从50K最常见词的列表中选择目标词进行游戏。通过对游戏结果进行强化学习，作者观察到LLMs在广泛的推理基准上的性能显著提高。更重要的是，通过迭代这一自我播放过程，LLMs的推理能力可以持续提升。

#### 总结
本论文提出了一个名为SPAG的新型训练方案，通过自我对抗性语言游戏的自我播放，有效提升了LLMs的推理能力，并且其改进是可以通过迭代过程持续增强的。