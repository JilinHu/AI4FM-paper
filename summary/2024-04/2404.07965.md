#### 背景
- **背景**       
文章提出传统的大型语言模型（LLMs）的预训练方法对所有训练令牌统一应用下一个令牌预测损失可能不是最优的，指出并不是语料库中的所有令牌对于语言模型的训练都同等重要。

- **已有的工作**
以往的研究偏向于对训练文档进行筛选，通过各种启发式方法和分类器去除噪声，以此优化数据质量。但即使在彻底筛选后，仍然存在一些不影响训练的“噪音”令牌。这些令牌的处理很具挑战性，因为过度的筛选会排除有用数据，而简单移除这些令牌可能会改变文本含义，造成偏见，尤其是当网络数据的分布与下游应用的理想分布不一致时。

#### 核心贡献
- **提出了一个名为RHO-1的新语言模型**
    - **挑战1：令牌级别的学习动态**
    文章分析了语言模型在令牌级别的训练动态，发现在训练过程中，显著的损失降低只限于一小部分令牌。为应对挑战，RHO-1采用选择性语言建模（Selective Language Modeling, SLM），这种方法只在与期望分布一致的有用令牌上进行训练，从而提高了预训练的效率。

    - **挑战2：效率和性能的提升**
    RHO-1通过选择性地训练那些对下游应用有益的令牌，显著提高了在预训练期间的令牌效率，并在下游任务上取得了更好的性能。这种方法利用参考模型对预训练令牌进行评分，并通过关注高额外损失的令牌来训练语言模型，有效地识别了与目标分布相关的令牌。
  
#### 实现与部署
RHO-1在持续预训练中使用15B(十亿) OpenWebMath 语料库，与使用传统因果语言建模的基线相比，RHO-1在9项数学任务中的少样本准确率上实现了高达30%的绝对改进。在微调后，RHO-1-1B 和 7B 分别在MATH数据集上取得了40.6%和51.8%的最新水平结果，而且相比DeepSeekMath，仅使用了3%的预训练令牌。此外，在使用80B（八百亿）通用令牌进行预训练时，RHO-1在15项各不相同的任务中平均提高了6.8%，在效率和性能上都有所提升。

#### 总结
本文提出了RHO-1，这是一种利用选择性语言建模（SLM）的新型语言模型。该模型在预训练中专注于对有用的令牌进行训练，这种方法在数学领域的连续预训练中显示出卓越性能，能够更快地达到基线性能，并且在少量令牌的情况下达到最新的状态。