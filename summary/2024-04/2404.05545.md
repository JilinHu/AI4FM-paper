#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）在做决策任务时，需要估计在系统不同部分进行干预后的因果效果。研究者希望使用LLMs自动化决策时，探讨它们的因果推理能力成为关键。尽管现有工作评估了LLMs检索常识性因果事实的能力，但这些评估没有充分测试LLMs关于干预的推理能力。

- **已有的工作**
    现有工作主要关注在常识性因果事实的检索，没有针对干预的推理提供足够的评估，缺乏对LLMs在响应干预时能否准确更新其对数据生成过程知识的研究。

#### 核心贡献
- **提出了因果图干预效果（IE）预测**
    - **挑战1：设计具体任务来测试LLMs在接收到干预实验信息后更新信仰的能力**
        对此挑战，文中提出了干预效果预测作为一个具体的任务，通过给定因果图和图上某个变量的干预，来确定两个特定变量之间的直接路径如何改变，从而检验LLMs在收到干预信息后更新其信仰的能力。

    - **挑战2：评估LLMs准确推理干预效果的程度**
        为了解决这个挑战，提出了将这些分类问题的实例转换为提示的方法，并引入了干预效果预测基准。同时，因为LLMs对提示设计非常敏感，文章中设计了研究来进一步解开提示设计中的变量名称和它们形成的因果关系的滥用属性对IE预测性能的影响。

#### 实现与部署
评测结果显示，在某些生成提示的选择下，GPT-4版本在预测干预下因果图关系的变化方面显示出有希望的准确性。然而，发现LLMs对于实例化提示时的变量名称选择非常敏感。当提示包含LLMs可能已经接受训练的事实时，它们的表现显著下降。这一结果强调了在创建基准时进行细致设计的重要性，以避免得出不合理的结论。

#### 总结
本文对大型语言模型（LLMs）因果推理能力进行了评估。通过提出干预效果预测，它主要测试LLMs在干预实验后如何更新自己对事实的理解。结果显示GPT-4在某些条件下能够准确预测干预效果，但提示设计的微小变化会显著影响其表现。