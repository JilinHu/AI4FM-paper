#### 机构&分类
IBM Research
Knowledge and Retrieval
  
---

#### 背景
- **背景**       
    论文讨论了大型语言模型(LLMs)的一个重要应用场景——在边缘计算设备(如手机、智能传感器等)上部署和使用。这些设备通常具有较小的内存和计算能力，不足以直接运行庞大的LLMs。企业希望在多种边缘设备上实现LLMs的部署，以应对不同的硬件需求和动态变化的资源水平。

- **已有的工作**
    当前研究主要集中在LLMs的压缩上，如参数剪枝、低秩近似和量化等。然而，现有的方法大多只是产生一个单一压缩模型，不能满足多种设备需求。超网络(Supernet)训练方法是为了同时训练具有权重共享的多个子网络而提出的，但当LLMs需要多种部署场景的细调时，全参数的超网络训练方法的适用性受限。

#### 核心贡献
- **提出了一个多阶段低秩精调超级变换器的方法(Multistage Low-rank Fine-tuning of Super-transformers, MLFS)**
    - **挑战1：提高在边缘计算设备上部署LLMs的效率**
        现有研究表明，压缩方法产生的单一模型无法适应多目标部署的需求。论文提出的MLFS方法不仅对LLMs进行有效压缩，还允许创造多种尺寸的小型模型以适应不同边缘设备的要求。

    - **挑战2：训练具有共享权重的超网络**
        基于参数的低秩精调方法(如LoRA)减少了可训练参数的数量，但这些方法无法应用于超网络