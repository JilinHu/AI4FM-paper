#### 背景
- **背景**       
    文章指出，自动化作文评分（AES）具有替代人工打分的潜力。与人工评估相比，AES可以显著减少成本、时间和打分间的不一致性。然而，目前AES系统往往需要特别为特定的写作任务定制，依赖标记好的作文，这要求巨大的成本和专业知识来获取这些数据。在处理未知写作提示的作文时，这些模型往往会面临挑战。

- **已有的工作**
    已有工作通过构建特定于写作提示的监督模型来实现与人类评分的高度匹配，但这些模型在面临未见过的写作提示的作文时会遇到困难。此外，许多研究利用大型语言模型（LLMs）作为机器生成文本评价指标，但在零样本自动化作文评分中利用LLMs的潜力还未被充分探索。

#### 核心贡献
- **提出了一个零样本作文评分框架(Multi Trait Specialization, MTS)**
    - **挑战1：如何利用LLMs在未见过的写作提示下评分**
        MTS通过将写作能力分解为不同的特质，并为每个特质生成评分标准，使用ChatGPT从多轮对话中得到每个特质的评分。这种处理方式旨在超越单一步骤的整体评分方法，以实现更好地与人类评分匹配。

    - **挑战2：如何确保LLMs生成的评价准确性**
        通过设计对话机器人使其在对话中回引文本内容（引用检索和评分机制），以提供忠实于作文内容的评价，然后基于给定的评分标准分配评分。通过平均特质分数并使用最小-最大缩放配合异常值截断机制，最终得到整体得分。

#### 实现与部署
MTS在ASAP和TOEFL11两个基准数据集上的实验结果显示，在所有LLMs和数据集组合中，MTS的平均二次加权卡帕 (QWK) 评分一致性超过了直接提示（Vanilla）评分，最高增益在TOEFL11上为0.437，在ASAP上为0.355。此外，小型Llama2-13b-chat模型在MTS的帮助下显著超过了ChatGPT，便于实际应用的有效部署。MTS在平均QWK上仅比监督式SOTA的成绩低0.171（在TOEFL11）和0.257（在ASAP），这为监督模型提供了一个强有力的零样本替代方案。

#### 总结
该研究提出了一个零样本的大型语言模型作文评分框架（MTS），通过多轮对话来为作文的不同写作特质打分，并采用最小-最大缩放和异常值截断机制来得到最终得分。MTS在准确度上显著优于直接提示评分方法，并在小型化部署中优于ChatGPT，提供了监督学习之外的零样本作文评分方案。