#### 背景
- **背景**       
    过去的研究表明，首选项优化方法在对预训练语言模型进行人类需求对齐时能够获得巨大的提升，但这些方法普遍在推理任务上的改进效果有限。尽管有自我奖励的语言大模型（Self-Rewarding LLMs）、SPIN 等方法的应用，它们在标准推理任务上的表现不尽人意。

- **已有的工作**
    已有方法虽然在一般指令调优任务上表现良好，却在标准推理任务上仅有轻微的提升或甚至性能下降。此外，利用偏好优化来训练生成性推理模型在现有的方法如 STaR，RestEM，和 V-STaR 等中并未被应用。

#### 核心贡献
- **提出了一个xxx**
    - **挑战1：如何优化生成的思路链（Chain-of-Thought, CoT）候选项之间的偏好，以提高正确答案得出的推理形式的效率**
        这个挑战的困难点在于为一组正确答案与错误答案的偏好对构建合适的训练数据集，并通过模型优化学习这些偏好。论文通过引入DPO（Dynamic Preference Optimization）损失修改版，加上负对数似然（Negative Log-Likelihood，NLL）损失项来应对这一挑战，结果表明该组合对性能至关重要，成功改善模型在多轮迭代后的理解能力。

    - **挑战2：在没有额外数据集的情况下，仅依赖训练集例子，提高推理任务上的性能**
        训练模型通常依赖外部数据集来增强其推理能力。然而，作者发现仅通过训练集的例子，通过迭代优化方法，可以有效地提高推理性能，从而在三个不同的数据集上取得显著的性能提升。
      
#### 实现与部署
通过迭代推理偏好优化（Iterative Reasoning Preference Optimization, 简称 Iterative RPO），论文所提方法在几个基准测试上实现了重大改进。例如，在 GSM8K 数据集上，将零样本性能从55.6%提高到81.6%，并且通过32个样本的多数投票提升到88.7%，在 ARC-Challenge 上从77.8%提升到86.7%，在没有使用额外数据集的情况下，这些结果超过了其他基于Llama-2的模型。此外，研究者提供了消融实验来表明是哪些组件导致了这些改进。总体而言，这一方法为提高大型语言模型在广泛任务上的推理能力提供了一个简单有效的方案。

#### 总结
本文提出了一种迭代推理偏好优化方法，通过在推理任务上应用偏好优化，特别是针对CoT推理，并通过在迭代训练中引入NLL损失项来提升模型性能。实验证明，该方法在数次迭代后能够有效提升推理性能，最终达到性能饱和。