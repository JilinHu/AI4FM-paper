#### 背景
- **背景**       
    大型语言模型（LLMs）面临着内存、延迟和功耗的限制。为了满足这些需求，人们提出了基于输入的动态稀疏（dynamic sparsity）各种形式，它通过跳过整个层的计算来减少计算量。动态层稀疏性质使得网络只需学习主分支的残差函数，而兼容跳过连接的设计和训练。利用这种设计，研究人员提出了径向网络（Radial Networks），这一由训练有素的路由模块指导的网络，支持在任意层间的路由。

- **已有的工作**
    此前的工作聚焦于网络宽度方向的动态稀疏或通过早期退出（early-exit）动态稀疏网络深度，早期退出需通过特殊训练并仅能跳过对网络贡献最大的后续层，限制了模型的实用性。而现有的大型模型中，每层对输出贡献较少，为动态层稀疏提供了新的机会。

#### 核心贡献
- **提出了一个新的神经网络结构**
    - **挑战1：如何利用层稀疏性质提高效率**
        现有的大型语言模型对输出的每一层贡献很少，提出径向网络可以通过训练有素的路由模块在网络层间进行令牌级路由，减少整个序列生成所需的整体资源，并降低计算和服务成本。

    - **挑战2：设计一个能够自适应路由的路由器**
        通过一个小型多层感知机（MLP）模型学习路由器，将中间嵌入式映射到输出路由器对数，并通过softmax函数转化为概率，然后选择概率最高的层作为前向传播的下一层。这样可以在不增加网络最大层深度的情况下，动态调整网络深度。

#### 实现与部署
试验结果表明，对于OPT-13B模型，在输入序列的生成阶段，大多数被跳过的层是网络的前半部分。层级贡献率的中位数对于OPT-125M模型是20%，对于OPT-66B模型则降低到5.9%，表明随着模型规模的增大，层稀疏性展现出扩展的趋势。此外，实验数据还显示模型的残差比率似乎与模型参数的数量紧密相关，而不仅仅是层数。径向网络通过令牌的计算变量提供了一种对于生成整个序列的资源更加高效的方法。

#### 总结
本论文提出了径向网络，这是一种新型神经网络结构，通过动态层稀疏性和一个经过训练的路由模块来实现令牌级的层间路由。这不仅提高了模型的性能，还显著降低了计算和服务成本，为大型语言模型的进一步扩展提供了可能。