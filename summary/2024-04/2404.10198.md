#### 背景
- **背景**       
   论文探讨了大型语言模型(LLMs)面临的一个重要问题：它们往往倾向于产生幻觉，而且通常只能回应训练语料库中包含的知识，因此无法回答关于近期事件或受公众限制信息的问题。为此，通常使用检索增强生成（RAG）来提供相关的检索内容，并改进LLMs的准确性。

- **已有的工作**
    尽管RAG被用于解决LLMs的幻觉问题并提供及时的知识，但大部分有关LLMs能力的评估还是在非RAG环境中完成的，这忽视了模型默认和启用RAG时答案之间可能发生的巨大差异。此外，网络结果的不断变化和可能包含的过时或错误信息，要求客观评估RAG启用的LLM行为，特别是在越来越多依赖RAG系统提供事实信息的领域中。

#### 核心贡献
- **提出了一个量化RAG与LLMs内部知识之间的张力**
    - **挑战1：不匹配的信息如何影响LLMs**
        对于LLMs内部知识和提供的检索信息不一致的情况，评估模型如何做出选择。分析发现，LLM倾向于使用检索信息的程度与模型在无上下文情况下的回答信心（其先验概率）成反比。当原始上下文渐进式地被修改为不真实的值时，LLM越来越倾向于回归到其先验知识上。

    - **挑战2：评估体系的缺失**
        目前缺乏系统性地探讨一个模型的信心（通过对数概率评估）及其对于RAG提供信息的偏好。本研究通过测试LLMs回答问题并在引入不同程度的参考文献文件干扰时测量词语概率，来解开这两个竞争力量。

#### 实现与部署
研究评估了GPT-4进行问答处理的能力，特别是当参与问答的RAG文件被引入不同级别的干扰时。核心数据集由来自6个不同领域的1,294个问题组成，测试表明，当模型的内部先验较弱时，越有可能重复错误、修改过的信息；反之，当先验辐射较强时，模型对错误信息的抵制力更大。这表明了LLMs在其预训练知识和RAG提供的上下文内容之间存在固有的张力。

#### 总结
论文通过分析在RAG环境下LLMs内部知识与检索信息之间的张力，发现了LLMs倾向于遵循RAG信息的程度与模型在无上下文情况下的回答信心成反比。研究基于跨超过1200个问题的六个领域数据集，揭示了在模型的预训练知识与检索到的信息之间的固有冲突。