#### 背景
- **背景**       
    论文研究了在强大的oracle（智能的决策实体）的偏好反馈作用下，大型语言模型（LLMs）后训练（post-training）过程中的自我迭代改进。传统的做法是通过从人类反馈中学习的强化学习（RLHF）来后训练LLMs，它通常涉及奖励学习和后续的政策优化。然而，这种奖励最大化的方法有一个限制，即它依赖于“点对点”的奖励，如Bradley-Terry模型，不能表达复杂的非传递性或循环偏好关系。尽管RLHF的最新进展显示奖励学习和政策优化可以合并为单一的对比目标以提高稳定性，但它们仍然局限于奖励最大化框架。

- **已有的工作**
    为了解决潜在问题，新兴的研究开始绕开奖励最大化的假设，直接针对“成对”或一般偏好进行优化。奖励函数定义为为输入x的单一响应y输出一个标量分数r(x, y)，在所有情况下都不能表达输出对之间的一般偏好y ≻ y' | x，例如非传递性或循环偏好。因此，在奖励最大化下训练的LLMs并不能总是与人类偏好保持一致。此外，即便在可以完美表示奖励为基础的BT模型下的偏好的设置中，向奖励优化也会产生问题行为。最后，实践中的奖励函数随着政策的转换很快就会变得“陈旧”，从而容易受到“奖励黑客”的攻击。

#### 核心贡献
- **提出了一个名为直接纳什优化（DNO）的算法**
    - **挑战1：奖励函数表达一般偏好的有限性**
        直接优化一般偏好函数本身，作为某种甲骨文实例，这些研究将RLHF重新构建为寻找一个具有来自规则化或非规则化一般偏好函数的“薪酬”的两人游戏的纳什均衡。 DNO 框架通过使用简单的批处理策略和基于回归的学习目标，实现了这种优化，这让 DNO 既稳定又可扩展。

    - **挑战2：缺乏如何扩展与一般偏好优化相关的清晰度**
        DNO作为一个批处理的在线策略算法，其回归基础的学习目标使得DNO稳定且可扩展，并充分融合了对比目标的可扩展性和一般偏好优化的理论严谨性。

#### 实现与部署
在实验中，使用DNO对齐的7B参数Orca-2.5模型在AlpacaEval 2.0上实现了对抗GPT-4-Turbo的行业领先胜率33%（即使在控制响应长度后），比初始化模型提高了26%（由7%提高到33%）。它在性能上超过了更多参数的模型，包括Mistral Large、Self-Rewarding LM（70B参数）和老版本的GPT-4。文章还进行了消融研究，分析了选择偏好对和使用LLMs作为偏好注释者的关键设计决策。这些结果突出了DNO在LLMs后训练中的潜力，并为人工智能研究社区提供了可行的洞见。

#### 总结
这篇论文介绍了DNO——一种能够将对比学习的简洁性与从优化一般性偏好而来的理论普适性相结合的算法。DNO在后训练大型语言模型方面显著提升性能，它的成功实证了通过优化一般偏好来指导模型学习与人类价值观保持一致是可能的。