#### 机构&分类
CMU  
Pre-training and Instruction Fine-tuning
  
---

#### 背景
- **背景**       
    文章介绍了大型语言模型(LLMs)在众多领域的成功应用，但它们也带来了巨大的计算和内存需求，特别是在模型size庞大、层数多且宽，进而导致推理速度缓慢的情况下。另外，LLMs中的稀疏结构，尤其是前馈（Feedforward，以下简称FF）块中的稀疏结构引起了关注，因为大量的计算资源被浪费在了对最终结果影响很小的中间特征上。此外，这些效率问题在对延迟敏感的场景中，如聊天机器人和自动驾驶车辆中特别突出。

- **已有的工作**
    已有的工作尝试通过修剪模型权重和构造专家混合体(MoEs)来利用LLMs中的稀疏性以提高效率。修剪方法通过移除低影响的预训练权重来减少存储开销，但这并不总能在实践中转化为速度提升，除非修剪是以硬件友好的方式进行的，这通常会导致更大的性能恶化。而MoEs通过根据输入适应性选择模型的子集以保持原有性能，但它们同样存在弊端。除非模型是以这种方式进行训练的，否则它需要学习一个廉价但有效的门控函数（专家选择机制），有时甚至需要进行完整的