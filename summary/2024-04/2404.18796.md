#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）成为更高级之后，我们评估它们质量的能力却被超越了。找到足够探究特定模型属性的数据很困难，而仅评估模型自由生成文本的正确性就是一个挑战。当前很多评估依赖于使用LLMs本身作为评判打分其他LLMs的输出，这种方法虽然流行，但成本高昂，已显示出带有内部模型偏差的问题。

- **已有的工作**
    使用单个大型模型（如GPT-4）作为评判存在偏差和成本问题，尤其是这些模型常常偏爱其自身生成的输出，并且评判模型往往选择最大、通用能力最强的模型作为评判，这即缓慢又昂贵，限制了适用性与可及性。

#### 核心贡献
- **提出了一个Panel of LLm evaluators (PoLL)**
    - **挑战1：评估LLM质量**
        文章提出了PoLL方法，它由多个不同家族的较小模型组成，这样的方法比使用单一大型评判模型表现得更好，并且在降低评分中的内部模型偏差同时减少了成本，超过七倍的成本效率提升。

    - **挑战2：不同评价设置和模型家族的适用性**
        文章对PoLL在不同评价设置下的表现进行了测试，其中包括单跳、多跳问答任务和Chatbot Arena，侧重于评估方法是否适用于多种情境和模型家族，并验证PoLL在这些设置中作为评估大型模型的有效替代方法。

#### 实现与部署
实验表明，在问答数据集和Chatbot Arena上，使用PoLL方法进行排名与人类判断的一致性更高。例如，在KILT数据集的不同单跳问答任务中，PoLL的Cohen's κ值与人类评判的一致性最强。同时，PoLL还能够在多个任务和数据集上 consistently 出色表现，并减少了模型间评分的偏差。此外，使用PoLL的成本显著低于单个大型模型，根据论文所述，使用PoLL的特定实例的成本是每输入1.25美元加每输出4.25美元，而GPT-4 Turbo的成本则是每输入10美元加每输出30美元。

#### 总结
该论文发展了一种以成员来自不同模型家族的小型模型组织成的“评审团”来评估LLM生成物的新方法，称为PoLL，显示出在不同任务中的适用性以及成本效率，减少了LLMs作为评判时存在的偏见问题。