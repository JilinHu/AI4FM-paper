#### 背景
- **背景**       
    本论文探讨了大型语言模型（LLMs）是否真的能够进行类比推理。类比推理是指人类通过回想与新任务相关的过去经验来处理不熟悉的挑战，这种能力在心理学中被认为与回忆无关的经验相比可以更好地帮助人们处理新任务。如今，人工智能社区发现，在上下文中自我生成相关例子可以帮助LLMs比手工编写的提示更好地解决给定问题。但目前还不清楚相关性是否是引发这种能力的关键因素。

- **已有的工作**
    先前的研究提出，LLMs可以通过自我生成与查询相关的示例作为上下文来更好地解决新问题。这表明LLMs可能具备了一些类比推理的能力。不过，目前尚不清楚LLMs在进行类比推理时，相关性是否是关键。此外，一些研究还没有系统地评估LLMs进行类比推理的能力，也没有进行广泛的实验和分析。

#### 核心贡献
- **提出了一个xxx**
    - **挑战1：自我生成相关例子的效果**
        本论文通过广泛的实验和分析，发现让LLMs自我生成随机例子能够意外地达到与生成相关例子相当甚至更好的性能，例如，在GSM8K数据集上使用随机生物学例子可以获得4%的性能提升。这一发现挑战了相关例子在类比推理中的重要性，表明LLMs并不总是可以进行类比推理。

    - **挑战2：自我生成例子的准确性**
        为了解决上述挑战，论文通过受控实验指出，自我生成例子的准确性是关键因素，由此提出了两种改进方法，能在显著降低推理成本的同时取得更好的性能。具体来说，让LLMs随机生成一些问题，并手动验证它们的正确性，然后使用这些固定集问题作为所有测试样例的上下文学习示例。
    - ...
#### 实现与部署
论文主要使用GPT-3.5模型作为LLMs，并在各种推理密集型任务上，包括数学推理和其他推理任务（如逻辑和时间推理）上进行评估。论文基于不同随机种子运行三次实验，并将发现一致的结果与相关工作进行对比，显示自我生成的无关例子在性能上一致超过了相关示例。此外，研究还扩展到开源的Llama-2-Chat模型，验证了观察结果和结论在不同模型中的一致性。

#### 总结
本论文系统地评估了LLMs进行类比推理的能力，并提出了两种可以在显著降低推理成本的同时获得更好性能的方法。研究结果表明，与以前认为相关性至关重要的观点相反，自我生成的无关例子在某些任务上可以达到相当甚至更好的性能。希望本研究能刺激更多关于自我生成上下文设计的进一步研究。