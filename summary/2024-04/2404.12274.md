#### 机构&分类
None

#### 背景
- **背景**       
    文章讨论了大型语言模型（LLM）在面对随机化平滑技术增强其鲁棒性方面的新方法。

- **已有的工作**
    以往的工作通过纯粹的随机化平滑技术增强大型语言模型的鲁棒性，但这些方法未能有效解决模型在受到恶意文本攻击时保持性能的问题。已有方法缺少一种激励模型自行去除输入噪声的能力，因此在追求更高的模型性能时会受限。

#### 核心贡献
- **提出了一个名为SELFDENOISE的框架**
    - **挑战1：如何在输入中引入随机性同时保持模型性能？**
        这一挑战难在于平衡引入的随机性不过高以至于损害原始输入信号的完整性。SELFDENOISE方法通过在随机化平滑的基础上加入一个自降噪步骤，使模型能够更好地处理被随机屏蔽的输入，在保持输入信号完整的同时提高鲁棒性。

    - **挑战2：保证模型在对抗性攻击面前的鲁棒性**
        对抗性攻击会试图通过精心设计的输入来误导模型。SELFDENOISE方法使用一种自动提取噪声的策略，允许模型在面对含有恶意修改的输入时维持高准确度，并提供了认证鲁棒性的保证。

#### 实现与部署
文章中的实验在两个公知数据集SST-2和Agnews上进行，利用了SELFDENOISE方法，并将其与其他方法进行了对比，其中包括基准的随机化平滑方法和另一种通过同义词替换来增加噪声的平滑方法。结果表明，在反对对抗性攻击的鲁棒性方面，SELFDENOISE在DeepWordBug攻击下比第二好的方法在SST-2数据集上提高了13.2%，在Agnews数据集上提高了19.7%。此外，SELFDENOISE方法还能够在各种扰动比例下显著提高认证准确率，例如在5%的扰动下，其表现比RANMASK在SST-2数据集上好11.5%，在Agnews数据集上好26.3%。

#### 总结
这篇论文为增强大型语言模型的鲁棒性提供了一种新的技术——SELFDENOISE。该方法通过一种简单的自降噪过程显著提高了模型对抗恶意文本攻击的能力，同时保持或提高了其在正常条件下的准确度。
  
---

#### Institution&Category
None
  
---

#### Background
- **Background**
The paper discusses a novel approach for enhancing the robustness of Large Language Models (LLMs) through techniques of randomized smoothing.

- **Existing Work**
Previous work has enhanced the robustness of LLMs using plain randomized smoothing techniques, which did not effectively maintain model performance when subjected to malicious text attacks. Existing methods lacked a mechanism to encourage the model to remove input noise itself, thereby limiting performance enhancements.

#### Core Contributions
  - **Introduced the SELFDENOISE framework**
    - **Challenge 1: How to introduce randomness into input while maintaining model performance?**
        The challenge is balancing the introduced randomness to not overly compromise the integrity of the original input signal. The SELFDENOISE approach adds a self-denoising step on top of randomized smoothing, allowing the model to better handle randomly masked inputs, and enhances robustness while preserving input signal integrity.

    - **Challenge 2: Ensuring the model's robustness in the face of adversarial attacks**
        Adversarial attacks attempt to mislead the model with carefully crafted inputs. The SELFDENOISE approach uses a strategy of automatic noise removal to allow the model to maintain high accuracy with inputs that contain malicious modifications, and it provides certified robustness.

#### Implementation and Deployment
The experiments are conducted on two well-known datasets, SST-2 and Agnews, utilizing the SELFDENOISE method, and comparing it to other methods, including a baseline randomized smoothing method and another smoothing method that adds noise by synonym replacement. Results showed that in terms of robustness against adversarial attacks, SELFDENOISE improved performance by 13.2% on the SST-2 and 19.7% on the Agnews datasets under DeepWordBug attack than the second-best method. Moreover, SELFDENOISE significantly improved certified accuracy across all perturbation scales, for instance, outperforming RANMASK by 11.5% on the SST-2 and by 26.3% on Agnews datasets at a 5% disturbance scale.

#### Summary
This paper presents a new technique—SELFDENOISE—for enhancing robustness in large language models. The method significantly increased the models' ability to withstand malicious text attacks with a simple self-denoising process while maintaining or improving accuracy under normal conditions.