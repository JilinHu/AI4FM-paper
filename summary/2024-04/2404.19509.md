#### 背景
- **背景**       
    文章介绍了非字面含义的理解对于大型语言模型（LLMs）变成类人社交交流者的重要性。研究者指出，以往评估LLMs在言外之意（Conversational Implicature）理解方面的能力大多数集中在英语上，缺少对其他语言的研究。此外，现有的对话推理数据集通常涉及较少对话轮次，而日常交流往往包含更完整的上下文及轮次交替。

- **已有的工作**
    已有的工作使用的数据集主要采用二选或多选问题，这在当前LLMs能够生成情境解释性文本并可以直接评估其质量的时代显得不够充分。

#### 核心贡献
- **提出了一个名为SwordsmanImp的数据集**
    - **挑战1：中文多轮对话言外之意的理解**
        在编制SwordsmanImp时注重中文的多轮对话体现的言外之意，从情境中突出一句具有非字面含义的话，并提供其中一个实际言外之意的解释、一个字面解释和两个包含相关信息的错误推断。挑战在于LLMs是否能识别并解释这种非字面含义，论文采用了专家精心节选的对话来检测LLMs的理解能力。

    - **挑战2：LLMs在处理不同Gricean maxims派生的言外之意的差异**
        研究者细致地标注了每段对话中违反了哪些合作原则中的格莱斯准则。通过这些标注来探究LLMs处理来自不同maxims的言外之意时是否存在显著差异，以及其理解和解释难点所在。

#### 实现与部署
根据对多个封闭来源和开放来源的LLMs进行测试，结果显示GPT-4在多选问题上达到了人类水平的准确性（94%）。CausalLM以78.5%的准确性位居其后。其他模型的准确性从20%到60%不等。此外，人类评价者被要求对LLMs生成的言外之意解释在合理性、逻辑性和流畅性方面进行评分。尽管所有模型都可以生成流畅且自洽的文本，但除了GPT-4外，其他模型在合理性方面的得分较低，表明大多数LLMs无法提供满意的对话言外之意的解释。

#### 总结
该研究通过创建一个新的中文多轮对话数据集SwordsmanImp评估LLMs理解言外之意的能力，特别是在涉及大量上下文和轮换的对话中，并揭示了LLMs在理解和解释非字面含义时的挑战和局限。