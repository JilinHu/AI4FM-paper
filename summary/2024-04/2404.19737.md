#### 背景
- **背景**       
    这篇论文探讨了如何通过改变大型语言模型（如GPT和Llama）的训练方式来提高样本的效率。目前，这些模型通常使用下一标记预测损失函数进行训练。
- **已有的工作**
    论文指出，现有的下一标记预测方法在训练中关注于预测单个未来的标记，而没有充分利用模型预测多个标记的能力，尤其是在处理大规模模型和数据集时。

#### 核心贡献
- **提出了一个以多标记预测为基础的训练方法**
    - **挑战1：提高样本效率**
        目前的语言模型通常关注于下一标记预测，该方法要求模型在训练语料库的每个位置上预测后续的n个标记，通过共享模型的主干和独立的输出头来实现。多标记预测被证明在代码和自然语言模型的下游任务中没有额外的训练时间成本，并且提高了性能，特别是在规模更大的模型以及对算法推理能力的培养方面。
    - **挑战2：提升推理速度**
        通过实现不同批量大小的贪婪自推理解码，可以使得训练有着4标记预测的模型在推理时速度提高3倍。论文实现了一个内存高效的多标记预测实现，有助于在不影响训练效率的同时提高推理效率。

#### 实现与部署
实验显示，在至少91B代码标记的训练中，不同大小（从300M到13B参数）的模型表现出多标记预测随着模型大小的增加而变得更有效。在MBPP和HumanEval基准测试中，大型模型在使用相同的计算预算的情况下，通过使用多标记预测能够获得更高的性能。此外，实验结果还表明，相对于下一字节预测，在字节级别模型上多字节预测取得了显著的改进，解决问题的效率提高了67%。训练包括4个未来标记的模型可以在HumanEval和MBPP的评估中连续跑通1，10和100的度量标准，表现超过了其他模型。这些结果表明，多标记预测不仅提高了模型在更大规模数据集上的性能，还能显著提高推理速度，特别是在使用更高级别的字节预测模型时。

#### 总结
论文提出了一种新的训练大型语言模型的方法，通过预测多个标记而不是单个来提高样本效率，并展示了如何提升生成任务中的性能并加快推理速度。实验证明了这种方法在提升大型模型性能和推理效率方面的显著优势。