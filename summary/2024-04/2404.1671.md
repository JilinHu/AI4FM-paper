#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）虽然已经被部署到许多应用中，但其高昂的计算和内存需求导致在GPU服务器上部署时带来了高昂的经济和能源成本。已有的加速解决方案虽然允许部署到普通GPU上，但准确率会大幅下降。进一步加速LLMs以适用于手机或边缘设备仍是一个活跃的研究领域。

- **已有的工作**
    尽管现有的LLM加速方法诸如减少非零权重、每个权重的位数、每层的头数等，但它们通常需要专用硬件或软件内核。更少的研究专注于通过在推理过程中提早退出来减少所需的层数，这是一个无需特殊硬件即可实现加速的方法。

#### 核心贡献
- **提出了一个名为LayerSkip的端到端解决方案**
    - **挑战1：加速大型语言模型而不丢失准确率**
        LayerSkip通过训练时使用层dropout和早期退出损失来实现在模型早期层次退出的健壮性，为同一个模型内部创造不同尺寸的子模型。实验表明，在没有增加任何辅助层或模块的情况下，这种训练方法可以提高早期层次退出的准确率。

    - **挑战2：提高推理速度并减少内存足迹**
        LayerSkip提出了一种新颖的自我推测解码(self-speculative decoding)方法，该方法在早期层次退出后，通过剩下的层来验证和纠正。与其他推测解码方法相比，这一方法具有更小的内存足迹，并且受益于草稿阶段和验证阶段共享计算和激励。这项解决方案在不同的任务上实现了1.34×到2.16×的加速。

#### 实现与部署
实验结果表明，在不同的Llama模型大小和训练类型（包括从头开始的预训练、持续的预训练、特定数据域的微调及特定任务的微调）下，LayerSkip方案能够实现显著的推理加速，加速比例在CNN/DM文档摘要任务中高达2.16×，编码任务中1.82×，以及TOPv2语义解析任务中2.0×。这证明了LayerSkip方案在提高大型语言模型运行效率方面的有效性。

#### 总结
LayerSkip是一个新颖的端到端解决方案，能够在不牺牲准确率的情况下显著加速大型语言模型的推理过程，具有实际应用价值和潜力。