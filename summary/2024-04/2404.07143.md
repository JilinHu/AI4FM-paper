#### 背景
- **背景**       
    论文指出，现有的大型语言模型（LLMs）在处理极长输入序列时面临的关键问题是如何有效地利用有限的内存和运算资源。现有的转换器（Transformer）模型仅能处理有限长度的序列，这在处理包含复杂结构或需要长期依赖的任务时成为制约因素。

- **已有的工作**
    为了应对序列长度限制，已有研究尝试使用压缩记忆系统来弥补，但现有大型语言模型尚未看到一个有效实用的压缩记忆技术，能够在简洁性和质量之间取得平衡。

#### 核心贡献
- **提出了一个名为Infini-attention的注意力机制**
    - **挑战1：处理无限长输入**
        Infini-attention机制将压缩记忆整合到标准的注意力机制中，支持蒙版的局部注意力和长期的线性注意力机制。通过这种方式，LLMs能够通过持续的预训练和微调，自然地扩展到无限长的上下文。

    - **挑战2：内存和计算资源的限制**
        该方法允许转换器模型以有界的内存和计算资源处理极长输入，并以流式方式进行。通过复用关键值状态，Infini-attention能够构建全局压缩和局部细粒度的状态，为每个注意力层提供高效的注意力机制。

#### 实现与部署
在实验中，该研究显示解决方案在长上下文语言建模基准测试上超越了基线模型，且在内存大小方面有着114倍的压缩比。当训练到100K序列长度时，模型取得了更佳的困惑度。一个1B LLM能够自然扩展到1M序列长度，并且在注入了Infini-attention后解决了密钥恢复任务。最后，一种采用Infini-attention的8B模型在持续预训练和任务微调后，在一个500K长度的书籍总结任务上达到了新的SOTA结果。

#### 总结
该研究提出一种全新的注意力机制Infini-attention，它通过将压缩记忆与标准的点积注意力相结合，并在设计上支持插拔式的持续预训练和长上下文调整，使得LLMs能以有界的内存和计算资源处理无限长的上下文。