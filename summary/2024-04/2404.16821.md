#### 背景
- **背景**       
    论文探讨了多模态大型语言模型（MLLM）在将文字和视觉信息桥接上的重要进步，尽管如此，开源模型与专有商业模型之间在多模态理解能力上仍然存在显著差距。论文指出这种差距主要体现在三个方面：参数规模、图像分辨率以及多语言能力。

- **已有的工作**
    当前商业MLLMs 通常拥有超过1000亿的参数规模，而开源模型常用的视觉基础模型参数规模急剧较小，通常与7亿或13亿的LLMs 结合使用。这导致在详细场景和文件理解上的能力上有显著的能力差距。另外，尽管开源模型依赖LLMs的零样本能力来支持其他语言，但通常只使用英语数据进行训练，这导致在非英语场景理解和OCR任务中的表现并不理想。

#### 核心贡献
- **提出了一个名为InternVL 1.5的模型**
    - **挑战1：通用性和弹性问题**
        为了增强模型性能和可用性，论文采用了连续学习策略，大规模的视觉基础模型InternViT-6B得到了精炼，这不仅提高了模型对视觉内容的理解能力，而且改善了它在不同LLMs中的适应性。

    - **挑战2：多模态输入的分辨率和任务适用性**
        论文采取了动态高分辨率策略，根据图像的宽高比和分辨率，将图像分割成448×448像素的瓷砖，可支持多达4K分辨率的输入，并且使用缩略图来捕捉全局上下文，以此提供灵活的分辨率选择及详细的跨解析度适用性。

    - **挑战3：语言能力差异问题**
        论文收集了大量不同场景、高质量的自然场景、图表、文件和对话的公共数据集，并用英文和中文进行了注释。同时开发了一个使用开源LLMs的数据翻译流水线，轻松扩展到更多语言，极大地增强了在中英文双语OCR和相关任务中的表现。

#### 实现与部署
评估结果显示，相较于其他开源和专有的模型，InternVL 1.5表现出相当的竞争性能，在18个基准测试中取得了8项最佳成绩。特别地，该模型在处理中文相关任务上通常优于领先的商业模型GPT-4V。

#### 总结
InternVL 1.5是一个强大的开源多模态语言模型，致力于弥补开源和商业模型在多模态理解方面的性能差距。该模型的优势包括改善视觉理解、处理动态高分辨率图像以及高质量的双语数据集的使用，这些它在多项任务中表现出色。