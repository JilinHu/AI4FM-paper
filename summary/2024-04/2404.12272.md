#### 背景
- **背景**       
    文章说明了大型语言模型（LLMs）在生成输出时会出现错误，比如幻觉现象、无视指令等，这些输出需要人为的验证。但是，验证LLMs的行为是具有挑战性的。针对这一问题，研究人员和工业开发者们创建了促进更系统测试输出的工具，例如促进工程和审查工具。

- **已有的工作**
    现有的方法要求使用指标——一系列自动为LLM输出评分的函数，每个函数通常是一个具有真或假值的断言。这些指标越来越多地包括调用“评价器”LLM（例如“裁判”），它们对代码难以描述的输出质量进行评分；例如输出的“简洁性”。但是，就像它们所评估的LLM一样，执行评估的LLM是不可信任的。这些“裁判”提示饱受与其他提示相同的问题困扰——它们对字词或结构上看似微不足道的变化非常敏感，但许多现有的系统不包括验证LLM生成评价的质量的支持，要求用户简单地信任这些输出。

#### 核心贡献
- **提出了一个称为EvalGen的混合主动式界面**
    - **挑战1：如何在保持效率的同时最小化或避免评估的不一致性？**
        为了解决评估自动化与效率之间的矛盾，EvalGen让用户在界面中生成评估准则，并实现断言。当生成候选实现（Python函数或LLM评分提示）时，它会要求用户对LLM的一部分输出进行打分，这些反馈用于选择更符合用户评分的实现。

    - **挑战2：处理标准漂移现象**
        研究中发现了被称为标准漂移的现象：用户在对输出进行评分时需要标准，但评分过程也帮助用户定义标准。一些标准似乎取决于观察到的具体LLM输出，而不是可以预先定义的独立标准。这对于假设评估与模型输出观察独立的方法提出了严峻的问题。

#### 实现与部署
EvalGen内置于现有的开源界面ChainForge中，用于促进工程和审查工具的设计。这项工作进行了定性研究，得到了EvalGen的整体支持，但也强调了实现一致性的主观性和迭代过程。研究还提出了对未来LLM评估助手设计的影响。

#### 总结
这篇论文提出了一个与人类偏好相一致的LLM辅助评估界面EvalGen，通过混合主动式方法解决了LLM生成的评估功能评估质量受信任度的问题。论文还探讨了用户如何定义和使用评估标准的动态性，以及在实际应用中所面临的挑战。