#### 背景
- **背景**       
    文章介绍了针对大型语言模型（LLMs）在静态数据集上训练的局限性，并阐述了将预训练的LLMs整合到动态数据分布、任务结构和用户偏好中所面临的非平凡挑战。一个主要的挑战在于平衡模型适应性和知识保留的关系，尤其是解决预训练LLMs在为特定需求进行微调时遇到的知识领域性能显著退化的问题，即“灾难性遗忘”现象。而持续学习(continual learning, CL)方法在LLMs领域中呈现独特的面貌。

- **已有的工作**
    现有研究通常采用重复数据收集和模型重训练来满足特定需求，这种方法过于昂贵且在实际情况中不切实际。另外，为了在保持过往知识域的性能的同时有效适应下游任务，研究者采用了持续学习的方法论。持续学习的灾难性遗忘问题一直是该领域研究的核心，尽管研究人员已经探索了不同技术来减轻机器学习模型中的遗忘问题。

#### 核心贡献
- **提出了一个对大型语言模型持续学习的综述**
    - **挑战1：垂直持续性/垂直持续学习（VCL）**
        综述中描述了LLMs如何从一般领域向特定领域的持续过渡，并探讨了健康领域等应用。这个方向涉及学习目标和执行实体的转变，并要求保持LLMs的一般推理和问答能力。

    - **挑战2：水平持续性/水平持续学习（HCL）**
        针对持续跨时间和领域的适应问题，文章综述了多阶段训练及其面临的灾难性遗忘风险，例如社交媒体平台不断更新LLMs以反映最新趋势。

#### 实现与部署
综述中详细讨论了如何评估LLMs的持续学习过程，提供了当前可用的数据源和评估协议。同时对持续学习LLMs的有趣问题进行了讨论，包括持续学习的新性质，传统增量学习类型以及对CLMs的记忆约束在LLMs持续学习背景下的角色变化，以及未来研究方向。

#### 总结
本综述为LLMs的持续学习提供了一个全面的视角，特别强调了连续预训练（CPT）和领域自适应预训练（DAP）的研究领域。强调社区需更多关注，特别是开发实用、易于获取且广泛认可的评估基准方面，以及需要针对新兴LLMs学习范式特别设计的方法论。