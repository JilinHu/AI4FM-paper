#### 背景
- **背景**       
    论文指出大型语言模型（LLMs）在处理长文本上展示出显著不足，尤其是在推理成本随着序列长度指数增长，使其在一些真实世界文本处理应用中部署成本很高，例如检索增强生成（RAG）。此外，LLMs还表现出“分心现象”，即无关的上下文质量降低输出质量。

- **已有的工作**
    以往的研究试图加速LLMs的推理过程，但这些优化常常需要对预训练模型进行重大的架构或参数修改，使得重新训练或微调过程代价昂贵。长上下文输入还可能导致模型输出出现幻觉或偏离正常回应。

#### 核心贡献
- **提出了一个名为“超级叠加提示”的方法**
    - **挑战1：处理长文本时的推理负担过重**
        超级叠加提示方法让LLMs能够并行处理输入文档中的提示路径，一旦这些路径被认为是无关的，就可以将其舍弃。通过这种方式，显著提升了时间效率，并在多个问题回答基准测试中用多个预训练的LLMs验证了该方法的能力。例如，在NaturalQuestions-Open数据集上，使用MPT-7B指令调优模型相比于传统RAG，计算时间降低了93倍，准确率提高了43%。

    - **挑战2：长文本输入导致的质量下降和幻觉**
        该方法通过修改LLMs的依赖结构，以独立处理所有文档，便于利用LLM logits修剪掉无关上下文。这不仅提高了长文本推理的能力，也因为KV缓存和logit计算的新缓存和并行机会，加快了提示处理的速度。

#### 实现与部署
该技术在无需额外训练或微调的情况下，通过新的通用提示框架提升了基于RAG的问题回答任务的模型准确率和计算时间效率。该框架针对LLMs在RAG情景下的提示采用了新的结构，使其可以独立处理各个文档并快速剔除无关内容。为了证实这些结果，作者在多个问题回答数据集上进行测试，展示了该方法在实际部署中的优势，并通过大量实验证据和解剖研究来支持设计决策的可行性。

#### 总结
该论文提出了一种新的检索增强生成（RAG）提示方法——“超级叠加提示”，用于处理大型语言模型处理长文本时遇到的问题，并在没有额外训练或微调的情况下显著提高了时间效率和准确性。这一方法在众多预训练模型上得到验证，并且作者计划发布一个开源代码实现。