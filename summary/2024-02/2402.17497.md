#### 背景
- **背景**       
    论文介绍了现有的大型语言模型（LLMs）在开放域问答（QA）任务中的挑战，包括难以处理实时性和领域特定知识点，尤其是在准确评估检索得到的文档的相关性上有所欠缺。

- **已有的工作**
    现有的大型语言模型（LLMs）在处理检索增强生成（RAG）的任务时，不能准确地评估检索文档的相关性，导致可能误用外部知识，甚至得出错误的结果。此外，现有方法中的文档相关性评估通常是基于二元标签（即相关或不相关），这种方法过于粗略，不能捕捉到细粒度的相关性。因此，现有方法未能有效地提高LLMs在QA任务中利用外部知识的能力。

#### 核心贡献
- **提出了一个REAR框架**
    - **挑战1：提升文档相关性自我意识**
        为了让LLMs在利用检索文档时更好地判断其相关性，REAR框架引入了一个特殊设计的排名头（rank head），用于精确评估检索到的文档的相关性，并采用它来捕捉相关信号，从而避免LLMs因不相关的外部知识而受到干扰。
        
    - **挑战2：解决二元判别法的局限性**
        以往的二元判别方法只能提供粗粒度的监督信号，不足以精细地判断文档的相关性。REAR框架通过设计双粒度相关性融合（bi-granularity relevance fusion）进一步利用细粒度的监督，并通过抗噪声训练（noise-resistant training）增强RAG系统的辨别能力。 
        

#### 实现与部署
REAR框架结合了模型架构和训练中的改进，通过在RAG系统中使用参考文档来提高源相关性的自我意识。在四项不同的开放域QA任务上进行的广泛实验表明，REAR框架在有效地识别检索文档的相关性方面显著优于以前的多个竞争RAG方法。

#### 总结
该论文提出了REAR框架，重点在于通过为LLMs加入文档相关性自我意识来增强其在QA任务中利用外部知识的能力，并证实该框架有效地超越了前述方法。