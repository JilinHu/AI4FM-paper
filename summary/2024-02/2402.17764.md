#### 背景
- **背景**       
    论文指出，近年来大型语言模型（LLMs）的规模和能力迅速增长，它们在自然语言处理任务中进行了显著表现。然而，模型规模的不断扩大带来了部署挑战，同时由于高能耗引发了环境和经济影响的担忧。

- **已有的工作**
    为解决这些挑战，一个办法是使用训练后量化，创建用于推断的低位模型，这能显著降低LLMs的内存和计算需求。但即便训练后量化在工业LLMs中得到了广泛应用，其依然是次优的。

#### 核心贡献
- **提出了一个xxx**
    - **挑战1：现有1位量化方案的限制**
        论文介绍了一个称为BitNet b1.58的新变体，该模型通过将每个参数量化为三元 {-1, 0, 1} 来提升原有1位量化方案的性能。这种方案减少了从DRAM到芯片上加速器（如SRAM）的内存传输成本，同时降低了内存占用，在延迟、内存吞吐量和能源消耗方面明显更有效。论文的方法通过引入额外的0值并支持特征过滤，以强化建模能力及性能改进。

    - **挑战2：支持特定硬件优化需求**
        BitNet b1.58定义了新的LLMs的训练规模定律，并为新一代高性能和经济高效的LLMs铺平道路。它不仅引入了几乎不需要矩阵乘法操作的新计算范例，而且支持设计针对1位LLMs优化的特定硬件的需求，开启了设计这类硬件的大门。
    - ...

#### 实现与部署
BitNet b1.58的核心是用1.58位的权重和8位的激活代替了nn.Linear。通过参数的absmean量化函数，将模型与几种不同规模的FP16 LLaMA LLM进行比较，结果显示从3B模型尺寸开始，BitNet b1.58在复杂度和端任务性能方面匹敌完整精度的LLaMA LLM，同时在速度上快2.71倍，使用的GPU内存少3.55倍。特别是，在3.9B模型大小上，BitNet b1.58比LLaMA LLM 3B更快，性能优越，同时内存使用更少。论文还介绍了BitNet b1.58的能源效率，指出其在进行矩阵乘法时节省了71.4倍的计算操作能源消耗。

#### 总结
论文提出BitNet b1.58模型，这是一个1.58比特量化的大型语言模型，与传统的完整精度LLMs在性能上可比，而且更高效、更节省能源。