#### 背景
- **背景**       
    论文调研了近几年兴起的生成型AI模型，这些模型能够创造新颖、有创造力的内容。尤其在语言和图像生成方面取得了突破性进展。然而，尽管有关于视频生成的早期迹象，但在级别的互动和参与度方面，视频生成模型与如ChatGPT这样的语言工具，或是更为沉浸式的体验仍存在一个差距。

- **已有的工作**
    目前存在的模型未能在视频生成方面实现与现有文本生成工具相匹敌的交互性和参与度，也没有能够生成完整的交互式体验。

#### 核心贡献
- **提出了一个名为Genie的生成型交互环境**
    - **挑战1：如何实现可控的视频生成？**
        传统的视频标注或动作标签获取成本高，数据不足。Genie通过无监督学习方式获得隐式动作，将未来帧的预测条件化于前一帧的动作。使用基于VQ-VAE的目标来限制预测动作的数量，保持动作数量小，使其具有人类可操作性并加强控制性。

    - **挑战2：如何压缩视频以实现更高质量的视频生成？**
        Genie使用VQ-VAE将视频压缩成离散的tokens，减少维数。这种方法是通过训练编码器来得到连续的隐式动作，编码器能够将所有先前的帧和隐式动作作为输入，预测下一帧。

#### 实现与部署
Genie的模型包含三个关键组件：1）一个隐动作模型（LAM）用于推断每对帧之间的隐动作；2）一种视频标记器将原始视频帧转换为离散令牌；3）一个动态模型，基于隐动作和过去的帧令牌来预测视频的下一帧。该模型利用了ST-transformer架构，并在训练时使用交叉熵损失来预测下一帧。在推断时，可以通过连续指定离散的潜在动作来控制视频生成，从而可以生成完全新的视频（或轨迹）。此外，Genie训练在一个从公共可用的互联网平台游戏视频中收集的大规模数据集上，该数据集包含了6.8M的16秒视频片段。

#### 总结
Genie是能够生成新视频并能通过用户输入控制视频内容的交互环境模型，弥补了传统视频生成技术与交互体验之间的差距。