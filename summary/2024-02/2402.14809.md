#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）对人工智能革命的巨大影响，尤其是在进行多样化任务的能力方面。这些模型特别擅长自我评价和改进，这依赖于他们的关键推理（critique-correct reasoning）能力，即批判——辨别提供回应中的问题，并纠正——提出合适的修改。

- **已有的工作**
    现有研究集中在对LLMs的批判和纠正能力进行了有限范围的模型和数据集研究，并因此产生了不一致的研究结果。这强调了系统评估LLMs批判和纠正能力势在必行。

#### 核心贡献
- **提出了一个名为CRITICBENCH的全面基准测试框架**
    - **挑战1：如何系统评估LLMs的批判和纠正推理能力**
        CRITICBENCH包括五个推理领域的15个数据集，旨在全面评估LLMs在多种任务中的批判和纠正推理能力。使用CRITICBENCH，研究人员调查了基础模型、训练策略、提示策略和oracle反馈对LLMs批判-纠正推理性能的影响。

    - **挑战2：如何揭示影响LLMs批判推理的关键因素**
        通过在CRITICBENCH上评估17个LLMs的性能，研究发现：（1）生成、批判和纠正（GQC）能力之间存在线性关系，而专注于批判的训练显著提高了性能；（2）任务类型对批判和纠正有效性有显著影响，逻辑导向的任务更容易被纠正；（3）当模型规模增加时，GQC知识的不一致性会减少；（4）模型间存在有趣的批判模式，更强大的模型在批判较弱的模型时表现得更好，而较弱的模型在自我批判中有时能超越更强的模型。

#### 实现与部署
研究使用了包含来自LLaMA、Vicuna和GPT系列的八个模型生成的回应，对17个LLMs（包括封闭源模型GPT-3.5和GPT-4，以及开源模型如Phi-2、LLaMa家族、Vicuna家族和Mistral家族）进行了广泛的实验评估。实验结果展示了在CRITICBENCH上模型的性能，并披露了模型间关于不同任务类型上的表现差异和批判纠正知识的一致性特征。

#### 总结
该论文通过CRITICBENCH评估了LLMs的批判和纠正推理能力，并探究了影响这些能力的关键因子，旨在促进LLMs批判和自我改进能力的后续研究。