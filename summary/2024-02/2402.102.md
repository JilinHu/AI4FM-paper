#### 背景
- **背景**       
    文章讨论了目前增强大型语言模型（LLMs）推理能力的研究，主要集中在特定的引导技术上，如少数样本或零样本的链式思维（Chain-of-Thought，CoT）引导。这些方法虽然有效，但往往需要大量的手动引导工程。

- **已有的工作**
    现有研究表明LLMs在推断能力上受限，主要通过手动引导和模型训练或指令微调来使用CoT数据。但是这些方法无法展现LLMs内在的、不需要外部引导的推理能力。

#### 核心贡献
- **提出了一个新的调整解码过程的方法，可以在不给予提示的情况下从LLMs中自然地引发CoT推理路径**
    - **挑战1：如何不使用提示技术就能从LLMs中引发CoT推理**
        论文通过简单地改变解码过程，使模型能够自然地产生CoT路径。研究发现，在标准的贪婪解码之外检查候选top-k标记可以揭示CoT路径。这种方法不仅避开了手动引导的复杂性，还可以评估LLMs的内在推理能力。
    - **挑战2：如何区分CoT和非CoT路径以及如何利用这一点来提高决策的可靠性**
        研究观察到，当CoT路径在解码过程中出现时，模型在解码最终答案时展现出更高的信心。因此，研究利用这一现象发展了一种称为CoT-解码的方法，它筛选出更可靠的解码路径，显著提高了在各种推理基准上贪婪解码的表现。

#### 实现与部署
在多种推理任务上的广泛实验显示，提出的CoT解码在解码过程中自动揭示CoT推理路径，显著提高了模型在贪婪解码上的推理能力。通过PaLM-2大型模型进行了实验，比较了标准的贪婪解码路径和选择不同top-k标记的其他解码路径。CoT-解码方法不仅改善了预训练LLMs的推理能力，而且还使模型在不同的推理基准测试中表现得更好。

#### 总结
这项工作揭示了通过改变解码策略，可以有效地从预训练的LLMs中自然地引发推理，并且在预训练数据中频繁出现的任务上CoT路径更常见。提出的CoT解码方法无需手动引导，就能显著提高各种推理基准上的模型性能。