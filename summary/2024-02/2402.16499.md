#### 背景
- **背景**       
    论文介绍了大型语言模型（LLMs）在动态多代理环境中的能力评估问题。多代理环境通常要求智能体具备特定的能力，如空间推理、对手建模和团队协作，而这些能力对于高效的智能体行为至关重要。

- **已有的工作**
    现有的评估方法通常限于单一智能体的评估，没有充分捕捉到大型语言模型在游戏或协作场景中的群体行为特征，且未涉及多模态动态环境下的评估。

#### 核心贡献
- **提出了一个名为LLMARENA的评估基准**
    - **挑战1：评估LLMs在动态多代理环境中的能力**
        挑战是如何量化和评估LLMs在多代理环境中各种能力的表现。LLMARENA透过7种游戏环境的深入分析，揭示了LLMs在空间推理、对手建模和团队协作方面的不足。该基准提供了一个新的平台来评估LLMs的多维能力，并能为未来的研究提供参考。

    - **挑战2：推动LLMs在动态多代理设置中的表现提升**
        研究指出，提高LLMs智能体在动态多代理环境中的表现是一个尚未解决的挑战，LLMARENA提供了一个评估工具来刺激未来研究在这方面的进步。

#### 实现与部署
根据评估结果，不同配置的LLMs在7种环境中的表现存在明显差异，展现出随着模型参数规模的增加，其能力有显著提升。然而，即便是在风险评估和团队合作方面，即便参数众多的模型（如GPT-4）也展现出独特行为，指示着这些模型在特定场景下的性能存在明显不同。

#### 总结
研究介绍了LLMARENA基准，用以评估LLMs智能体在复杂多代理环境中的能力，指出了存在的问题并促进了未来的研究方向，包括多模态动态环境中的能力及利用外部工具的潜力。