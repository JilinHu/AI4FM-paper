#### 背景
- **背景**       
    论文介绍了大型语言模型(LLM)在用户行为数据和个性化应用方面存在的挑战。由于用户交互数据的复杂性和潜在噪声，使用传统的文本提示方法存在局限性，例如对用户行为及其变化的理解不够深入，处理长时间序列数据的计算负担重。

- **已有的工作**
    现有的工作使用单一模态或多模态用户交互数据来训练用户模型和推荐系统，但是在捕捉长期依赖性及上下文关系以及处理用户历史行为数据方面仍存在挑战。

#### 核心贡献
- **提出了一个名为USER-LLM的框架**
    - **挑战1：多模态交互数据中的复杂性和噪声**
        面对用户行为数据的多维性、多模态性和稀疏性，该框架通过自监督预训练生成用户嵌入（用户embeddings），捕获用户偏好及其随时间的演变，从而降低噪声对于模型理解的干扰。

    - **挑战2：计算资源的限制**
        由于传统的LLM在处理长序列用户行为数据时计算成本过高，该框架使用横向注意力（cross-attention）和软提示（soft-prompting）将紧凑的用户嵌入与LLM整合，以此降低长序列处理的计算需求。

#### 实现与部署
USER-LLM在MovieLens、Amazon Review和Google Local Review三个公共数据集上的实验显示，相比传统的文本提示基础的上下文化方法，在需要深入理解用户和处理长序列任务的任务中，获得了显著的性能提升，同时保持了计算高效性。USER-LLM框架支持不同的编码器架构和多模态融合机制，在个性化推荐和文本生成等应用领域展现了潜力。此外，通过使用Perceiver层进一步优化了用户编码器和LLM之间的整合，提升了计算效率。USER-LLM也提供了灵活的训练策略，例如只微调用户编码器而保持LLM冻结可以有效的对LLM进行个性化调整，且优于基于LoRA的文本提示LLM调整（该调整同样保持了原始LLM权重）。

#### 总结
USER-LLM是一个通过用户嵌入来上下文化LLM的框架。它能有效地解决用户数据的复杂性和长序列处理的问题，提升了LLM在个性化应用上的效能，同时也保证了计算效率。