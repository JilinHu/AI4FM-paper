#### 背景
- **背景**       
    论文中提到，大型语言模型（LLMs）通过大规模预训练在参数中存储了大量的事实知识，这些知识可以用来回答各种问题。然而，这些事实知识是静态的，随着世界的发展可能会过时，或者在专业领域或私人领域使用LLMs时证明是不够的。为了使LLMs保持最新状态，通常会继续在新文档上进行预训练以将知识存储在参数中，从而让LLMs有效回答需要最新信息的查询。

- **已有的工作**
    在论文的第一部分中，通过使用Llama-2模型进行广泛实验，研究者们探讨了LLMs可以通过继续在新文档上进行预训练，有或没有随后的指令微调，来增加存储在其中的知识。结果发现，虽然文档的困惑度（perplexity）被最小化，但LLMs回答相关文档问题的正确率仍然有限，这一发现被称为“困惑度诅咒”（perplexity curse）。

#### 核心贡献
- **提出了一个名为预指令微调（PIT）的方法**
    - **挑战1：困惑度诅咒**
        研究人员发现，QA对在文档复杂性中整合许多事实陈述时很直接，而文档往往更加复杂和杂乱。因此，他们提出了在新文档上继续预训练之前，故意暴露LLMs于QA对，从而编码来自复杂文档的知识过程考虑到通过问题如何获取这些知识。这就是PIT，通过在LLMs中引入新的训练阶段来解决这一挑战。

    - **挑战2：持续的知识获取**
        PIT通过只在QA对上进行训练，然后在与文档相关联的QA对上进行训练，从而提高LLMs从信息密集型文档中吸收知识的能力。研究人员构建了一个名为Wiki2023的数据集，实验结果表明，PIT显著提高了LLMs吸收新文档知识的能力，在Llama-2 7B模型上将QA准确性提高了17.8%，在Llama-2 70B模型上提高了16.3%。

#### 实现与部署
PIT实质上改变了继续预训练和指令微调的标准流程，让模型首先学会如何通过问题访问知识，然后再学习如何从文档中编码知识。通过对Wiki2023数据集的广泛实验，研究人员发现PIT引导模型在学习从文档中编码知识之前，优先学习如何访问知识。这种新方法大大提高了LLMs的表现，并在跨域通用性方面也显示出前景，为进一步的推广到更广泛的文档和指令，提供了实践证据。

#### 总结
本文介绍了一种名为预指令微调（PIT）的方法，有效地提高了LLMs从文档中吸收知识的能力，解决了所谓的困惑度诅咒问题，并且在多域的知识获取中也取得了显著进展。