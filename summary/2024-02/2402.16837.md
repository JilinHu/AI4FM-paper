#### 背景
- **背景**       
    论文研究大型语言模型（LLMs）在不直接给出输入信息的情况下是否能够潜在地进行多跳推理。例如，一个复杂的提示“‘Superstition’歌手的母亲是谁”，LLMs是否能够通过其内部存储的知识推导出“‘Superstition’的歌手”是指史蒂夫·旺达，并进一步利用对史蒂夫·旺达母亲的知识来完成这一提示。

- **已有的工作**
    论文指出近期的工作已经表明，基于Transformer的大型语言模型能够在参数中存储和检索事实信息，以完成简单的提示，如“史蒂夫·旺达的母亲是谁”。但目前仍有疑问，LLMs是否能够在输入提示中没有直接给出必要信息时，检索其参数中存储的事实信息，进行潜在的多跳推理。

#### 核心贡献
- **提出了一个研究框架**
    - **挑战1：设计实验框架**
        该研究提出了一种研究框架，利用特定的提示来分析LLMs是否在进行潜在的多跳推理。研究者们通过修改提示来间接提及中间实体（bridge entity），从而测试改变是否会提高LLMs对该实体的内部回想（internal recall）。随后，测试提升的内部回想是否能使得LLMs更好地利用它们对于中间实体的知识。

    - **挑战2：度量推理效果**
        采用两个新颖的度量指标——内部实体回想得分（internal entity recall score）和一致性得分（consistency score），以作为LLMs回忆某个实体的程度的代理指标。这两个指标帮助评估各跳推理效果——第一跳的实体回想以及第二跳的知识利用。

#### 实现与部署
通过构建TWOHOPFACT数据集，该数据集基于Wikidata，包含45595个涉及52种事实组合类型的双跳提示。在实验中，研究者使用LLaMA-2模型，分别是7B、13B和70B规模。研究结果显示，存在显著证据表明有限场景下LLMs能够进行第一跳的推理，并且模型大小增加时，第一跳的推理效能同样显著提高，但第二跳的推理效能并没有顺应模型大小的提升而增强。约70%的实验中，当提示被改变以间接提及中间实体时，Transformer模型后期层次显示中间实体的回想被提升。然而，整体的推理路径并没有得到太强烈的支持，二跳推理以及完整的多跳推理表现处于中等水平，仅对于第一跳比较显著。

#### 总结
本文对LLMs是否能够进行潜在的多跳推理进行了研究，并通过实验提出了评估LLMs潜在多跳推理能力的新方法。研究提示LLMs对某些关系类型的提示有很强的多跳推理证据，但这种推理路径的运用在不同类型的提示中表现出高度的情境依赖性。