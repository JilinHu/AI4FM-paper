#### 背景
- **背景**       
    论文指出，尽管大型语言模型（LLMs）在解决各种任务方面具有强大的问题解决能力，但大多数基于LLM的智能代理只是被设计为特定任务的解决方案，而不是能够通过交互学习和发展的代理。这些特定任务的解决方案通常需要人为制定的提示（prompts），以传达任务规则和规范LLM的行为，但这种做法无法解决复杂的动态场景，如大型交互式游戏。
- **已有的工作**
    已有工作的局限在于基于LLM的代理主要依赖复杂的提示工程设计，包含详细的任务描述和行为规范，无法通过过去的经验学习和变化行为策略。此外，这些代理不能与任务场景交互，也不能像人类那样在交互中学习和调整他们的行为。

#### 核心贡献
- **提出了一个名为Agent-Pro的基于LLM的代理，它能够通过政策级反思和优化进行学习和演变**
    - **挑战1：单一行动级自我修正策略不足以应对复杂交互环境**
        使用心智理论（Theory of Mind - ToM）的启发，Agent-Pro 能够在环境中自主学习和演变，即能够自主反思过去的经验，校准关于自己和环境的信念，并在不进行参数调整的情况下优化其行为策略。

    - **挑战2：智能代理无法有效从长序列动作中推导出有效策略**
        Agent-Pro 进行政策级反思和优化，自主“微调”其信念，寻找有用的提示指令，并将其整合成新的行为策略。
  
#### 实现与部署
通过在两个游戏（Blackjack 和 Texas Hold'em）中的评估，Agent-Pro表现出色，超过了常规LLM和专业模型。结果表明，Agent-Pro能够在复杂和动态的场景中学习和演变，这也有益于众多基于LLM的应用。

#### 总结
Agent-Pro是一个新型的基于LLM的智能代理，能够通过政策级反思和优化在交互环境中学习和发展策略，解决了现有工作无法通过交互学习和适应的问题。