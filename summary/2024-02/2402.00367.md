#### 背景
- **背景**       
    文章探讨了大型语言模型（LLMs）中存在的知识差距问题，即LLMs中的知识可能存在缺失或过时，因此模型可能会在缺乏可靠知识时产生“幻觉”或偏见，而用户与之互动时这种现象则更为突出。由此，研究者提出如果模型不能对问题提供准确答案，应该让模型放弃（abstain）生成输出。

- **已有的工作**
    其他研究通过检索增强、搜索引擎集成以及多LM协作等方法扩展LLMs的知识，但这些方法仍有局限性且依赖于外部数据集，而现有的基线方法大多需要独立数据集进行训练和调参，这可能会损害模型在不同知识领域的泛化能力。

#### 核心贡献
- **提出了一个关于如何让LLMs放弃回答问题的研究**
    - **挑战1：知识缺失**
        当知识在LLMs训练数据中缺失或来源不可靠时，模型应选择放弃回答。论文提出了两种基于多LM合作的新方法(COOPERATE和COMPETE)，其中一个LLM使用其他模型来反馈建议的答案，并综合输出为放弃决策。

    - **挑战2：自我反思**
        现有方法依赖于一个LLM可以被用来评估它自己生成文本的“自我反思”假设，但幻觉和确认偏误等挑战使得这一假设站不住脚。新方法(COOPERATE和COMPETE)通过多LLMs合作来反思生成的文本，解除了对独立数据集的需求，增强了模型的放弃功能。

#### 实现与部署
在四个知识密集型问答任务上对基线方法和提出的合作方法进行评估，实验表明COOPERATE和COMPETE在9个任务中胜过所有基线方法，放弃准确性提高了最多19.3%。进一步分析表明，合作基于多LLM的方法有助于识别检索增强的失败案例，确定多跳推理中的知识差距，并有助于提高模型的可靠性，减少幻觉和偏见。

#### 总结
本文关注的是如何在大型语言模型(LLMs)中识别知识差距并在必要时放弃回答问题。研究提出了两种基于多LLM合作的新方法，通过对比实验显示它们能有效提高LLMs放弃生成低信心输出的能力。