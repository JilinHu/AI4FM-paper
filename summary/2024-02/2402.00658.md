#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）在通过逐步推理生成步骤处理复杂推理任务方面的潜力。然而，近期研究对其在推理过程中出现的幻觉和错误表示担忧，并为提高生成的推理过理的可靠性和忠实度作出了巨大努力。

- **已有的工作**
    现有工作将推理过程建模为计划过程，或者侧重于过程监督的标注。然而，基于计划的搜索过程常因频繁评估中间推理状态和广泛的探索空间而导致高延迟。此外，通过人类标注监督推理过程的成本高昂，且难以适用于LLMs的规模训练。

#### 核心贡献
- **提出了一个新的离线训练框架**
    - **挑战1：如何提高推理模型的准确性和忠实度**
        文章提出了通过收集轨迹和直接偏好优化（DPO）来学习基于规划的推理，这依赖于依据合成的过程奖励对轨迹进行排序。这种方法旨在提高模型的可靠性而不依赖于计划过程中的即时评估。

    - **挑战2：如何在缺乏教师模型或人类标注的情况下合成过程奖励**
        文章提出了合成粗粒度过程奖励的方法，使用从LLMs收集的种子轨迹，并通过直接偏好优化来学习生成可靠推理的更好策略。
  
#### 实现与部署
依据合成过程奖励收集轨迹，并通过直接偏好优化的方法进行训练，该框架无需教师模型或人类标注。研究者在两个具有挑战性的逻辑推理基准测试上验评估了该方法，结果表明所提出的方法和框架比起强基线模型有显著改进，证明了方法的有效性。

#### 总结
文章提出了一个新颖的离线训练框架，专注于改进大型语言模型在处理复杂推理任务时的可靠性和精确性，通过收集轨迹和基于结果监督的直接偏好优化，无需教师模型或人类标注。在两个逻辑推理基准测试上的结果证明了该方法的有效性。