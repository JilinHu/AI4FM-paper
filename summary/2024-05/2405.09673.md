#### 背景
- **背景**       
    文章探讨了大型语言模型 (LLMs) 在进行微调时面临的GPU内存需求问题，以及参数高效的微调方法如LoRA（Low-Rank Adaptation）如何在减少训练时内存占用的同时，尝试保持模型对原始任务的表现。

- **已有的工作**
    大多数现有研究聚焦于LoRA与全参数微调的比较，但很少有研究在具有数十亿参数的LLMs上进行。已有的评估大多使用较为粗略的基准测验，对现代LLMs的适用性不足。而对于代码和数学这样的具体领域，LoRA表现通常不如全参数微调。

#### 核心贡献
- **提出了一个针对LoRA和全参数微调的综合评估体系**
    - **挑战1：如何在保留源任务性能的同时，有效地提升目标领域的性能**
        通过对比LoRA与全参数微调在代码和数学领域的表现，发现LoRA在数学领域的性能比全参数微调更接近，但在代码领域则明显逊色。尽管如此，LoRA在保持源领域性能方面表现更好。

    - **挑战2：如何提升LoRA的参数效率**
        通过研究LoRA配置下的微调，特别是低秩扰动的选择，发现LoRA对学习率和目标模块的选择极为敏感。此外，通过对比标准正则化技术，发现LoRA提供了更强的正则化效果。

#### 实现与部署
通过在具体的编程和数学基准测试中评估，研究显示在大多数情况下LoRA的性能不足以与全参数微调相匹敌。然而，相比全参数微调，LoRA在维持源域表现方面有显著优势。此外，LoRA在输出层面提供的正则化帮助模型保持了更多样的解决方案，显示出比传统正则化方法，如dropout和权重衰减更强的正则化效果。

#### 总结
LoRA虽然在目标任务的学习效率和精确度方面通常不如全参数微调，但在保持源任务性能方面展现了更好的表现和更强的正则化能力。根据本文研究，对使用LoRA做微调时的最佳实践做出了建议，尤其注意到学习率、目标模块选择和扰动的秩。