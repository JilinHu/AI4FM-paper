#### 背景
- **背景**       
    文章介绍了现今的大型语言模型（LLMs）中长期记忆系统的缺失问题。在多数自然环境中，哺乳动物需要储存大量世界知识，并且持续融合新信息，同时避免灾难性遗忘。然而，尽管取得了显著的成就，包含检索增强生成（RAG）的LLMs仍然难以在预训练后高效、有效地融合大量新知识。

- **已有的工作**
    目前的RAG方法在帮助LLMs融合需要跨文档知识的任务上仍然存在不足，尤其是在必须对多个单独编码的段落进行信息整合的情况下。为了解决此类问题，现有系统通常使用多次检索以及LLMs生成步骤来迭代方式拼接不连贯的段落。

#### 核心贡献
- **提出了HippoRAG框架**
    - **挑战1：跨文档知识整合**
        现有方法在多文档知识整合上的不足激发了本文提出HippoRAG框架，该框架模仿人类记忆系统结构，通过LLMs来将知识库转换成无模式的知识图谱作为人造的海马体索引，利用个性化PageRank算法对查询中的关键概念在知译图谱上进行信息检索整合。

    - **挑战2：多跳检索的高效率**
        HippoRAG的个性化PageRank算法使得在单个检索步骤中就能进行类似于多跳推理的过程，并且与当前RAG方法相比有显著性能提升。相较当前的迭代检索方法，如IRCoT，HippoRAG的在线检索不仅成本更低（10至30倍），速度也更快（6至13倍），同时保持了相似甚至更好的性能。
    
#### 实现与部署
HippoRAG在MuSiQue和2WikiMultiHopQA等流行的多跳QA基准上提供了大概3到20个百分点的性能提升，并且它的在线检索过程比现有的迭代检索方法如IRCoT更便宜（10至30倍）和更快（6至13倍）。在实现上，HippoRAG还可以与IRCoT结合，进一步提供高达4%到20%的互补性能提升，甚至能够在HotpotQA这种较不复杂的多跳QA基准上取得进步。

#### 总结
HippoRAG是一个受人类记忆系统启发的新型检索框架，解决了传统LLMs在长期记忆和知识整合方面的不足。通过模拟人脑结构和运作机制，HippoRAG有效地提升了LLMs处理复杂知识整合任务的能力，并且在效率和性能上均超越现有方法。