#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）中的知识编辑问题，着重于生命周期知识编辑的挑战，即需要持续对模型进行更新以纠正过时或错误的知识。当前的编辑方法通常集中在单次或批量编辑，但在生命周期编辑场景中，因为知识遗忘和模型性能退化，它们不能很好地工作。

- **已有的工作**
    已有的编辑方法在生命周期编辑场景中因累积的参数偏差导致的性能退化或失败，以及难以同时维护编辑的效率和推断速度方面存在问题。例如，参数修改方法（修改模型参数）、额外参数添加方法（增加新的神经元）在连续编辑过程中可能导致模型性能下降。虽然基于检索的方法缓解了这些问题，但在编辑和应用检索到的知识后，仍要处理速度问题和实际复杂性。

#### 核心贡献
- **提出了一个RECIPE（RetriEval-augmented ContInuous Prompt lEarning）框架**
    - **挑战1：如何在生命周期学习中提高编辑效率和推断效率**
        RECIPE通过转换知识陈述为短小而信息丰富的连续提示符，前置于LLMs输入查询的嵌入，从而有效地细化基于知识的响应。这种方法是建立在连续提示符可以使LLMs更有效执行下游任务的先前研究基础上的。

    - **挑战2：如何动态确定是否检索到与输入查询相关的知识**
        RECIPE加入了“知识哨兵”（Knowledge Sentinel, KS），它充当中介，动态计算阈值，以确定检索库是否包含相关知识。这使用特别设计的对比学习机制，与提示符编码器共同训练，以实现检索与模型编辑的对齐。

#### 实现与部署
RECIPE在多个LLMs和编辑数据集上进行了广泛评估，包括LLaMA-2（7B）、GPT-J（6B）和GPT2-XL（1.5B）。实验结果表明，RECIPE不仅在编辑性能和抵抗LLMs性能退化方面实现了最优表现，而且在编辑和推断速度方面也具有显著优势。该方法不需要对现有LLMs进行显著更改，也减轻了累积参数偏差或添加额外参数的需要。

#### 总结
RECIPE方法通过转换知识陈述为连续提示符并结合知识哨兵来动态管理检索过程，有效提高了LLMs在生命周期学习场景中的编辑效率和推断速度，同时保持了模型整体性能。这种方法克服了以前方法的缺点，并在多个评估指标中表现出色。