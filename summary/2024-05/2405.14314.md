#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）在复杂的物理世界中，特别是多代理系统中合作任务的规划和协调是很有挑战性的。现有方法过度依赖实体验证或自我反思，在与LLMs的查询方面显得既过度也低效。

- **已有的工作**
    现有的研究主要通过两种方式来增强LLMs的规划功能：自我反思和实体验证。自我反思通过LLMs进行自我评估以改进计划生成；实体验证则使用外部环境的反馈动态再规划。但这些反馈经常是间歇性的或基于启发式设计的，缺乏针对LLMs的有效反馈机制。

#### 核心贡献
- **提出了一个闭环反馈框架Reinforced Advantage (ReAd)**
    - **挑战1：多代理设置中的有效反馈**
        现有方法难以在多代理合作环境中对单个行动的影响进行有效评估，从而导致频繁查询或过多的与物理环境交互。论文提出的ReAd框架克服了这些问题，通过捕获和利用多代理优势函数，使得LLMs可以高效地进行计划修正。

    - **挑战2：交互和查询次数过多**
        现有LLMs在实体任务中需要进行大量的交互和查询才能获得可行的联合行动方案。ReAd通过估计每个代理的优势函数并以此驱动LLMs生成最大化优势值的行动，减少了这些互动次数。

#### 实现与部署
ReAd通过基于批评网络来估计优势函数，然后使用LLMs作为优化器来生成最大化优势值的行动。论文证明了ReAd在DV-RoCoBench和Overcooked-AI这两个数据集上均能显著降低交互和查询轮次，并在成功率上超越基线模型。这展示了ReAd在多代理合作任务中有效地训练LLMs的能力。

#### 总结
本文针对多代理合作任务中LLMs的有效规划提出了ReAd框架，证明了其降低交互次数并提高成功率的能力，为LLMs在多代理系统中的应用奠定了基础。