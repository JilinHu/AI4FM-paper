#### 背景
- **背景**       
    文章介绍了大型语言模型（Large Language Models, LLMs）在自然语言处理（NLP）中的广泛应用和创新，但同时也指出了这些模型产生幻觉内容的倾向。这些不准确的生成内容被称为“幻觉”，可能会误导用户。

- **已有的工作**
    之前的工作在检测LLMs的幻觉方面存在局限，主要是因为缺乏一个细粒度的检查系统，以及一个可以在不同真实世界应用状况下精确测量幻觉的综合性评估系统。

#### 核心贡献
- **提出了一个名为REFCHECKER的框架**
    - **挑战1：细粒度幻觉检测**
        在回应中检测幻觉的精细程度不够，尤其是在面对长且复杂的回应时。研究者提出使用“claim-triplets”（知识三元组）作为检测单位，以达到高精细度的语义分割和检测，比之前的句子级和子句级检测有显著改进。

    - **挑战2：全面性基准测试**
        缺乏涵盖多任务的综合性基准测试对幻觉进行评估。REFCHECKER基准测试涵盖了三类真实世界LLM任务，并对7种LLMs的2100个回应中的11000个claim-triplets进行了人工注释，以获得高质量的评估数据。

#### 实现与部署
REFCHECKER框架包括两个主要部分：一个负责提取claim-triplets的抽取器（Extractor）和一个负责根据参考资料对每个claim-triplet进行评估的检测器（Checker）。框架的自动化程度高，可以扩展到不同任务的幻觉检测。对比早期方法，REFCHECKER在评估中提高了6.8到26.1分。该框架同时支持专有模型和开源模型。

#### 总结
REFCHECKER是一个用于检测LLMs中细粒度幻觉并进行基准测试的框架。其通过使用claim-triplets，能在细粒度上检测并验证回应中的事实一致性，显著提高了检测的精度和与人类判断的一致性。