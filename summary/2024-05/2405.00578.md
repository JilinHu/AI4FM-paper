#### 背景
- **背景**       
    本文介绍了大型语言模型（LLMs）在理解人类指令和生成高质量回答方面展现出的强大能力，以及其在诸多下游应用中的推广使用。

- **已有的工作**
    目前，LLMs广泛应用模型对齐技术避免生成无帮助甚至有害的响应。尽管如此，现有的模型对齐方法，包括通过人类注释和原则、AI辅助和在线演示的方法，依然存在缺点。这些方法常常涉及长时间的训练过程和预定义的偏好设置，这限制了模型适应线上多样化人类偏好的能力。

#### 核心贡献
- **提出了一个名为人类行为强化学习（RLHB）的新型LLM对齐框架**
    - **挑战1：战胜模型的预定义偏好和有限的适应性**
        针对LLMs的这一局限性，本文提出了RLHB，这个框架利用真实在线人类行为来对齐LLMs。RLHB采用对抗生成框架，通过训练生成器（LLM）按照期望的人类行为生成响应，并通过鉴别器验证查询、响应和人类行为的三元组是否源自线上真实环境，实现了一种积极且可持续的在线对齐。

    - **挑战2：实现并持续更新LLM与现实人类行为的对齐**
        为了更好地利用LLM理解和遵循自然语言的能力，文章选择以自然语言的形式表达人类行为，并将其作为生成指令。在模型训练过程中，生成器和鉴别器进行对抗性训练，经过训练后，生成器能够产生与给定人类行为匹配的回应。在推断阶段，已经对齐的生成器可以直接在线部署，并接受用户查询和最受欢迎的行为信号作为输入。

#### 实现与部署
在实验验证中，作者使用了人类评估和自动化评估（如GPT-4）的方法来测试RLHB的有效性。实验结果验证了该方法在对齐LLMs与在线人类行为方面的有效性。通过直接利用在线人类行为进行模型对齐，RLHB框架避免了LLMs依赖于人工注释的需求，从而可以推广应用到各种场景。RLHB还能够持续学习以跟随人类行为的更新，这得益于其多模型同时训练机制和自然语言形式的行为建模。

#### 总结
本文提出了一种新型的大型语言模型对齐框架RLHB，它通过利用真实线上人类行为创新性地对LLMs进行调整和优化，克服了现有方法中的局限性，并通过实验有效地验证了其方法。