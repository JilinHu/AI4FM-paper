#### 背景
- **背景**       
    文章介绍了目前最流行的几个LLMs评估基准，其中多数基准采用开放式生成来评估模型，这导致了在提取和对比预期答案时出现许多错误反馈（false negatives）。因而，作者提出了一个问题：是否可以将类似GSM8K和MATH这样的简答式生成基准转化为多项选择题（MC）格式，以此提高评估准确性。

- **已有的工作**
    已有的工作（如GSM8K和MATH）在提取与评估模型生成答案时，由于无法精准匹配答案格式，常常会造成错误反馈，评估方式可能导致效率低下并且结果不够稳健。

#### 核心贡献
- **提出了一个基于MC的数据集**
    - **挑战1：评估效率和准确性**
        为了解决现有评估方式造成的问题，作者构建了基于多项选择题形式（MC）的数据集——GSM-MC、MATH-MC和PythonIO，并展示了它们与原始版本（GSM8K和MATH）间的强相关性及其对选项的稳健性。作者还证明了采用MC格式在评估时可以降低多达30倍的时间消耗。

    - **挑战2：防止无效答案影响评估准确性**
        作者通过收集超过50个开源模型在GSM8K和MATH上的错误预测来构建每个问题的干扰项（distractors），并证实LLMs在GSM-MC上的性能对干扰项的选择和选项顺序非常稳定。此项研究有助于实现对LLMs评估方法的改进，避免了无效答案干扰评估的准确性。

#### 实现与部署
文中提到实验结果强调，GSM-MC、MATH-MC和PythonIO数据集与原始基准（GSM8K和MATH以及HumanEval和MBPP）之间存在强相关性，并且对选项和顺序的稳定性很高。这些数据集降低了评估大型语言模型（LLMs）所需的时间，高达30倍的效率提升，但并未牺牲评估的准确性。此外，作者还使用了多个知名的开源LLM进行了测试，以验证新提出的评估方法的有效性和鲁棒性。

#### 总结
该研究成功将常规的开放式生成问题转换为多项选择格式，以提高LLMs的评估效率和准确度。这一方法在防止无效答案的影响、提高评估效率方面取得了突破。