#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）通过链式思考（Chain-of-Thought，CoT）提示展示出在多种NLP任务中的出色推理能力。然而，这种方法的鲁棒性值得进一步研究。特别是，论文提出了一种名为“预先给出答案”的新场景，在这种场景下，LLM在开始推理之前就已经得到一个答案。这种情况可能因为恶意用户的提示注入攻击或无意中发生。

- **已有的工作**
    之前的研究已经探索了攻击CoT推理的场景，但这篇文章提出的“预先给出答案”的场景是一个显著的现实世界威胁，因为大量的训练数据来自于网络，而网络上大量的问题解答数据都是先给出答案再详细解释。

#### 核心贡献
- **提出了一个“预先给出答案”场景**
    - **挑战1：研究预先答案对CoT推理能力的影响**
        文章通过实验发现“预先给出答案”显著降低了模型的推理能力，包括各种CoT方法和广泛的数据集。需要进一步的研究提高CoT的鲁棒性。

    - **挑战2：开发减轻预先答案效果的策略**
        为了减少预先答案的影响，论文提出了问题重述和自我反思两种策略。问题重述通过重新介绍问题的陈述来重新调节模型的注意力。自我反思则是通过模型自评其输出并识别潜在的错误，这有助于模型更有效地整合推理步骤中的信息。

#### 实现与部署
在聊天GPT和GPT-4上进行的广泛实验表明，在面对预先给出的答案时，常见的CoT方法会遭遇高达62%的性能下降。论文报告了两种预先答案设置情况下GPT-4模型显示出更大的鲁棒性。此外，论文还发现Few-Shot和Zero-Shot CoT变体之间在抵御预先答案攻击方面的韧性没有显著差异。尽管自我一致性（Self-consistency, SC）增强了CoT的抵抗力，但在某些情况下仍可能存在性能下降。

#### 总结
论文研究了预先答案对LLMs推理能力的负面影响，并提出了减轻其影响的策略。实验结果表明，这些策略不能完全抵消预先答案的影响，提示需要进一步增强CoT的鲁棒性。