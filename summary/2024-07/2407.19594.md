#### 背景
- **背景**       
    现有的大型语言模型（LLMs）在很多领域的知识积累已迅速超越了人类。然而，传统上提高这些模型的性能依赖于开销巨大的人工数据。尽管最近的自我奖励机制已经显示出LLMs可以通过自我评估而不是依赖人类标记来提高性能，但现有方法主要集中在改进模型的响应而不是判断能力，这导致在迭代训练中迅速饱和。

- **已有的工作**
    现有的“自我奖励”机制让大型语言模型通过作为参与者和评判者两个角色来提高生成响应的能力。然而，这些方法没有重点提高模型作为评判者的能力，导致训练中参与者能力的提高很快就会饱和，甚至可能过拟合奖励信号，即所谓的奖励黑客。

#### 核心贡献
- **提出了一个成为Meta-Rewarding的新方法**
    - **挑战1：如何同时提高模型作为参与者和评判者的能力**
        相较于传统的自我奖励机制，Meta-Rewarding方法通过引入一个新的meta-judge角色来评估模型自己的判断。这种方法创建了基于判断的偏好对（preference pairs）以及标准评判者生成的参与者响应之间的偏好对，共同用于训练模型的下一次迭代。

    - **挑战2：解决评判过程中的长度偏差问题**
        研究人员通过在评判得分中结合响应长度信息来选择获胜响应，从而解决了评判倾向于偏好较长响应的问题，确保当得分接近时选择较短的响应。

#### 实现与部署
研究人员从Llama-3-8B-Instruct开始，进行了多轮Meta-Rewarding训练。在AlpacaEval 2上的评估中，模型的长度控制（LC）胜率从22.9%提高到39.4%，甚至超过了GPT-4-03141。研究还显示，即使加入了长度偏差改善，Meta-Rewarding方法也优于标准的“自我奖励”训练（35.5%对比39.4%），强调了meta-judge的重要性。在针对模型回答复杂和困难问题的能力的Arena-Hard基准测试上，也可以观察到类似的改进。

#### 总结
Meta-Rewarding机制通过引入一个meta-judge角色来评估模型自身的判断，改进了LLMs的自改进流程。该方法不仅提高了模型跟随指令的能力，而且打破了依赖人工数据的局限，表现出了自改进模型在无人监督下的巨大潜力。