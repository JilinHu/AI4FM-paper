#### 背景
- **背景**       
    论文提到目前的语言模型和RAG（Retrieval-Augmented Generation）系统能够处理数百万个输入标识（tokens）。然而，评估这些系统在长文本任务上的输出质量依然是一个挑战，因为像“Needle-in-a-Haystack”这样的任务缺乏复杂性。

- **已有的工作为什么解决不了**
    针对长文本任务的输出质量评估，已有的工作采用如“Needle-in-a-Haystack”任务来进行，但这种任务过于简单，未能反映最新一代大型语言模型的能力，多数先进模型能轻松达到近乎完美的表现。现存的总结性评估工作通常关注于单文档的摘要或较短的输入内容（在1000-2000个标识范围内），对于长文本而言，获取高质量摘要的成本非常高，且自动评分指标与人类判断的相关性不高。

#### 核心贡献
- **提出了一个名为“Summary of a Haystack”（SummHay）的任务**
    - **挑战1：制定评估长文本模型和RAG系统的新方法**
        SummHay任务要求系统处理一系列文档（Haystack），并生成针对特定查询的摘要。这些摘要需要识别相关见解，并准确引用源文档。通过合成数据生成方法，制定了一个可以精确得分的自动评估系统，该系统能够基于两个方面评估摘要的覆盖度和引用情况。

    - **挑战2：在两个不同领域对10个LLMs和相应的50个RAG系统进行的广泛评估**
        研究发现，SummHay对于目前的系统来说是一个未解的挑战，即使是提供了文档相关性信号的系统，其综合得分也比人类表现（56%）低10个百分点以上。在没有检索器的情况下，长文本LLMs如GPT-4o和Claude 3 Opus在SummHay上的得分低于20%。

#### 实现与部署
研究者们在对话和新闻两个领域内生成了“Haystacks”，并对10个大型语言模型（LLMs）及其相应的50个RAG系统进行了大规模评估。研究表明，SummHay任务对当前系统构成了重大挑战。结果显示，提供了预先相关性信号的系统在综合得分上比人类表现还低10个百分点以上（人类表现为56%）。在没有检索器帮助的情况下，如GPT-4o和Claude 3 Opus这类LLMs在SummHay任务上的得分不足20%。该论文还展示了SummHay可以用来研究企业级RAG系统和在处理长文本时的位置偏差问题。

#### 总结
本文提出了一种新的评价大型语言模型和RAG系统处理长文本能力的方法，即SummHay任务，并通过合成数据生成和自动评估系统两个角度，解决了长文本评估的挑战。实验结果表明，目前的系统在此任务上表现不佳，为未来系统的提升指明了方向。