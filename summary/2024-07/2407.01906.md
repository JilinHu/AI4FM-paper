#### 背景
- **背景**       
    这篇文章讨论了大型语言模型（LLM）的参数高效微调（PEFT）。在训练资源受限的情况下对LLM进行定制开发时，PEFT显得尤为重要。然而，现有的PEFT研究集中在密集结构的LLM上，对稀疏结构LLM的探索却相对不足。

- **已有的工作**
    已有的PEFT方法如低秩适应（LoRA）和P-Tuning等，虽然已经被用于密集结构的LLM，但却未能很好地适应具备Mixture-of-Experts（MoE）等稀疏架构的LLM。MoE结构在处理不同任务时可激活不同的专家组件，而现有研究缺乏针对这种结构的有效微调技术。

#### 核心贡献
- **提出了Expert-Specialized Fine-Tuning（ESFT）**
    - **挑战1：保持专家特化性**
        对于使用MoE结构的LLM，实现高效的参数微调的挑战在于如何在维持各个专家组件之间的专业化分工的同时进行微调。ESFT通过只调整与下游任务高度相关的专家组件的参数，同时冻结其他专家组件和模块的参数，避免了全参数微调中非目标任务专家的参数更新，有效保持了专业化分工。

    - **挑战2：节省计算资源**
        在保持高性能的前提下减少训练资源的消耗是进行LLM微调的另一个重大挑战。ESFT方法只调整选定专家的参数，与全参数微调相比，能够有效减少多达90%的存储空间和约30%的训练时间。

#### 实现与部署
本研究通过实验验证了ESFT在不同下游任务上的性能。实验结果表明，ESFT在维持或超越全参数微调性能的同时，更好地保持了在通用任务上的性能，并显著减少了所需的计算资源。除此之外，作者还深入研究了ESFT的工作机理，研究了专家选择过程及其优点，并通过消融实验彰显了专家相关性评分的重要性以及精细化的专家分割架构的重要性。

#### 总结
本文提出了针对稀疏架构LLM的参数高效微调方法ESFT，该方法通过只微调与下游任务最相关的专家，既保持了专家的特化性，又显著节省了计算资源。