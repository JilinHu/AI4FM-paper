#### 背景
- **背景**       
    本文指出大型语言模型（LLMs）在进行准确反馈时常表现出理性推理不足或产生幻觉内容。为了解决这些问题，一些以"Self-"为前缀的研究方法如Self-Consistency（自我一致性），Self-Improve（自我提高），和Self-Refine（自我精炼）已经被提出。这些方法的共同点在于它们涉及让LLMs评估和更新自身以减轻上述问题。然而，现有的调查主要集中于分类，而没有考虑这些工作背后的动机。

- **已有的工作**
    以往的工具和方法不能从统一的角度总结和解决LLMs在理性推理不足和产生幻觉内容方面的问题。

#### 核心贡献
- **提出了一个理论框架**
    - **挑战1：内部一致性**
        本文提出了“内部一致性”（Internal Consistency）框架，它提供了一种统一解释不足理性推理和幻觉出现的现象。内部一致性是基于采样方法评估LLMs的潜层、解码层和响应层之间的一致性。

    - **挑战2：自我反馈**
        在内部一致性框架的基础上，文章引入了一种称为“自我反馈”（Self-Feedback）的理论框架，该框架包含两个模块：自我评估（Self-Evaluation）和自我更新（Self-Update）。前者捕获内部一致性信号，后者则利用信号来增强模型的响应或模型本身。这个框架已经在许多研究中被采用。

#### 实现与部署
本论文系统地分类了这些研究，总结了相关的评价方法和基准，并对“自我反馈是否真的有效”进行了深入探讨。提出了几个关键观点，包括“内部一致性的沙漏演变”、“一致性即正确性”假设和“潜在与显式推理的悖论”。进一步勾勒出未来研究的有前途方向，作者还公开了实验代码、参考列表和统计数据。

#### 总结
本文针对大型语言模型在保持一致性和避免产生幻觉方面的问题，提出了内部一致性和自我反馈的概念。这为我们理解和改进这些模型提供了新的视角，并展望了未来的发展方向。