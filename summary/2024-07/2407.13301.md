#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）在医学诊断领域的应用带来了重大变革，但这些模型的可解释性问题仍未得到解决。为了加强LLM在医学诊断中的可解释性，研究者创建了诊断链（Chain-of-Diagnosis， CoD），该诊断链仿照医生的思维过程，提供了透明的推理路径，并输出疾病置信度分布以确保决策的透明性。

- **已有的工作**
    已有的大型语言模型LLMs在医学诊断方面存在可解释性的挑战。考虑到幻觉问题，LLMs可能会任意地作出诊断，缺乏解释性使其难以达到合理分析和伦理标准。虽然LLMs能够为它们的决策提供基本解释，但缺乏全面的过程来解释排除其他潜在疾病的原因，以及它们作出此类决定的置信程度。

#### 核心贡献
- **提出了一个Chain-of-Diagnosis (CoD)**
    - **挑战1：提升医学诊断中的可解释性**
        CoD提供了推理和决策制定的透明性，将黑箱决策转换为五个独特步骤的医生思维过程模型，为每个决策背后的思路揭开了序幕。决策透明度方面，CoD输出置信度分布，以较高的置信度表明对特定疾病的强烈置信，这允许通过置信度阈值控制LLM的决策。

    - **挑战2：合成病例数据生成**
        由于实际病例受限，创建了一种从疾病百科全书产生合成病例的方法。这种方法能够在支持各种疾病的可扩展性方面提供帮助，同时避免了患者隐私问题。通过合成病例，研究者构建了含有48,020个CoD实例的培训数据集，开发了能够诊断9,604种疾病的DiagnosisGPT模型。

#### 实现与部署
DiagnosisGPT在包括一个新创建的基准DxBench的公共诊断数据集上超越了其他LLM，并且在所有数据集上以0.55的诊断阈值实现了超过90%的准确率。DxBench是一个包含461种疾病的1,148个真实病例的诊断基准，这些病例来源于公共医患对话，经过人工验证。这些结果突出了DiagnosisGPT在置信度方面的可靠性。

#### 总结
这项工作提出了Chain-of-Diagnosis (CoD)，它是一种旨在增强疾病诊断中LLM的可解释性的方法，该方法通过合成病例结合疾病百科全书数据，有效地生成了培训数据并开发了DiagnosisGPT模型。实验表明，DiagnosisGPT在多个诊断数据集上的性能优于其他LLM。