#### 背景
- **背景**       
    文章探讨了在与基于大型语言模型（LLM）的商业聊天机器人互动中，用户个人信息泄露的问题。研究团队对真实用户与GPT模型的互动进行了细致的分析，发现个人可以识别信息（PII）和敏感信息的泄露。

- **已有的工作**
    当前的研究无法全面检测到用户与商业聊天机器人互动中的个人信息泄露，尤其是在意料之外的背景下，比如翻译和代码编辑任务中。同时，现有的PII检测系统不能充分捕捉到人们在与LMM互动时常讨论的敏感话题，例如详细的性偏好或具体的药物使用习惯。

#### 核心贡献
- **提出了一个对用户聊天机器人互动中个人泄露的细致分析**
    - **挑战1：发现PII和敏感话题**
        研究者们通过分析真实的用户聊天数据集-“WildChat”，创建了任务和敏感主题的分类体系，实现对用户查询的自动任务和敏感主题分类。

    - **挑战2：量化泄露频率和检测其可靠性**
        使用机器学习方法对从WildChat中选择的5K对话进行自动分类，通过人工注释的子集进行验证，并为了检浀PII和敏感信息的限制，发布了相关的数据注释。

#### 实现与部署
该研究使用“WildChat”数据集，这是一个包含一百万用户-GPT互动的数据集，并先对其进行了一轮的PII移除。研究者发现尽管如此，超过70％的查询依然包含某种形式的PII，而接近15%的查询提到了非PII的敏感话题。研究在揭露用户与聊天机器人互动中存在的风险方面取得了成果，尽管用户可能不知情自己的数据正在被收集并带来了风险。该研究对于聊天机器人设计者和LLM研究者都有重要含义，它呼吁设计适当的引导机制以帮助用户适当地进行互动，同时也要求聊天机器人公司提高透明度。

#### 总结
本项研究强调了在与聊天机器人的互动中，用户个人信息泄露的问题。它呈现了在这些互动中共享的敏感信息的类型，并呼吁对聊天机器人设计采取措施，以保护用户隐私并保持交流内容的适当透明度。