#### 背景
- **背景**       
    本文介绍了ChatQA 2，一个基于Llama3的模型，旨在填补开源大型语言模型（LLMs）与领先的专有模型（如GPT-4-Turbo）在长文本理解和检索增强生成（RAG）能力方面的差距。长文本理解和RAG能力对于LLMs处理大量无法在单个提示中显示的信息至关重要，并且这两项能力根据下游任务和计算预算的不同，彼此是互补的。

- **已有的工作**
    现有的RAG管道存在局限性，可能会削弱下游任务的准确性，例如：top-k分块检索会引入上下文的碎片化；小的top-k值会导致召回率较低，而较大的top-k值会向LLM引入太多不相关的上下文。然而，现有的技术中的长文本检索器可在很大程度上缓解这些问题。

#### 核心贡献
- **提出了一个新的LLM模型**
    - **挑战1：扩展上下文窗口及提升指令跟随能力**
        NVIDIA提出了一种详细连续训练方法，将Llama3-70B-base的上下文窗口从8K扩展至128K token，并采用三阶段指令调整过程来增强模型的指令跟随、RAG性能和长文本理解能力。

    - **挑战2：提升长文本理解任务和RAG基准的性能**
        通过使用长文本检索器解决RAG中的top-k上下文碎片问题，Llama3-ChatQA-2-70B模型在许多长文本理解任务中达到了与GPT-4-Turbo相当的准确性，并在RAG基准上超越了它。

#### 实现与部署
Llama3-ChatQA-2-70B的评估结果表明，在最大32K token输入范围内的基准测试中，使用RAG与直接长文本评估相比，准确性略有下降。对于超过100k的任务，RAG的结果仍然比长文本评估要好。模型还证明了在长文本情况（超过100k token）下，RAG建议用于更高的准确性和较低的推理成本，尽管即使使用最多30个top-k块，RAG可能略逊于直接长文本解决方案。此外，研究证明Llama3-ChatQA-2-70B可以实现与GPT-4-Turbo-2024-0409相当的精度水平。

#### 总结
论文提出了一个名为Llama3-ChatQA-2-70B的模型，用于桥接开源LLMs与专有模型之间的差距，具备处理最长128K token上下文的能力，并在多个基准测试上达到了与GPT-4-Turbo相当的性能。