#### 背景
- **背景**       
    论文指出，视觉数学推理作为一种基本的视觉推理能力，已经引起了大规模多模态模型（Large Multimodal Models, LMMs）社区的广泛关注。然而，现有的基准测试更多地关注以结果为导向的性能，忽略了知识获取和泛化背后的基本原理。这导致了在评估LMMs处理复杂问题时的局限性。

- **已有的工作**
    既有工作中使用链式思维（Chain of Thought, COT）和程序化思维（Program of Thought, POT）的方法尝试引导语言模型模仿人类的推理模式，但在更具挑战性的视觉数学推理场景中，这些方法可能并不能充分评估模型解码图像中视觉信息并根据文本问题进行推理的能力。

#### 核心贡献
- **提出了一个名为WE-MATH的基准测试**
    - **挑战1：综合问题分解**
        挑战在于逐层分解复合问题成为子问题，这要求按照所需的知识概念进行分类。论文针对这一挑战，通过细化问题和采集分类，创造了一个旨在探索问题解决原理的基准测试。

    - **挑战2：多维度评估**
        再一个挑战是提出一种新的四维度评估方法，包括知识不足（IK）、泛化不足（IG）、完全掌握（CM）和死记硬背（RM），用于层次化评估LMMs推理过程中的内在问题。
  
#### 实现与部署
实验结果表明，高级别LMMs（如GPT-4o）在视觉数学推理任务上表现出从IK向IG过渡的阶段性挑战，表明这类模型正朝着知识泛化阶段迈进。相比之下，其他LMMs倾向于死记硬背，它们能够正确解决涉及多个知识概念的复合问题，但却在回答子问题时失败。这表明现有的LMMs还没有达到真正的知识泛化和理解。通过WE-MATH基准进行的评估确认了LMMs的IK问题可以通过知识增强策略得到有效改善。进一步地，研究还揭示了解决步骤和问题特定性能之间存在负相关关系。

#### 总结
这篇论文创建了一个名为WE-MATH的视觉数学推理基准测试，旨在超越传统的端到端性能评估，深入探讨和评价LMMs的问题解决原理及它们的知识获取和泛化能力。通过新的多维度评估方法揭示出多模态模型在内在推理过程中的挑战，并通过实验验证了知识增强策略的有效性，推动了LMMs在视觉数学推理方面的进步。