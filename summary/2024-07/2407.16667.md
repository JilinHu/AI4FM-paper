#### 背景
- **背景**       
    对于许多现实世界场景中的应用程序，例如 Code Copilot，已经集成了先进的大型语言模型（LLMs），例如 GPT-4。这些应用程序显著地扩展了LLMs的攻击表面，使得这些模型面临多种威胁。在这些威胁中，通过精心构造的促使攻击性响应的提问（即“jailbreak prompts”）进行的越狱攻击引起了严重的安全关切。

- **已有的工作**
    现有的红队方法在模拟潜在对手情景来测试目标LLM时，不考虑LLM在不同场景（例如与代码相关的任务）中的独特漏洞。因此，很难调整jailbreak prompts以找到特定上下文的漏洞，从而缺乏效率。同时，现有方法仅限于使用少数变异操作（如同义词替换）来改进手工制作的jailbreak模板，缺乏自动化和扩展性，无法持续适应不同场景。

#### 核心贡献
- **提出了一个名为RedAgent的多代理LLM系统**
    - **挑战1：缺乏高质量的jailbreak prompts**
        以往的工作虽然能够在短时间内生成数千个提示，但这些提示经常无效。RedAgent通过抽象和建模现有攻击为一个连贯的概念“jailbreak策略”，并结合这些策略生成特定上下文的jailbreak prompts，解决了高质量jailbreak prompts的缺失问题。

    - **挑戜2：缺乏自动化和可扩展性**
        RedAgent通过在额外的记忆缓冲区中自我反思上下文反馈和红队尝试，持续学习如何利用策略实现特定上下文中更有效的jailbreak，增加了自动化和可扩展性。

#### 实现与部署
RedAgent通过多次实验展示了其在仅仅五次查询内就能jailbreak大多数黑盒LLMs，提高了现有红队方法的效率两倍。此外，RedAgent能够更有效地jailbreak定制的LLM应用程序。通过生成针对GPTs流行应用程序的特定上下文的jailbreak提示，RedAgent仅使用每个漏洞两次查询就发现了60个严重的实际应用程序漏洞，且已报告所有发现的问题，并与OpenAI和Meta交流了bug修复。结果还表明，与外部数据或工具增强的LLM应用程序比基础模型更容易受到jailbreak攻击。

#### 总结
RedAgent系统成功地通过模拟特定上下文的jailbreak策略，有效地发现并利用大型语言模型的安全漏洞，它不仅提高了红队方法的效率和自动化，也为理解和强化LLM应用程序的安全性提供了新的视角。