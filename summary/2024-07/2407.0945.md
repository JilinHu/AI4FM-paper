#### 背景
- **背景**       
    论文介绍了大型语言模型（LLMs）在处理大规模环境下的表现仍面临挑战，限制了它们在长序列上保持连贯性和准确性的能力。而人类大脑则擅长组织和检索大量的情节性体验，跨越生命周期。对此，本文提出了一种称为EM-LLM的新方法，将人类情节记忆和事件认知的关键方面整合到LLMs中。

- **已有的工作**
    现有工作通过检索增强、key-value对的检索、非重叠片段的组织和检索块的方法来解决上述挑战。但即使使用长上下文架构，现有方法在长上下文任务中的表现仍然与短上下文任务存在差异。

#### 核心贡献
- **提出了一个基于事件认知和情节记忆的新型大型语言模型**
    - **挑战1：处理长上下文信息**
        在处理长文本序列时，现有的Transformer基础架构及其相关技术面临推理能力不足和计算成本高昂的问题。EM-LLM通过引入一个高效的记忆形成过程来解决这一挑战，该过程基于模型在推理过程中的“惊奇”级别动态确定情节性事件的边界，然后通过图论度量进行精炼。

    - **挑战2：高效和人类化的记忆检索**
        记忆检索通常是耗时的，并且需要模型能够从大量信息中找到相关信息。EM-LLM使用基于相似性的检索和时间连续性机制来有效访问相关信息，这不仅效率高，而且模仿了人类的时间动态，提高了处理需要细致时间推理的复杂任务的能力。

#### 实现与部署
EM-LLM在处理长上下文任务方面的优势得到了实验证明，在LongBench数据集上与当前最先进的InfLLM模型相比，EM-LLM在各种任务上整体相对提高了4.3%，在PassageRetrieval任务上更是提高了33%。同时，该模型的事件细分与人们感知到的事件之间呈现出强相关性，为人工系统和生物对应系统之间构建了一座桥梁。这不仅使LLMs处理延伸上下文的能力有了进步，而且为探索人类记忆机制提供了一种计算框架，为人工智能与认知科学跨学科研究开启了新的途径。

#### 总结
文章通过整合人类情节记忆和事件认知到大型语言模型中，创造了一种新型结构EM-LLM，使LLMs能够处理实际无限上下文长度，同时保持计算效率。这项研究不仅改进了大型语言模型处理广泛上下文的能力，还有助于揭示人类记忆机制。