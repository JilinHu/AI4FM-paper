#### 背景
- **背景**       
    文章介绍了针对大型语言模型（LLMs）在检索增强生成任务中的能力（特别是噪声鲁棒性、负拒绝能力、信息整合能力以及反事实鲁棒性）提出了一个新的基准测试。作者指出现有的基于问答格式的评价方法可能由于模型内置的知识过多而存在偏差，进而不能准确评估LLMs在复杂真实世界情景下对外部信息的检索和应用能力。

- **已有的工作**
    尽管已有研究将基准测试应用于LLMs，但现有工作未能全面评估LLMs在检索增强生成任务中的多项能力，也无法深入探究模型面临噪声信息时的表现以及处理多重信息源的能力。

#### 核心贡献
- **提出了一个基于实际新闻文章的检索增强生成基准测试**
    - **挑战1：处理噪声信息的鲁棒性**
        文章提出的基准测试能够通过引入不同比例的负面文档（不包含答案的文档）来评估LLMs的噪声鲁棒性。其中使用了准确率来度量噪声文档比例对模型性能的影响。

    - **挑战2：处理多源信息的整合能力**
        基准测试专门为评估LLMs的信息整合能力而设计，需要模型整合多个文档中的信息来回答问题。通过精确匹配的手段来度量模型的精确度。

    - **挑战3：拒绝做出错误的答复**
        当提供的外部文档只包含噪声时，LLMs应该输出指定的内容以表示无法回答问题，用此来衡量模型的负拒绝能力。

    - **挑战4：面对外部知识错误的反事实鲁棒性**
        评估模型在给定外界知识中可能存在错误的情况下的性能表现，需要模型能够辨识出不准确的信息并提供正确回答。

#### 实现与部署
基准测试的数据建构过程包括实际新闻文章的收集、使用ChatGPT生成问题和答案以及利用搜索引擎检索外部文档。在评价时，使用准确率和拒绝率这两个指标来衡量模型在噪声鲁棒性和信息整合能力上的表现，同时检验LLMs在只有噪声信息可用时的负拒绝能力。实验结果表明，在不同的噪声比例下，模型的准确率会有所降低，这揭示了当前LLMs在处理复杂信息环境下的局限性。

#### 总结
本论文提出了一种新的基于实际新闻文章的检索增强生成基准测试，用以彻底评估大型语言模型在复杂信息环境中的多项能力，并通过实验结果展现了现有LLMs在这些方面的局限性。