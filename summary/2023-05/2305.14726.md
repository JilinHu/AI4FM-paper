#### 背景
- **背景**       
    文章介绍了在零样本（zero-shot）任务上，大型语言模型（LLMs）可以使用上下文示例来提高性能。然而，选择最佳的上下文示例具有挑战性，因为模型的性能会根据所选示例而有很大的不同。

- **已有的工作**
    文章指出，当前的方法，在选择最佳的上下文示例时的表现具有巨大的差异，并且所选的示例可能会对输出标签空间带来不利的先验偏见。已有的方法包括通过嵌入距离选择最近邻居等简单方法，以及需要训练检索模型的基于检索的方法。但是，这些方法不能在任何规模的训练数据上应用，并且只能训练小型参数高效微调（PEFT）模型。

#### 核心贡献
- **提出了一个基于交叉熵差异（CED）的上下文示例选择方法**
    - **挑战1：如何选择最佳的上下文示例**
        文章通过交叉熵差异（CED）方法来选择上下文示例，该方法基于观察到的一个现象，即示例的有效性与测试样本的困惑度呈负相关。使用参数高效微调来训练小模型并计算测试样本与每个候选上下文示例之间的交叉熵差异，用这个度量来独立评估和选择每个测试输入的上下文示例。

    - **挑战2：如何理论上解释CED的有效性**
        文章提供了对于CED有效性的理论性解释。这种方法近似于训练样本和测试样本之间梯度对齐的程度，借鉴之前有关上下文示例可能作为冻结的LLM上的“元梯度”的发现，表明与测试输入具有相似梯度的示例可以提高下游任务的性能。

#### 实现与部署
在由二分类，多项选择，抽取式问答和生成式问答组成的4个文本生成任务的8个基准测试集上评估了CED-ICD选择方法。实验结果表明，使用CED-ICD进行的下游模型性能超过了最近邻居的基线，并且可以跨越不同大小的模型，允许在小模型上进行选择，但在包括GPT-3.5在内的更大型模型上评估测试样例。

#### 总结
文章提出了一种新的基于交叉熵差异（CED）的上下文示例选择方法，并提供了理论上的解释，实现了对不同大小和类型的大型语言模型性能的提升。