#### 背景
- **背景**      
    当前大型语言模型（LLMs）经常能够生成流畅而令人信服的解释。然而与人类不同，这些模型常常在不同的输入上生成不一致的解释。例如，对于问题"Can sparrows fly?"（麻雀能飞吗？），一个LLM可能会生成解释"All birds can fly"（所有鸟类都能飞），但同时对于相关问题"Can penguins fly?"（企鹅能飞吗？）可能会回答"No"（不能）。为了允许人类能够模拟LLM在多个例子上的决策过程，解释在相关例子之间应当是一致的。

- **已有的工作**
    当前流行的方法，如监督式微调或者通过人类反馈的强化学习等，尚未清楚是否能够解决LLMs在解释一致性方面的问题。

#### 核心贡献
- **提出了一个Explanation-Consistency Finetuning方法**
    - **挑战1：提高LLMs的解释一致性**
        挑战在于如何改进LLMs，使得它们能够在相关的例子上生成更一致的解释。论文通过引入Explanation-Consistency Finetuning（EC-Finetuning）来解决这一挑战，该方法通过在仔细构造的含有一致解释的合成数据上微调LLMs来实现这一点。
     
    - **挑战2：改进LLMs在不同领域的推广能力**
        此方法还面临如何让改进后的模型能够推广到训练过程中未见过的数据集的问题。实验表明，EC-Finetuning在四个微调数据集上实现了10.0%的解释一致性相对提升，并且在七个未在微调过程中出现的分布外数据集上也实现了4.5%的相对提升。

#### 实现与部署
论文中提出的EC-Finetuning方法在实验中显示了有效性。该方法不仅提高了LLMs在相关问题上的解释一致性，而且各个数据集上都观察到了提升，特别是在MedQA-Diff上表现出更大的一致性增益。这表明EC-Finetuning也能提高LLM对原始问题更加不同的相关问题上解释的一致性。此外，一致性的提升也伴随着预测准确度的适度提升。论文还提供了一些EC-Finetuning提高解释一致性的示例。

#### 总结
EC-Finetuning方法成功地提高了LLMs生成解释的一致性，并且可以推广到未见过的数据集，表现出微调数据集上10.0%和分布外数据集上4.5%的解释一致性相对改善，同时也适度提升了预测准确度。