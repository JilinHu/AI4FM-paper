#### 背景
- **背景**       
    文章介绍了如何通过组合已有的基础模型和特定域模型来扩展大型语言模型（LLMs）的能力。

- **已有的工作**
    由于LLMs的庞大结构，对它们增加新的能力或增强现有能力既具有挑战性也十分昂贵。已有的工作依赖进一步的预训练或微调来适应新的任务，这不仅消耗大量计算资源，且涉及隐私问题和组织边界，这限制了它们的可行性。

#### 核心贡献
- **提出了一个新框架 CALM（Composition to Augment Language Models）**
    - **挑战1：如何有效整合两个独立的模型来启用新的能力**
      CALM通过在两个模型中添加少量可训练参数，跨模型进行交叉关注，使得能在不改变任何模型权重的情况下整合不同模型的能力。

    - **挑战2：在有限数据下扩展域的能力**
      通过CALM框架组合后的模型，在处理小语种翻译和编码生成方面显示出显著改进，而这些是单独的模型所没有的能力。

#### 实现与部署
CALM在多个实验中展示了其有效性。例如，在以键值映射和数学运算结合任务为例的实验中，使用CALM合成后的模型在新任务上的表现显著超过了单独的模型，实现了相对于基准模型40%的改善。此外，对于包括代码生成在内的多个任务，CALM引入的额外参数能够在不改变原有模型权重的情况下扩展模型的能力，为低资源语言翻译和算术推理任务带来了13%的绝对提升。

#### 总结
该论文提出了一个新的模型扩展框架 —— CALM，有效整合了两个大型语言模型以实现新的任务，且在多个实验中证明了其有效性。