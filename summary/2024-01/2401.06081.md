#### 背景
- **背景**       
    研究人员为了进一步提高大型语言模型（LLM）的能力，提出了有监督的微调和强化学习方法，使得LLM能够在预训练之后更好地适应特定领域和下游任务。然而，现有的RL方法大多采用实例级奖励，无法为复杂推理任务提供细粒度的监督，也无法专注于导致错误的关键token。

- **已有的工作**
    现有的强化学习方法无法实现对错误的关键token进行有效的学习和纠正，它们仅使用实例级别的奖励即对每个采样输出进行评分，而这类奖励往往不能准确提供复杂推理任务的细粒度监督。

#### 核心贡献
- **提出了一个名为RLMEC的新的RL方法**
    - **挑战1：无法提供细粒度监督**
        现有的RL方法无法提供针对复杂推理任务的细粒度监督和关注导致不正确的少数关键token。本文提出的RLMEC方法通过一个生成式模型作为奖励模型来解决此难题，该模型能够在最小编辑约束下对错误解决方案进行重写，从而产生token级别的奖励，为RL训练提供精细监督。

    - **挑战2：无法稳定训练进程**
        作者设计了基于生成式奖励模型的token级别RL目标，并加入了基于模仿的正则化来稳定RL过程，从而专注于学习错误解决方案中的关键token，减少其他不重要token的影响。

#### 实现与部署
实验结果表明，RLMEC方法在数学任务和问答任务上均展现了其有效性。作者比较了RLMEC与其他具有竞争力的SFT和RL方法，发现在使用7B和13B LLMs的复杂推理任务中，RLMEC通常表现更佳。此外，分析实验还表明RLMEC能够稳定RL训练过程，并减少LLMs采样输出中的错误步骤。

#### 总结
本论文提出了一种新的RL方法，名为RLMEC，通过生成式奖励模型和最小编辑机制，使大型语言模型在RL训练过程中实现更精细的监督和训练的稳定性。