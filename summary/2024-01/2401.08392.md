#### 背景
- **背景**       
    当前的大型语言模型（LLMs）主导的视觉智能体主要关注静态图像任务，而对动态场景的理解能力有限，这限制了它们在真实世界中的应用范围，例如，引导学生实验并识别错误。现有研究对视频模态进行研究较少，而视频能更好地反映现实世界不断变化和感知密集的本质。

- **已有的工作**
    许多现有工作都是用于特定任务的多模态系统，但因缺乏泛化能力，适用于真实世界场景的能力很有限。尽管有LLMs创造了一些进展，但大部分还是集中在静态模态的复合任务解决策略上，忽视了静态与动态模态之间的本质差异，这是朝向实现通用人工智能（AGI）的关键方面。

#### 核心贡献
- **提出了DoraemonGPT**
    - **挑战1：如何理解动态视频任务**
        引入DoraemonGPT后，通过转换输入视频内容到符号记忆，存储任务相关属性，这个结构化表示允许利用子任务工具进行时空查询和推理，产出简洁相关的中间结果。并结合插件式工具评估外部知识，解决不同领域的任务。

    - **挑战2：如何有效规划并执行任务**
        设计了一种基于蒙特卡罗树搜索（MCTS）的新颖规划器，有效地探索庞大的规划空间，不断寻找可行的解决方案，将多个解决方案汇总成改进的最终答案，处理更复杂的问题。

#### 实现与部署
DoraemonGPT在实现上包含了任务相关的符号记忆（TSM），用于将输入视频的大量内容转换成存储与任务相关属性的符号记忆。它包含基于实例和时间的两种记忆类型，并通过子任务工具对此信息进行查询。此外，DoraemonGPT能自动安排工具集来查询符号记忆、访问外部知识和调用其他实用工具。规划器使用MCTS算法来探索规划空间，返回多个可能的答案，并总结出最佳答案。通过对DoraemonGPT在动态场景中的广泛评价，展示了其处理比以往研究更复杂问题的能力。

#### 总结
DoraemonGPT是一个LLM驱动的智能体，通过符号记忆和工具集来理解并解答涉及动态视频的复杂问题。其采用了MCTS规划器优化回答的生成过程，能够在真实世界场景中处理更为复杂的任务。