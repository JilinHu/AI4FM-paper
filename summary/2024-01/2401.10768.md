#### 背景
- **背景**       
    文章指出，在人类辅助和调整对齐（alignment）后，大型语言模型（LLMs）在完成多样化任务方面表现卓越，然而，这些模型仍然可能产生与上下文或世界知识相矛盾的自信响应，这种现象被称为“幻觉”。

- **已有的工作**
    现有研究建议通过改变超出模型能力界限的对齐训练数据来减轻不一致性，但这些方法有两个主要缺陷：基础和对齐后的LLM之间的能力存在不一致，不准确反映预训练语料库和对齐训练数据之间的不一致性；鉴于现实世界任务的多样性，精确评估LLM在不同场景下的能力仍然是一个重大挑战。

#### 核心贡献
- **提出了一个名称为知识一致性对齐（KCA）的新方法**
    - **挑战1：知识不一致导致幻觉**
        文中介绍了如何通过检测训练数据中的知识不一致问题来减轻LLMs产生的幻觉。特别是，KCA方法采用了一种策略，即利用训练好的模型设计多项选择题，以全面评估LLMs对隐含知识的理解。

    - **挑战2：准确评估LLMs能力**
        该方法提出评估LLMs在处理知识一致性问题时的表现，包括使用开放书籍调优（open-book tuning）、丢弃调优（discarding tuning）以及拒绝调优（refusal tuning）等策略。

#### 实现与部署
使用包括Pythia 7B、Llama-2 7B、Mistral 7B和Llama-2 13B等不同基础模型的LLMs，跨六个基准评估了KCA方法减轻指令调整设置中的幻觉效果。评估包括普遍指令遵循、真实问题回答、搜索和检索以及临床报告生成等，使用基于指标和基于LLM的判断来确认。结果显示KCA方法一致性地和显着地降低了知识不一致性，从而降低了幻觉比例，与基线相比甚至在特定的基础LLMs和基准中超过了10分。

#### 总结
这篇论文引入了一种新颖的KCA方法，通过减少外部知识和内在知识之间的不一致性，从而减轻LLMs在校准过程中产生的幻觉。研究提供了未来研究的几个见解，尤其是KCA方法在多种场景下的出色表现，以及其简单性与有效性的结合。