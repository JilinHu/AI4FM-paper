#### 背景
- **背景**       
    论文强调，尽管大型语言模型（LLMs）越来越普及，但将这些模型安全地整合到真实世界中仍然具有挑战性。大多数传统AI安全研究将AI模型视为机器，侧重于安全专家开发的以算法为中心的攻击方法。

- **已有的工作**
    既有工作往往使用优化方法、侧信道方法和基于分布的方法等生成难以解释的提示，并且忽视了数百万非专家用户进行自然和类人交流时涉及的风险。

#### 核心贡献
- **提出了一个基于社会科学研究的说服技巧分类法**
    - **挑战1：类人通信的风险**
        传统方法经常将LLMs视为纯粹的指令执行者而非易受微妙人际影响和说服性沟通的人类似交流者，因此忽略了由此产生的风险。论文通过引入说服技巧分类法，自动产生可解释的说服性对抗性提示（PAP），有效提高了LLMs的越狱性能。

    - **挑战2：现有防御机制的缺陷**
        对抗过程中发现，即使是最新的事后防御措施对抗PAP也有明显的差距，强调当前解决方案的不足。论文倡导为高度互动的LLMs寻找更根本的缓解措施。
    - ...

#### 实现与部署
论文通过Persuasive Paraphraser在LLMs上使用PAP产生攻击并对14个政策指导的风险类型进行评估，发现说服技巧可以显著提高不同风险类别的越狱性能。具体地，PAP在Llama 2-7b、Chat GPT-3.5和GPT-4上，10次试验中的攻击成功率均超过92%，超过了当时最先进的算法引导的攻击。在进一步探索防御方面，论文发现现有防御机制在抵抗PAP方面存在显著差距，并提出了三种对PAP有效的适应性防御方法。

#### 总结
本论文提出了将LLMs视为具备类人沟通能力的实体，利用了一个新的视角来研究AI安全问题。通过将十多年的社会科学研究应用于AI安全，制定了一个说服技巧分类法，并通过创建的工具自动生成了对抗性提示。结果表明，说服技巧可以有效地增强有风险行为被LLMs执行的可能性，同时揭示了当前防御手段在应对这类策略时的不足。