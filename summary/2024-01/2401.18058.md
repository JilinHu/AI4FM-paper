#### 背景
- **背景**       
    论文介绍了大型语言模型（LLMs）在处理长文本任务（如总结、问答和编码）中的应用优势，并指出它们可能构成生活中持久对话和复杂智能代理情景的基础。现有的长上下文LLMs构建工作主要集中在扩展上下文，特别是位置编码扩展和长文本的连续训练。然而，该论文着眼于长上下文对齐，即对LLMs进行指令微调以处理较长的用户提示。

- **已有的工作**
    尽管不需要微调的方法（例如滑动窗口注意力或邻近标记压缩）能以即插即用的方式扩展LLMs的上下文长度，但它们仍然无法达到经过优化调整的方法的性能。而针对长上下文缩放的主要微调方法通常包括位置编码扩展和长序列的持续预训练。除此之外，为了确保模型可以在聊天界面中与各种用户请求进行互动，需要对模型与指令跟随数据进行对齐。长序列的引入，为数据训练方法和评估对齐带来了独特的挑战。

#### 核心贡献
- **提出了一个xxx**
    - **挑战1：缺少长指令跟随数据集**
        为了解决长指令数据集缺乏问题，论文提出了使用Self-Instruct来构建多样化的长指令跟随数据集。数据集涵盖了来自各种长上下文来源的广泛任务范围。

    - **挑战2：长上下文数据的不均匀长度分布造成训练效率低下**
        论文采用装箱和排序批处理战略来解决该问题，以加快对长度分布不均的数据进行监督微调的速度。此外，研究人员还开发了一种损失加权方法，在装箱训练期间平衡不同序列对损失的贡献。
    - ...

#### 实现与部署
在实验中，LongAlign在长上下文任务中比现有的LLMs配方表现出色，性能提高了高达30%。同时还能保持在处理短通用任务中的熟练度。论文中采用的打包和排序批处理策略可在不妥协性能的情况下将训练速度提高100%以上。此外，提出的损失加权技术可以将长上下文性能提高10%。

#### 总结
论文提出了一种新的长上下文对齐配方LongAlign，通过构建长指令数据集、采用新的训练策略并引入评估基准来提高LLMs处理长上下文的能力，且代码、数据和长对齐的模型已开源。