#### 背景
- **背景**       
    该文章介绍了当前大型语言模型（LLMs）虽然能够在多种任务中展现出色的性能，但在解决简单的决策任务时常常会失败，原因是LLMs所包含的知识和环境之间存在不一致性。LLMs可能会生成无效行动，或者不能准确地估计环境动态变化，导致无法解决特定领域的任务。

- **已有的工作**
    已有工作通过强化学习（RL）从零开始训练代理的策略，确保RL代理能与环境保持良好的一致性。但大多数RL方法从随机策略开始更新，导致样本效率低下。尽管通过初始化策略和训练期间的探索来结合先验知识可以提高样本效率，但如何有效地将LLMs的先验知识和RL方法相结合依旧是一个挑战。

#### 核心贡献
- **提出了一个在线框架TWOSOME**
    - **挑战1：如何减少无效行动的生成**
        提出使用LLMs来计算每个有效行动的联合概率，形成合理的行为策略从而解决无效行动的问题。通过该方法，可以在保留LLMs知识的同时，提供与环境相匹配的有效行动。

    - **挑战2：如何优化策略稳定性和鲁棒性**
        针对发现长动作容易导致联合概率较低的问题，提出了基于动作的token数和词数的token规范化和词规范化方法，来纠正不合理的行动分布。此外，采用参数高效微调方法（例如Low-rank Adapters，LoRA），共享同一个冻结LLM的构架来提高训练效率，并为观察和行动提示设计了高效的概要原则。

#### 实现与部署
在经典的强化学习决策环境Overcooked以及模拟家庭环境VirtualHome进行了广泛的实验评估TWOSOME的表现。实验结果显示，TWOSOME展现出比传统强化学习方法PPO和提示调整方法SayCan在所有任务上更高的样本效率和性能。TWOSOME在未见任务上显示出优越的泛化能力，并在在线PPO微调期间未让LLMs丢失其原始的能力。

#### 总结
TWOSOME框架通过强化学习来有效地将大型语言模型（LLMs）与体现环境对齐，提高了样本效率和任务泛化能力，同时保留了LLMs的原始功能。