#### 背景
- **背景**       
    文章介绍了大型语言模型（LLM）在生成看似事实性的内容时会出现“幻觉”（即产生无根据的内容）的问题。幻觉问题被认为是安全部署LLM至真实世界生产系统中的最大障碍，尤其是在医疗记录总结、客户支持对话、财务分析报告以及提供法律建议等敏感领域。文章指出，LLM的广泛采用重度依赖于解决和减轻幻觉现象。

- **已有的工作**
    已有的工作没有很好地解决LLM在生成任务中出现幻觉的问题。尽管有一些技术被提出来缓解这一问题，但缺乏一个系统化的分类和总结，以便更好地理解、比较和发展减轻幻觉的技术。

#### 核心贡献
- **提出了一个综述**
    - **挑战1：缺乏对幻觉减轻技术的系统性综述**
        作者介绍了超过32种旨在LLM中减轻幻觉的技术，并提出了一个详细的分类法来基于各种参数（如数据集使用、常见任务、反馈机制和检索器类型）对这些方法进行分类。该分类有助于区分针对解决LLM中的幻觉问题而特别设计的不同方法。

    - **挑战2：评估和验证技术**
        论文概述了多种反馈和推理技术以及它们如何通过对输出进行校验和自我提炼反馈以降低幻觉的发生，包括Prompting GPT-3, ChatProtect, Self-Reflection Methodology, Structured Comparative reasoning等，强调了这些技术的实际效果及其在减少LLM幻觉中的应用。

#### 实现与部署
通过对实验结果、自动和人工评估的全面比较，文章展示了如何通过自我校验和改进答案的反馈机制来减少幻觉，并提供了这些方法的效果证据。例如，CoVe方法能够减少基于列表的Wikidata问题和长篇文本生成中的幻觉。而Mind's Mirror方法通过将自我评估能力从LLM转移到SLM，提高了精确度和可靠性。

#### 总结
本文是对LLM幻觉减轻技术的全面综述，提出了分类框架和系统化的反馈和理由方法，并评估了这些技术的有效性和影响。