#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）作为通用代理的评估对于理解它们的能力及促进它们在实际应用中的整合至关重要，但评估过程提出了重大挑战。主要障碍是在统一框架内跨多种场景基准检测代理性能时，尤其是保持部分可观测的环境和确保多轮交互。

- **已有的工作**
    现有的评估框架大多集中在最终成功率的测量上，期间的洞察能力不足，而且未能深入理解模型的能力。现有的代理基准测试很少同时符合任务多样性、多轮交互以及部分可观测环境的评估需求。

#### 核心贡献
- **提出了一个全新的基准测试（AGENTBOARD）**
    - **挑战1：多轮交互和部分可观测的环境**
        AGENTBOARD 设计为具有多轮交互能力的 LLM 代理提供了一个详尽评估框架。比现有的以最终成功率为主要指标的方法更加系统和分析性，能够捕捉代理在任务中逐步取得的进展和细微区别。

    - **挑战2：深入理解模型功能**
        AGENTBOARD 提供了一种细粒度的进展率指标，该指标能跟踪代理详细的进步，通过交互式可视化的多面向分析工具，考查智能代理的多个维度。这不仅揭示了 LLM 代理的能力和限制，还将其性能的可解释性推向了前沿。

#### 实现与部署
为了应对这些挑战，研究团队开发了 AGENTBOARD，一个全面的基准和开源评估框架，专为 LLM 代理的分析性评估量身定制。它提供了一组多样化的9种任务和1013个示例环境，覆盖从体现AI、游戏代理到网络和工具代理的范围。每个环境都经过人工精心制作并验证，确保以统一方式具有多轮和部分可观测的特征。为每个数据样本定义或手动标注了子目标，引入了统一的进展率指标，追踪代理的详细进步。同时，AGENTBOARD 还带有一个分析性的网络面板工具箱，通过互动可视化，考察智能代理在各个维度上的能力。通过 AGENTBOARD 的评估，研究人员可以就当前 LLM 模型作为代理的能力得到清晰的视角，如 GPT-4 在众多任务中的广泛熟练程度和明显的代理能力。

#### 总结
研究人员提出了一个新的基准测试 AGENTBOARD，专门评估具有多轮交互能力的大语言模型代理，它提供了细粒度的进展率和交互式分析工具，以增进对 LLM 代理性能的深入理解。