#### 背景
- **背景**       
    论文探讨的是在代码片段和自然语言查询之间存在的模式不一致性问题以及大型语言模型（LLMs）在代码生成任务中的表现。尽管LLMs能够生成功能准确的代码，但生成的代码通常与代码库中的真实代码在风格上有显著的差异。

- **已有的工作**
    以往的研究使用生成增强检索（GAR）方法尝试通过添加样例代码来改善代码搜索效果。然而，这种方法并没有显著提高代码搜索的效率，主要是因为样例代码和真实代码之间存在重大的风格差异。同时，已有的评价指标如BLEU和ROUGE因为不考虑代码的语法和编程惯例，也不太适合衡量两段代码之间的风格差异。

#### 核心贡献
- **提出了一个简单且有效的框架 ReCo**
    - **挑战1：如何处理自然语言查询与代码风格的不匹配问题。**
        ReCo首先使用LLMs生成基于查询的样例代码，然后重写代码库中的代码以实现风格规范化。这一过程包括将代码总结为自然语言描述，然后重新生成与样例代码风格相匹配的重写代码。

    - **挑战2：如何量化代码风格的相似性**
        ReCo引入了一个新的评价指标“代码风格相似度”（CSSim）。这是第一个专门从风格角度量化两段代码之间相似性的指标。CSSim从变量命名、API调用和代码结构三个方面评价代码风格，并结合编辑距离（Edit Distance）作为更柔和的测量手段。

#### 实现与部署
论文的实验在多种程序设计语言和搜索场景中评估ReCo的效能，包括StackOverflow、CoNaLa、APPS、MBPP和MBJP等数据集。使用常用的Mean Reciprocal Rank（MRR）作为评价指标，ReCo在各种模型上（包括BM25、CodeBERT和UniXcoder等）都取得了显著提高的检索准确度，特别是在零样本设置下，和传统方法相比，ReCo显著提高了性能，对于未经过专门针对代码任务训练的检索模型也同样有效。

#### 总结
ReCo利用LLMs重写代码库中的代码，通过风格规范化显著提高了代码搜索的准确性，并通过新的评价指标CSSim量化了风格的差异，推动了代码样式标准化的研究。