#### 背景
- **背景**       
    文章介绍了在评估大型语言模型（LLMs）的输出质量时存在的挑战，因为这些评估通常要求在不同任务和场景下评估LLMs的效果，而现有的基准测试覆盖范围有限，需要大量人工注释。为了解决这一问题并提供可扩展的元评估方法，作者提出了SCALEEVAL框架，该框架利用多个通信LLM代理之间的辩论来辅助人类注释者判断评估能力最强的LLM，从而大幅减少在元评估过程中所需的大规模注释工作。

- **已有的工作**
    以前的评估方法涉及使用LLMs来评估其它LLMs生成的响应。但是，这些作为评估者的LLMs的元评估通常受限于现有基准的覆盖范围，或者需要大量的人工注释。收集这些数据集非常昂贵，因为它们需要有经验的人类专家进行精心的注释。这限制了在多变的新任务或场景中使用LLMs作为评估者的可能性，因为没有进行适当的审查，在许多情况下这些评估者本身尚不稳定。

#### 核心贡献
- **提出了一个可扩展的元评估框架SCALEEVAL**
    - **挑战1：处理昂贵的人工标注问题**
        在SCALEEVAL框架中，使用多个LLM代理之间的辩论，然后在代理无法达成一致意见时进行最少的人工监督，以帮助用户轻松应对原本需要大量人工标注的情况。该框架允许用户在应用框架时使用自己的提示和回应，并能根据用户定义的任何场景或标准进行调整，为各种评估上下文提供了灵活性和适应性。

    - **挑战2：提升LLMs作为评估者的可信度和效率**
        实验表明，所提出的方法与纯人工专家注释者进行的元评估高度相关。作者进一步评估了不同场景下各种LLMs作为评估者的可靠性和成本效益权衡，并详细研究了它们作为评估者的特定能力和限制。

#### 实现与部署
实验结果表明，SCALEEVAL 框架在与纯人工标注的元评估相关性方面表现良好。在不同场景下，评估了多款LLMs作为评估者的可靠性和成本效益，同时也审视了这些模型的具体能力和局限性。此外，作者还探讨了用于评估的提示变化对LLMs作为评估者表现的影响。SCALEEVAL 被用来与不同的评估策略进行比较，如LLM-as-a-Judge、FairEval、ChatEval，突出显示了SCALEEVAL在自定义评估标准和可扩展性方面相对于现有方法的优势。

#### 总结
SCALEEVAL 是一种新型的元评估框架，用于评估LLMs作为评估者的可靠性和效率。通过利用LLM代理间的辩论和最小化的人类监督，该框架在评估中引入灵活性和可扩展性，并在实验中显示出与纯人工评估高度一致的结果。