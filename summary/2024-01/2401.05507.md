#### 背景
- **背景**       
    论文提出了基于大型语言模型（LLM）的智能代理（代理LLM）在数据分析领域内的表现。数据分析涉及数据的审查、清理、转换和建模，以发现有用信息、推导结论并支持决策制定，这些任务需要代理LLM在自然语言和代码间的和谐能力。

- **已有的工作**
    已有研究开发了诸多LLM基础的智能代理，例如AutoGPT、BabyAGI和AgentGPT，以及用于数据分析的OpenAI的高级数据分析（ADA）代理和其他开源代理。然而，对于数据分析领域的代理评估，目前社区中缺乏一个全面的基准。

#### 核心贡献
- **提出了一个新型基准 InfiAgent-DABench**
    - **挑战1：制定通用且自动化的数据分析任务评估方法**
        提出了"DAEval"，一个包含311个数据分析问题的样本集，来自55个不同领域的CSV文件。该基准利用格式提示技术确保问题是封闭式的，可以自动评估。

    - **挑战2：创建一种有效的智能代理框架**
        InfiAgent-DABench不仅包括评估数据集，还建立了一个智能代理框架，通过这个框架可以让LLM在数据分析方面的任务中作为代理进行推理、文件交互和工具调用（例如Python代码沙箱），以便进行简化的评估。

#### 实现与部署
论文细致地描述了评估基准（DABench）的构建，从收集真实世界的CSV文件到构造相应的问题，再到通过人工评估保证问题质量的整个流程。同时，在创建LLM智能代理框架方面，也开发了一个基于Docker的本地Python代码沙箱，允许隔离的安全代码执行。此外，论文中还包括对问题难度的分类，并在"DAAgent"上应用了一个指令调整数据集，用于数据分析的专业智能代理的训练。

#### 总结
InfiAgent-DABench提供了一个新颖的评估基准，这不仅有助于衡量智能代理在数据分析任务中的性能，同时也是探索如何改进和优化LLM在这一特定领域应用的重要一步。