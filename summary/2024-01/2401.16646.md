#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）在生成连贯文本方面展现出的卓越能力，并引出了LLMs在形成连贯概率判断的能力上是否同样出色的问题。尽管LLMs有时会产生一些与事实不符的信息（例如幻觉现象），但这通常不会影响其输出的连贯性。本文着重评估LLMs在概率判断领域的连贯性，因为在概率理论规则下能对连贯性进行严格评估。

- **已有的工作为什么解决不了**
    已有的研究更多关注LLMs在预测单个词汇方面的能力和在现实世界应用中与人类互动的能力。在评估这些系统预测的事件概率时，人工智能研究主要是为了使AI生成的概率估计与真实频率更加吻合。尽管确保AI生成的概率判断准确度至关重要，但它们的连贯性同样重要。因为连贯性与有界理性代理的准确性之间存在密切的理论联系。

#### 核心贡献
- **提出了一个对LLMs进行概率判断连贯性评估的方法框架**
    - **挑战1：如何量化概率判断的不连贯性**
        通过应用一系列概率恒等式来定量评估这些判断的不连贯性。连贯的判断根据恒等式理应始终产生零值，任何偏离都可以作为LLMs概率判断不连贯性的衡量标准。这种方法与Costello和Watts (2014)提出的用于评价人类概率推理的框架相一致。此外，可以通过重复提交同一提示，考察LLMs对同一事件的概率判断的随机性（即随时间/信息的一致性）。

    - **挑战2：解释LLMs在概率判断中的人类样偏差**
        提出了一个假设：LLMs在概率判断中人类样模式的产生可能源自于它们所采用的自回归训练目标。研究人员还检查了两个解释人类概率判断的主要计算模型：噪声下的概率理论（PT+N）模型，以及贝叶斯取样器模型，并认为LLMs的反应更符合后者。这个模型认为反应是由贝叶斯推理指导的，它比PT+N模型基于概率理论与噪声失真的表征更能捕捉神经网络的行为。这一假设进一步得到了支持，通过将LLMs中的自回归过程与机器学习中最新提出的隐式贝叶斯推理机制进行比较。

#### 实现与部署
在实验中，研究人员将任务交给LLMs，让它们评估设定在2025年发生的各种未来事件的概率，并使用一系列概率恒等式来量化判断的不连贯性。 通过重复对同一事件进行查询并使用温度设置1.0（即LLMs的输出是基于模型学到的概率），研究人员发现，概率判断的平均值和方差计算在重复次数上显示出与人类概率判断中推导出的平均值-方差关系类似的倒U形图案。评估了四个最先进的LLMs，这些模型在训练数据集大小、计算资源和模型参数数量方面各不相同，但它们在概率恒等式上展现出的偏差与人类认知中的偏差相似。

#### 总结
这篇论文探讨了大型语言模型在形成概率判断方面的连贯性问题，并发现这些模型在该领域表现出的偏差与人类认知中的系统性偏差相似。通过应用概率恒等式和重复判断的方法，研究人员量化了这些判断的不连贯性。研究还提出了一个假设，即LLMs在做出概率判断时的人类样偏差可能源自它们采用的自回归训练目标，这一假设得到了以贝叶斯取样器模型和LLMs中的自回归过程之间潜在联系为基础的支持。