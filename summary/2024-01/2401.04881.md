#### 背景
- **背景**       
    文章指出大型语言模型（LLM）已经能够处理更复杂的输入类型，包括结构化文档和多模态内容。然而，现存的方法在处理任意长的输入序列时面临着计算复杂度高和内存需求大等问题。

- **已有的工作**
    现有的研究提出了使用重复状态或连续内存来在输入序列的各个块之间保留上下文。例如，用于存储注意力子层的键值（K/V）的内存，以便后续查询能够得到响应。但这些方法有限制，例如只能处理单向注意力，不适合于在预训练时使用或部分使用双向注意力的模型结构。

#### 核心贡献
- **提出了一个基于内存的转换器，通过使用驱逐策略来减小内存大小且能适应多种架构**
    - **挑战1：减少所需要的内存大小，同时不牺牲性能**
        当前方法通常需要大容量的内存来达到预期性能。论文中提出的存储驱逐策略（例如LRA和LFA）在内存插入时就进行，能够有效减少所需的内存大小。利用计算出的注意力得分作为重要性的代理，从而决定保留或驱逐内存中的键值对。

    - **挑战2：支持双向注意力并允许处理“未来”的内容**
        传统方法通常只处理来自先前或当前上下文的键值对，并未考虑“未来”的上下文。文章提出的ATTENDRE层引入了一种等待后再注意的机制，通过检索带有驱逐查询的键值内存（K/V内存）来支持模型对“未来”的键值对进行注意力计算。

#### 实现与部署
使用TriviaQA阅读理解任务作为实验评估方法，评估结果显示，即使内存大小只有128（相对于基线方法的2048），使用论文提出的策略依然可以达到与之相当的性能。此外，ATTENDRE层的引入使原模型能够处理整个长序列，并且达到了原始模型处理整个长序列的性能水平。

#### 总结
论文成功地提出了一个新的基于内存的转换器方法，通过存储驱逐策略和ATTENDRE层，有效地减少内存需求并支持双向注意力，在长序列处理上表现出与传统方法相当的性能。