#### 背景
- **背景**       
    该论文讨论了大型语言模型（LLMs），如ChatGPT，因其预训练和微调过程中获得的广泛知识，在各种应用中发挥了基础性作用。尽管如此，它们生成事实和常识错误的倾向在关键领域如医疗保健、新闻业和教育中引起了担忧，有可能误导用户。当前评估LLMs真实性的方法受到测试数据泄露或需要大量人工劳动的限制，这阻碍了高效和准确的错误检测。

- **已有的工作**
    对大型语言模型进行错误触发的现有方法存在一些需要关注的缺陷：(1)高成本：现有基准测试依赖于问题的制定和人工注释，需要大量的努力；(2)数据污染：LLM评估遭受数据污染的问题，LLMs使用的网络资源语料库可能包含了公开可用的评估数据；(3)覆盖范围有限：以前的研究方法在范围和问题类型上显示出局限性，往往只关注个别关系或受限于有限的问题语法；(4)不同的测试环境：大多数针对问答（QA）系统的测试框架专注于封闭域QA模型。

#### 核心贡献
- **提出了一个名为FactChecker的自动化测试框架**
    - **挑战1：低效的错误检测**
        传统的方法依赖于人工工作量大，而FactChecker利用自动构建的知识图谱和规则生成问题来自动定位LLMs的事实错误。

    - **挑战2：数据污染和有限的覆盖范围**
        FactChecker使用从大型知识数据库检索的事实三元组构建知识图谱，生成与这些三元组相关的问题，减少数据泄露风险并增加测试问题的多样性，以更准确地揭示LLMs的事实错误。

#### 实现与部署
FactChecker首先创建一个结构化的知识图谱，使用数据库如Wikidata的知识三元组。然后利用知识图谱，自动生成涉及单跳和多跳关系的三种类型问题（是非题、多项选择题和WH问题）。通过将LLMs的回答与知识图谱得出的预期答案进行比较，框架有效识别出潜在事实错误。对六个著名的LLMs进行的广泛测试表明，FactChecker能够在这些模型中触发多达45%的问题的事实错误。此外，还证明了FactChecker的测试用例通过上下文学习和微调可以提高LLMs的事实准确性（例如，llama-2-13b-chat的准确性从35.3%提高到68.5%）。

#### 总结
本文介绍的FactChecker提供了针对大型语言模型的事实错误自动测试新框架，通过构建知识图谱并生成测试问题，揭示并减少了模型的事实错误。