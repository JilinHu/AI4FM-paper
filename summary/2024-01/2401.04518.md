#### 背景
- **背景**       
    文章介绍了现有大型语言模型(LLMs)的发展过程中自然语言批判评价的重要作用，其有助于训练更好的模型、对模型生成进行对齐评估以及改善模型输出的缺陷。但目前存在的问题是缺乏一个自动化且有效的方法来评估这些批判的质量。

- **已有的工作为什么解决不了**
    已有的工作缺乏定量化的标准来确定批判的评分，透明性不足无法计算可比较的分数，以及在批判评估中难以把握多个概念间复杂的关系。

#### 核心贡献
- **提出了一个名为METACRITIQUE的评估框架**
    - **挑战1：定量化标准缺失**
        为了应对这个挑战，METACRITIQUE通过创造精确性和全面性的量化标准—即准确度评分和召回率评分—来评估批判的质量。这两个指标分别用来量化批判内容的准确性与覆盖的信息广度。
    
    - **挑战2：透明性不足**
        METACRITIQUE通过引入原子信息单元（AIUs）来提升评估过程的透明性。AIUs是将批判分解为最基本的不可分割的信息单元，从而减少评估过程中的歧义，并串联起每个AIU的评判来得出最终分数。
    
    - **挑战3：复杂关系理解**
        METACRITIQUE生成自然语言理由来一步一步解释每个AIU的判决，这种方式被称为Chain-of-Thought（思维链条），增强了判断的可靠性并促进了评估循环中的人类参与。

#### 实现与部署
METACRITIQUE框架通过三个步骤实现：参考生成、AIU提取和假设批判。首先，采用GPT-4生成的内容作为参考答案和批判的代理，通过精细化的指令和人工审核来改进GPT-4的指令，以更好地与人类偏好一致。其次，通过激励GPT-4来准确地提取AIUs，实现对批判的分解。再次，通过AIU水平上的二元分类任务来验证假设批判的事实性，并评估假设批判对参考批判的覆盖程度。METACRITIQUE在一系列对比实验中展示出了近人类的性能，证明了其作为有效的批判评估工具的可行性和有效性。此框架所用的代码和元评估数据集也已公开发布。

#### 总结
METACRITIQUE是首个针对自然语言批判进行评价的框架，其通过精确度和召回率的原则评估批判的质量，并实现了高度的可解释性和透明性。