#### 背景
- **背景**       
    文章讨论了大型语言模型（LLMs）在指定任务指令数据集上进行监督式微调（Supervised fine-tuning，简称 SFT）的情况，这一方法在提升零样本泛化能力方面发挥着至关重要的作用。随着需要覆盖的任务类型增多，生成高质量响应指令所需的注释工作变得越来越昂贵。

- **已有的工作**
    尽管有越来越多的努力扩展这些数据集包含的任务类型以改善LLMs的泛化，但是扩展数据集规模需要注释大量的指令和详尽的响应，这在规模上非常昂贵。尽管提出了自动注释方法以减轻人工注释者的负担，但存在的LLMs可能无法产生高质量的注释，尤其是对于特定领域的任务和数据集。

#### 核心贡献
- **提出了一个基于实验设计的框架**
    - **挑战1：如何在不进行高成本标签工作的情况下提高模型的标签效率**
        实验设计技术选择最具信息量的样本进行标记，通常最大化一些不确定性和/或多样性的概念。研究提出了评估多种现有和新颖的实验设计技术的框架，这些技术在增加标签效率方面取得显著成果，且计算开销很小。

    - **挑战2：实验设计在经验上的益处一直未能得到充分探索**
        提出了一系列显著提高SFT标签效率的实验设计技术，发展了新的分数如最大标记不确定性来量化LLMs对特定样本的不确定性，并与其作为训练数据的有用性呈正相关。

#### 实现与部署
根据论文实施的评估结果，使用实验设计框架提出的方法与通过随机采样所需的注释成本相比，可以实现相同的泛化性能，并且只需50％的注释成本。这一框架包含了许多临时设计的测试，并且在不同的注释预算上，新策略在准确性上显著优于随机采样超过2%。与之前的工作相比，这一研究首次在生成性任务上看到使用只有随机采样一半的注释预算就能达到相同泛化性能的注释成本节约。

#### 总结
这篇论文提出了一个实验设计框架，为了提高大型语言模型在监督式微调（SFT）过程中的标签效率。它展示了实验设计技术可以在维持低计算成本的同时，大幅提高标签效率，在一些任务中与随机采样相比节省了50%的注释成本。