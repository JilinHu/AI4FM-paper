#### 背景
- **背景**       
    论文认为，为了实现超人类智能代理，模型需要得到超人类的反馈，以便为训练提供足够的信号。目前的方法经常是基于人类喜好训练奖励模型，这可能受制于人类性能水平，这些独立的固化奖励模型在LLM训练过程中无法进一步学习提高。

- **已有的工作**
    现有方法因为受限于人类喜好数据的大小和质量，以及用这些数据训练出的奖励模型的质量，从而成为了性能的瓶颈。即使最近有直接使用人类喜好训练LLM的替代方法，如直接偏好优化（DPO），也会受到类似瓶颈的限制。

#### 核心贡献
- **提出了一个自奖励语言模型**
    - **挑战1：如何避免人类喜好数据的瓶颈**
        论文提出自我改进的奖励模型概念，该模型不会固化，而是在LLM校准过程中不断更新。研究人员通过发展一个同时具备训练期间所需全部能力的代理来实现这一点，避免了将能力分散到如奖励模型和语言模型等独立模型中。

    - **挑战2：改善指令遵循能力和奖励建模能力**
        使用迭代DPO框架训练“自奖励语言模型”，它们除了能够遵循指令，生成给定提示的响应外，还能生成并评估新的指令遵循示例以添加到自己的训练集中。实验表明模型不仅指令遵循性能得到提升，重要的是奖励建模能力也有所提高。

#### 实现与部署
研究团队进行了一系列实验，发现添加了评价精细化训练（EFT）的模型不会影响指令遵循的性能。他们发现迭代过程中，在头对头的测试中第二次迭代的模型（M2）比第一次迭代（M1）有显著的性能提升。第三次迭代（M3）与第二次迭代相比在头对头评估中也有所提升。此外，模型在AlpacaEval 2.0排行榜上的性能也表现良好，连续的训练迭代产生了提升的赢率，并且第三次迭代的模型在包括GPT-4 Turbo在内的多个模型中获得了高赢率。

#### 总结
本文提出了自奖励语言模型（Self-Rewarding Language Models），旨在通过自我训练来避免人类偏好数据的瓶颈，并提高模型的自奖励和执行指令的能力。实验结果表明，该模型表现出色，有望成为连续自我改进模型的开山之作。