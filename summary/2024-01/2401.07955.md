#### 背景
- **背景**       
    文章提出了在多项选择题(MCQ)任务中，大型语言模型（LLMs）面临的限制问题。

- **已有的工作**
    已有的研究未能充分解决LLMs在MCQ任务上正确回答问题、不依赖于选项顺序和理解任务需求的问题。

#### 核心贡献
- **提出了一项对26个开源模型进行MCQ表现测试**
    - **挑战1：模型对MCQ的理解不足**
        相关研究表明，大多数模型不能很好地理解MCQ任务。论文设计了实验，结果指出超过65%的模型实际上在解决MCQ中表现不佳，并且很难理解任务要求。论文提出通过随机化选项顺序来排除模型因为选项排序而偏向特定答案的问题，从而得到更真实的模型能力。

    - **挑战2：模型选择的回答受选项顺序影响**
        论文通过实验确定了超过70%的模型回答依赖于选项的顺序而非选项本身，这会使得答案不可接受。论文使用了各种方法来提取模型答案，以确保评估尽可能准确和公正。

#### 实现与部署
论文测试了26个不同的开源大型语言模型在MCQ任务上的表现。选择的模型包括了基于OpenLLM排行榜的小型效果最佳模型和训练数据集最多的模型。测试包括了15个指令调整型模型、10个基础模型和1个强化学习调整模型。为了避免因样本不足而产生偏差，论文确保每个类别和每个字母响应至少有1个样本。通过随机化选项顺序实验排除模型对选项顺序的偏见。文中提出了三种评估方法：解析文本响应、使用对数偏置和使用概率。其中文本解析响应方法的准确率高达95%，该方法从模型生成的文本中提取单个字母答案。

#### 总结
该论文针对LLMs在MCQ任务中的限制进行了研究，指出多数模型在此类任务中表现不佳。论文还发现模型的回答往往依赖于选项顺序，并提出了有效的评估方法来排除这些偏见。论文推荐在使用MCQ评估LLMs时要格外小心，并测试模型是否真正理解了任务。