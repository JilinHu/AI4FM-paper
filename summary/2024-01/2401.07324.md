#### 背景
- **背景**       
    论文提出了大型语言模型（LLM）在进行任务规划、内存管理、工具调用和结果概括等复杂任务方面，现有单一训练LLM的方法尤其针对小型模型存在性能限制。更新工具后，可能需要对整个LLM进行重新训练，这进一步增加了复杂性。

- **已有的工作**
    现有的解决方案通过训练单一的LLM来融合上述能力，但在小型开源LLM上观察到了显著的性能限制，并且当工具更新频繁时，整个LLM可能需要重新训练。

#### 核心贡献
- **提出了一个多LLM框架α-UMi**
    - **挑战1：模块化多功能性**
        论文的方法通过创建一个分为计划者（planner）、调用者（caller）和总结者（summarizer）的框架来解决这一挑战。这些由单一LLM实现的组件分别关注特定功能，并协同合作以完成任务，为每个能力提供增强的表现，并允许根据需要更新单个组件。
    
    - **挑战2：有效的训练**
        论文介绍了一种新型的两阶段微调策略，首先在整个数据集上微调一个LLM背骨网络以增强对任务的全面理解，然后使用这个微调的LLM来分别实例化计划者、调用者和总结者。通过在各自的子任务数据集上进行连续微调来进一步提升性能。

#### 实现与部署
在多个工具使用和程序辅助数学推理的代理基准测试中进行评估，实验结果表明，提出的多LLM框架在不同模型和数据规模上均优于传统的单LLM方法。此外，研究还展示了两阶段训练策略对于框架成功至关重要，并深入分析了改进性能背后的原因。

#### 总结
研究表明小型LLM在作为工具学习者方面较为薄弱，通过引入α-UMi多LLM框架来构建性能更优的LLM代理，提出了必要的双阶段微调策略，并深入分析了数据规模法则。