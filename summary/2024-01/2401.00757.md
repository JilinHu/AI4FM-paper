#### 背景
- **背景**       
    论文讨论了大型语言模型（LLMs）在各种任务上取得的突破，如写作辅助、代码生成和机器翻译。特别是如ChatGPT这样的先进LLMs展示了它们的“推理”能力。但是，现有的评估大多侧重于LLMs在下游任务上的准确性，而不是直接评估它们的推理过程。因此，衡量LLMs的推理能力依然是一个挑战。

- **已有的工作**
    已有的努力主要集中在开发用于评估LLMs推理的基准和指标，但这些尝试通常存在数据泄漏或覆盖范围有限的问题。

#### 核心贡献
- **提出了一个名为LogicAsker的自动方法**
    - **挑战1：如何评估和提高LLMs的逻辑推理能力**
        LogicAsker为LLMs提供了一套基于命题和谓词逻辑的原子推理技能全面评估和改进方法，并揭示了LLMs没有学习好的逻辑规则。

    - **挑战2：如何在不同的LLMs中找到逻辑推理失败的测试用例**
        LogicAsker生成的测试用例能够在不同LLMs中发现逻辑推理失败的比例为25%至94%。此外，LogicAsker的测试用例可以用于设计上下文中学习的示例，有效地提高LLMs的逻辑推理能力。

#### 实现与部署
在包括GPT-3、ChatGPT、GPT-4、Bard、Vicuna和Guanaco在内的六个广泛部署的LLMs上进行评估，测试结果显示，通过LogicAsker生成的测试用例能够在不同的LLMs中发现逻辑推理失败的比例为25%至94%。此外，这些测试用例可以进一步用于设计上下文学习的示例，这种方式对于提高如GPT-4的LLMs逻辑推理能力十分有效，例如改进了10%。据作者所知，该项工作是首个基于测试结果创造提示的方法，有效地改进了LLMs的形式推理能力。

#### 总结
本论文针对LLMs的逻辑推理能力的评估和改进问题，提出了一个名为LogicAsker的方法，能够全面评估LLMs的推理能力，并通过问题生成和上下文学习有效提升这些能力。