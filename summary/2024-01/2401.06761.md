#### 背景
- **背景**       
    文章介绍了大语言模型（LLMs）对于高效部署策略的迫切需求，以及传统的自回归（AR）解码过程所面临的挑战，尤其是在达到低延迟和高吞吐率方面的困难。

- **已有的工作**
    已有的自回归解码过程存在速度不足，这是因为每个token的生成都依赖于前面全部生成过的tokens，导致过程可能受内存限制而无法充分利用GPU计算资源。同时，Transformer中对所有前置tokens的注意力计算也限制了服务的吞吐量，特别是在响应长度较长时。

#### 核心贡献
- **提出了一个自动并行自回归解码的方法（APAR）**
    - **挑战1：解码速度慢，GPU计算资源利用率低**
        APAR通过在通用领域数据上进行指令微调，让LLMs能自主计划生成过程并执行自动并行自回归生成，显著减少了生成步骤的数量，从而提升了解码速度，并且与现有的推理加速方法相兼容，相较于传统的自回归解码，APAR实现了高达2-4倍的速度提升。

    - **挑战2：高吞吐率场景下的延迟和缓存消耗问题**
        APAR通过降低关键值缓存使用和减少生成期间的注意力计算，实现了在高吞吐率场景下相较于先进框架的20-70%吞吐量增加和20-35%延迟降低。
  
#### 实现与部署
据介绍，APAR的并行解码策略利用了LLMs内在的并行化结构，并且通过在具有层次结构的语料上进行微调，使模型学会在遇到可并行化的响应结构时自主启动并行生成线程。这种方法将常规的线性生成转变成了一个可并行化的段落树结构，不仅增加了解码并行性，也减少了注意力范围，使得消耗的关键值缓存内存可以提前释放。在实验中展示，在内存受限的场景下APAR可减少模型延迟，平均生成速度提升2倍；而与预测性解码策略结合时，速度提升可达4倍甚至6倍。生成质量方面，APAR没有妥协，多个类别的评估显示其响应质量与AR模型相比保持一致，变化区间在±2%之内。

#### 总结
通过实施APAR，该研究成功提高了LLMs在内存受限场景和高吞吐率场景下的解码效率和生成速度，同时保持了生成质量，为大语言模型的部署提供了一种新的高效策略。