#### 背景
- **背景**       
    论文指出，现有的大型语言模型（LLMs）在处理推理任务时候往往没有很好地将推理过程与具体情境相结合，也就是所谓的推理过程没有很好地在上下文中找到根据。比如，采用Chain of Thought（CoT）prompting的方法在进行推理时，没有针对性地依据给定的上下文，有时会自行假设或想象一些与问题部分或完全不相关的假设，从而导致错误。

- **已有的工作**
    已有的工作无法很好解决这个问题，导致生成的答案常常会发生理由不足或错误的推理的现象。在论文分析的案例中，有高达23%的wh问题和25%的yes/no问题的错误答案可以被归类于没有基于情境的推理。

#### 核心贡献
- **提出了一个Evidence to Generate (E2G) Prompting**
    - **挑战1：链式推理不结实** 
        即使是SOTA的LLMs在使用CoT提示生成答案和推理时，往往会产生一些无法从情境中验证的、基于假想的解释。论文为此引入了E2G框架，该框架通过提示模型在生成答案的同时提供来自上下文的证据和解释，以期通过明确的证据来显著减少这种错误率。从而实现在构建答案时，将理由明确地植根于情境之中，减少臆测和不真实声明。

    - **挑战2：任务复杂性**
        一些推理任务本质上是复杂的，可能涉及到多步骤或多个不同信息源。E2G通过一个两步骤提示来简化任务复杂度，在第一步中，要求模型生成输出答案的证据或理由，以减少错误; 在第二步，模型使用来自第一步的证据作为新的上下文，从而获得最终答案。这种方式有助于减少处理信息时的复杂性，并兼顾任务的精简和实用。

#### 实现与部署
研究者在多个具有复杂情境的语言任务上评估了E2G，涉及八种不同的推理任务，使用了ChatGPT、GPT-4和PALM（540B）作为底层模型进行实验。针对不同任务，研究者提出了E2G的两个变种，即E2G-base和E2G-Pro，并根据任务情境的大小调整使用这两种方法。在DROP数据集上的评估显示，E2G的表现优于CoT，在EM（Exact Match）和F1分数上有明显提升。这一实验结果验证了E2G方法在改善基于情境的推理能力方面的有效性。

#### 总结
本论文提出了一个新的、用于改善LLMs在上下文推理能力的单代理双步提示框架——Evidence to Generate (E2G)。通过要求LLMs在生成答案的同时提供证据与解释，E2G能够减少错误推理并提高模型在处理各种推理任务时的准确度。实验结果表明，E2G方法在多个情境密集型语言任务中表现出较CoT更好的性能。