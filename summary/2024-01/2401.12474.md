#### 背景
- **背景**       
    文章指出，大型语言模型（LLMs）在理解意图、遵循指令和解决各种应用领域的任务方面展示了前所未有的熟练程度。然而，作为通用任务助理，LLMs通常与人类交谈者不同，缺乏经验事件和情感，因此在促进与用户的吸引和广泛对话方面面临限制。为了将情感价值注入用户互动中，角色扮演LLMs使用户能够为其首选角色定义并创建个人资料。现有工作通常是通过使用较弱的开源模型模仿专有模型（如GPT-4）的角色扮演能力，尽管GPT-4已经展示了出色的角色扮演能力。

- **已有的工作**
    现有研究通常假设存在更高效的角色扮演模型，并依赖手动标注大量数据集来构建这些模型。模仿模型虽然能够复制GPT-4的风格，但在复制事实性方面存在不足，增加了幻觉作为权衡，并受到OpenAI使用条款的限制。

#### 核心贡献
- **提出了一个自对齐方法DITTO**
    - **挑战1：如何提高LLMs在角色扮演能力**
        DITTO利用在巨大训练语料库中内嵌的角色知识，鼓励遵循指令的LLM模拟角色扮演对话作为阅读理解的变体。这个方法创建了一个包含4000个角色的角色扮演训练集，其规模是目前可用数据集角色数量的十倍。

    - **挑战2：如何设计一个有效且可复制的角色扮演评估方法**
        设计了一种客观的角色扮演评估，此评估注重一致的角色身份、准确的角色相关知识和认知边界。与手动注释相比，此评估方法具有可复制性、可解释性和效率性。 

#### 实现与部署
使用自动生成的数据集对LLM进行微调，以增强其角色扮演能力。在精心构建的可复制角色扮演基准和MT-Bench的角色扮演子集上进行评估，DITTO在各种参数规模下始终保持一致的角色身份，并在多回合角色扮演对话中提供准确的角色特定知识。值得注意的是，它超越了所有开源角色扮演基线，并展示了与高级专有聊天机器人相当的性能水平。同时，通过扩展自对齐设置进行跨监督的全面分析，实验表明，在强到弱的设置中，角色扮演中的知识受到LLMs固有能力的限制，而在弱到强的泛化上，知识相关指标表现稳定。这些观察结果深刻且坚实地理解了LLM角色扮演和对齐，表明种子LLM的知识和适当的示范，如DITTO的模拟数据，是令人印象深刻的角色扮演能力的关键。

#### 总结
论文提出了一种名为DITTO的自对齐方法，能够通过知识增强和对话模拟增强LLMs的角色扮演能力。此外，它提供了一种客观、可复制、可解释且高效的角色扮演评估方法，并通过跨监督的实验了解角色扮演的分解，为LLMs构建角色扮演功能提供了深入的理解和见解。