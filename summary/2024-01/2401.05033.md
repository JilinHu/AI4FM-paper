#### 背景
- **背景**       
    论文介绍了大型语言模型（LLMs）作为对话代理的能力，以及将其专门用于执行特定功能的挑战。现有方法通过对模型进行指导性的微调指令和示例响应的方式有效果，但这需要大量数据样本和成本。在对话中追随特定的工作流而非单独的指令时，成本会进一步增加。

- **已有的工作**
    已有工作依赖现有或新建数据集来适应新任务，这可能会对时间和资源造成限制。自游戏中的强化学习（RL）自我对弈已被证明是一种有效的引导策略。同时，越来越大的LLMs对话能力的提高与其理论化模仿单个用户或代理组的能力有着密切关联。但尽管存在有关利用LLMs自身产生数据进行迭代性改进的研究，这些方法仍涉及大量人力监督和参与。

#### 核心贡献
- **提出了一个通过LLMs自我对话生成训练数据的新方法**
    - **挑战1：自动化和过滤对话数据质量的方法**
        论文提出了一个结构化的提示方法和对话过滤机制来提高自生成对话数据的质量，并且通过自动化和人工评估对话质量进行评测。

    - **挑战2：如何确保模型遵循特定的工作流**
        为此，作者引入了一个定向图表示的结构化提示系统，该系统能够指导模型根据预定工作流生成和续导对话。

#### 实现与部署
作者采用了MosaicML NLP团队的30亿参数客户端模型和7亿参数代理端模型进行实验，通过在特定角色扮演中提示这两个模型，使其相互对话，并记录为潜在的训练样本。实验中采用了不同的过滤器来选择对话进行微调，并通过自动化和人工评价来评估对话的质量。结果显示，虽然仅使用随机筛选对话的基线方法可能会略微提升性能，但表现最好的过滤器是选择至少完成了5个工作流步骤或完成度最高的5％对话。此外，自生成数据的其他方法则体现在对话多样性的减少上。

#### 总结
论文提出了一种基于大型语言模型（LLMs）自我对话生成训练数据的新方法，该方法有潜力改进任务导向对话代理的性能。尽管存在一些限制，研究结果表明，当选择高质量对话作为训练数据时，可以有效提高模型的性能。这证明了在正确的设置下，通过自我生成数据进行微调的语言模型确实有潜力实现自我改进，并成为更好的任务导向对话代理。