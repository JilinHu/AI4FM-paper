#### 背景
- **背景**       
    文章介绍了检验大型语言模型（LLMs）隐藏表示中编码的信息如何帮助解释模型的行为以及验证它们与人类价值观的一致性的重要性。由于LLMs能够生成人类可以理解的文本，研究者提议利用模型本身来用自然语言解释其内部表示。

- **已有的工作**
    之前的研究提出了多种解释工具，这些方法主要依赖于三种方法：在隐藏表征之上训练叫做探针的线性分类器、将表示投影到模型的词汇空间和在计算中干预以识别特定预测所依赖的表征。尽管这些方法广泛成功，但它们存在实际中的不足，例如不能检查早期层次、缺乏表达性等。

#### 核心贡献
- **提出了一个叫做Patchscopes的框架**
    - **挑战1：提高解释方法的表达性和可扩展性**
        利用LLMs产生类人文本的先进能力，Patchscopes框架可配置用于从LLMs表示中查询各种信息。通过"patching"将特定表示译为独立于原始上下文的信息，从而实现具体信息的解码。

    - **挑战2：统一现有的解释技术并扩展新的应用**
        Patchscopes能够作为现有解释方法特殊实例的超集，通过新的配置，它可以更有效地解决相同的问题并克服以前方法的限制。此外，它还开辟了使用更强大模型来解释较小模型的表示和实现多段推理自我纠正等新的应用的可能性。

#### 实现与部署
Patchscopes针对自回归型LLMs进行了一系列实验。首先，研究者考虑了从模型的中间表示来估计模型的下一令牌预测的问题，并展示了使用少量样本的令牌身份提示能比词汇投影方法有重大提升。接下来，评估Patchscopes如何在与原始上下文分离后从LLM表示中解码实体的特定属性。实验表明，尽管没有使用训练数据，Patchscopes在六个中的六个常识和事实推理任务中表现得比探针法要优越，并且在剩余六个中的五个中表现得相当好。此外，Patchscopes能够解决现有方法难以回答的问题，例如，评估早期层次中LLMs如何对输入实体名称进行上下文化处理，以及一个更表现力强的模型如何被用来检查一个较小模型的隐藏表示。最后，示范了Patchscopes在实际中的应用性，例如用于修正当模型能够正确进行每一步推理但无法在上下文中处理它们的连接时的潜在多步推理错误。使用Patchscope后，准确度从19.57%的基准提高到了50%。

#### 总结
该论文提出了一个名为Patchscopes的框架，提供了一种新的方法去解释大型语言模型（LLMs）隐藏表示中编码的信息，并且能够纠正多步推理错误。Patchscopes作为一种通用的可配置框架，不仅统一了现有的解释工具，并解决了它们自身的一些不足，同时也开辟了新的研究和应用可能性。