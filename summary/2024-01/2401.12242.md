#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）在处理需要系统推理过程的任务时能从链式推理（COT）提示中获益。同时，也指出了COT提示带来的新的安全漏洞，即在特定的后门触发条件下会让模型在推断时输出恶意内容。

- **已有的工作**
    已有研究不足在于它们大多通过在受害模型的训练集中加入含有触发器的实例或在部署期间通过微调或“手动制作”来操纵模型参数来发起后门攻击。这些方法对于仅通过API访问操作的商业LLMs来说并不切实际。

#### 核心贡献
- **提出了一个名为BadChain的后门攻击方法**
    - **挑战1：如何在不访问训练数据集或模型参数的情况下对LLMs进行后门攻击**
        BadChain通过插入一个后门推理步骤进入LLMs的推理步骤序列，从而在查询提示中存在后门触发器时改变最终响应。在COT提示中操纵一部分示范来加入后门推理步骤。这样，任何含有后门触发器的查询提示都会误导LLM输出非预期内容。

    - **挑战2：有效性和防御**
        该研究实证表明，BadChain在两种COT策略、四种LLMs（Llama2, GPT-3.5, PaLM2, 和GPT-4）和六项复杂基准任务（包括算术常识和符号推理）上的有效性。演示了传统后门攻击在这些复杂任务上失败的同时，说明了更强推理能力的LLMs更容易受到BadChain的攻击。提出了两种基于洗牌的防御方法，但证明了这些防御对BadChain大体上无效。

#### 实现与部署
在多种LLMs和复杂任务上通过实验测试了BadChain的有效性，强调了对这类攻击防御的迫切需求。除了提出和评估攻击方法，本文还探讨了潜在的防御策略，但结果表明，这些策略对BadChain无效，表明LLMs依旧面临严重的安全威胁。

#### 总结
本文提出了BadChain，这是一种针对采用COT提示的LLMs的后门攻击，不仅不需要访问训练数据集或模型参数，而且计算开销低。该方法有效地揭示了COT提示下LLMs的安全漏洞，强调了进行后门攻击和设计有效防御的重要性。