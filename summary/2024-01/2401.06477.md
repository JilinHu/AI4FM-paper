#### 背景
- **背景**       
    文章讨论了在大型语言模型（LLM）中进行中文指令微调时遇到的一个关键挑战：原始数据在被标注后直接被用于指令数据集，这些未筛选的原始数据通常存在指令与响应之间的不匹配问题。

- **已有的工作**
    目前开源社区缺乏高质量的中文指令微调数据集。目前的数据集如Firefly、COIG、BELLE、MOSS以及OL-CC面临诸如限定范围窄、质量差、商业限制或者覆盖度不足等问题。这一缺口阻碍了LLM在有效处理和执行复杂中文指令方面的进步。

#### 核心贡献
- **提出了一个称为Kun（昆）的策略**
    - **挑战1：如何提高数据集中指令与响应之间的一致性？**
        Kun通过一个称为AP（Answer Polishment，答案打磨）的过程来解决这个问题，它细化原始数据以确保指令和响应之间有更紧密的相关性。不同于依赖于大型语言模型的方法，Kun提供了一个独立且可扩展的训练方法。

    - **挑战2：如何避免成本高昂和耗时的人工标注？**
        Kun的算法进展增强了数据保留和清晰度，并采用了创新的数据生成方法，大大减少了对昂贵且耗时的人工标注的依赖。

#### 实现与部署
Kun的训练方法需要一个基础模型、高质量种子数据和大量原始数据。原始数据主要包括与标注数据不同的大量未标记数据。作者使用了一个6B参数的名为Yi的模型作为基础模型，并进行了紧密的数据和指令的匹配，优化了数据集。为了评估模型的有效性，研究人员使用了500个来自ShareGPT-zh的命令进行了人工评测，在各种任务中测试了模型生成的响应，并与其他模型的响应进行了比较。他们的Kun-52k变体展示了优越性。此外，他们还评估了包含来自Wudao、Wanjuan和SkyPile等来源的1000个指令-输出对的数据集质量，这项评估关注于清晰度、可执行性、实用性和对齐，以确保数据集的高质量。

#### 总结
这篇论文提出了Kun策略，解决了中文大型语言模型指令微调中存在的数据一致性问题，通过AP过程和新的数据生成方法，减少了对人工标注的依赖。评估结果表明，Kun策略在创建高质量数据集方面具有明显优势。