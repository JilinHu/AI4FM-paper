#### 机构&分类
Google Research, University of Michigan
Reasoning

---

#### 背景
- **背景**       
    这篇论文研究了大型语言模型（LLMs）中的提示工程自动化。提示工程对于开发基于LLM的应用至关重要，但通常是以手动试错的方式进行，这种手动过程既耗时又常常得到的提示是次优的。此外，即使是看似有效的提示，也总存在可以通过进一步修改而得到改进的可能。

- **已有的工作**
    已有的自动化提示工程工作在实践中还未广泛采纳，并且通常使用较小型的模型如BERT和RoBERTa作为提示重写器/生成器模型，并不清楚这些方法是否能够推广到更强大和更大的模型。其他工作，如Automatic Prompt Engineer、Promptbreeder和OPRO等，虽然显示出了很好的效果，但大多使用固定的LLM基础提示重写器，因此并不是端到端优化。

#### 核心贡献
- **提出了一个名为PRewrite的自动提示重写工具**
    - **挑战1：自动化提示工程中的优化问题**
        手动提示往往未针对特定任务进行优化，且编写良好的手动提示时间消耗大，需要具备一定的LLM工作经验。PRewrite 使用强化学习框架，实现端到端的提示重写，克服了先前工作的缺点，可以生成人类可读且自解释的提示，而且显示出了比一些先前的最佳模型更出色的性能。

    - **挑战2：不同提示对不同任务性能的影响**
        这项工作通过在多个数据集上的大量实验，发现用这种新方法生成的提示不仅优于专业制作的提示，还超过了之前方法生成的提示。这证明了PRewrite在各种场景下改善提示并提升性能的有效性。

#### 实现与部署
PRewrite在NQ和AG's News数据集上显示出比使用初始提示的模型更好的性能。然而，在SST-2数据集上，PRewrite重写的提示与初始提示相比表现略低，这可能是因为任务的简单性限制了提示的改进空间。与之前的方法相比，PRewrite在SST-2和AG's News数据集上表现显著优于AutoPrompt、RLPrompt和TEMPERA等方法。未来的工作将测试该方法在更多样化的数据集和比PaLM 2-S更大的模型上的有效性，并探究不同的元提示和初始提示对任务性能的影响。

#### 总结
PRewrite通过强化学习框架自动改写初始手工制作的提示，能够生成人类可读并且符合逻辑自解释的提示，并在某些情况下比先前的模型表现出更好的性能。这项研究弥补了先前方法的不足，并在自动化提示改写领域迈出了新的一步。

---

#### Institution&Category
Google Research, University of Michigan
Reasoning
  
#### Background
- **Background**
This paper investigates the automation of prompt engineering in Large Language Models (LLMs). Prompt engineering is essential for developing LLM-based applications, but it is typically done manually in a trial-and-error manner. This manual process is time-consuming and often produces sub-optimal prompts. Moreover, even when prompts seem to work well, there is always a question of whether they can be improved with further modifications.

- **Existing Work**
The existing works on automated prompt engineering have not been widely adopted in practice and often use smaller models such as BERT and RoBERTa for the prompt rewriter/generator models. It remains unclear whether these methods can be generalized to more powerful and larger models. Other works like Automatic Prompt Engineer, Promptbreeder, and OPRO have shown promising results but mostly use a fixed LLM-based prompt rewriter, and thus are not optimized end-to-end.

#### Core Contributions
  - **Introduced an automated prompt rewriting tool named PRewrite**
    - **Challenge 1: Optimization issues in automated prompt engineering**
      Manual prompts are often not optimized for the task at hand and require significant time and experience with LLMs to craft effectively. PRewrite uses a Reinforcement Learning (RL) framework to perform end-to-end prompt rewriting, overcoming the drawbacks of previous works. It can generate human-readable and self-explanatory prompts, showing improved performance over some of the existing SoTA models.
      
    - **Challenge 2: The impact of various prompts on performance for different tasks**
      This work conducted extensive experiments across multiple datasets and found that the prompts generated by this new method not only outperform professionally crafted prompts but also exceed those generated by previous methods. This indicates PRewrite's effectiveness in improving prompts and boosting performance across a variety of scenarios.

#### Implementation and Deployment
PRewrite showed improved performance compared to models using the initial prompt on datasets like NQ and AG's News. However, on the SST-2 dataset, the performance with PRewrite's rewritten prompt was slightly lower compared to the initial prompt, which may be due to the simplicity of the task offering little room for improvement. PRewrite significantly outperformed previously proposed methods like AutoPrompt, RLPrompt, and TEMPERA on both SST-2 and AG's News datasets. Future work will aim to test this approach on more diverse datasets and with more substantial models than PaLM 2-S, and explore the impact of different meta-prompts and initial prompts on task performance.

#### Summary
PRewrite automates the rewriting of initial hand-crafted prompts using a reinforcement learning framework, producing human-readable and logically self-explanatory prompts, and outperforming previous models in certain contexts. This research addresses the shortcomings of previous approaches and represents a new step forward in the field of automated prompt rewriting.