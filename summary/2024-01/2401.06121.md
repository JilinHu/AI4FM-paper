#### 背景
- **背景**       
    文章介绍了大型语言模型(LLMs)在处理训练数据时存在的隐私和安全问题，特别是它们可能会记忆和泄露敏感或私人数据。提出了利用“遗忘”或调整模型来保护训练后数据的方法。

- **已有的工作**
    已有的遗忘方法存在对于实际效果评估不足和定义不明确的问题。现有工作重点在分类模型上，而对于当今LLMs经常处理的问答任务，其遗忘方法的评估通常使用的指标，并不能完全反映出遗忘的行为。

#### 核心贡献
- **提出了一个名为TOFU的新任务**
    - **挑战1：精确评估遗忘效果**
        提出了一个包含200个虚构作者个人资料的数据集，这些数据集之前没有出现在大型语言模型的预训练数据中。通过这个数据集可以清晰定义遗忘任务，并可以精确地确认需要遗忘的信息源，因此能够更准确地评估遗忘效果。

    - **挑战2：评价遗忘的方法**
        提出了一套新的评估机制，包括模型实用性和遗忘质量两个维度。通过创建新的评估数据集，并将这些数据整合成单一的模型实用性指标，同时提出了一个新的指标来比较在遗忘集上生成正确答案和错误答案的概率。这样的方法使得研究人员可以比较未遗忘模型和从未训练过敏感数据的“保留模型”。

#### 实现与部署
论文评估了四种基线遗忘方法，在模型实用性和遗忘质量这两个维度上对所有三个遗忘严重度级别进行了比较。结果显示现有方法在遗忘方面的尝试效果不佳，学习和遗忘过程相互交织，难以单独在遗忘集上进行遗忘同时保持在保留集上的性能不变。这些发现促使未来的工作，并为该新基准任务提供了大量的改进空间。

#### 总结
文章为LLM遗忘问题提供了新的数据集和评估机制，TOFU任务展示了现有遗忘技术的不足，鼓励了相继而来的改进和研究工作。