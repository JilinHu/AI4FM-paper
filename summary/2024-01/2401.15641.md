#### 背景
- **背景**       
    文章指出了大型语言模型(LLM)在学术和工业界引起了广泛的关注，特别是在如何有效评价和比较它们的能力上存在明显的难点。现有的评估范式依靠人类标注者或模型评估器来评估LLM在不同任务中的Performance。

- **已有的工作**
    目前的评估方法高成本、低通用性和固有偏见使得它们难以支持LLM的长期可持续发展。

#### 核心贡献
- **提出了一个Peer Review Evaluator (PRE)**
    - **挑战1：高评估成本**
        现有的方法需要大量标注工作，成本随着评估的LLM数量成比例增加，长期来看成本过高。PRE通过模拟学术同行评审机制，自动评估LLM，显著降低成本。

    - **挑战2：低通用性**
        现有方法需针对具体任务构建数据集并对评估器进行预训练，评估器难以推广至其他任务。PRE无需这样的依赖，可以轻易泛化至不同的任务。

    - **挑战3：固有偏见**
        现有评估工具由于模型结构或算法设计存在固有偏见。PRE使用多个评估者模型来评估，通过将结果聚合减少偏见的影响。

#### 实现与部署
PRE模型在文本总结任务的广泛实验中显示了与人类偏好（真实数据）的最高一致性，并且超越了包括GPT-4在内的所有基线模型，证明了同行评审机制的有效性。

#### 总结
这篇论文提出的PRE模型通过模拟学术界的同行评审机制，提供了一种全新的自动评估LLM的框架，它显著降低了成本，并且具有更高的通用性和可靠性。