#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）推理过程中的自回归解码是耗时的活动。现有的方法如推测性采样，尽管提高了速度，但仍存在挑战。

- **已有的工作**
    现有的推测性采样方法使用一个较小的、同系列的低参数版本的LLM作为草稿模型来减低推理延迟，并在验证阶段通过LLM的单次前向传播并行验证这些令牌。这些方法的共同问题是它们生成草稿的准确率较低，且对于最小的模型变种来说找到合适的草稿模型是困难的。

#### 核心贡献
- **提出了一个名为EAGLE（Extrapolation Algorithm for Greater Language-model Efficiency）的框架**
    - **挑战1：减少时间开销和提高草稿的接受率**
        在推测性采样中增强加速度的关键是减少时间开销和提高原始LLM对草稿的接受率。EAGLE不执行直接的标记预测，而是在特征级别进行自回归操作，并且通过整合前一时间步的tokens，来规避与特征级别自回归相关的不确定性。

    - **挑战2：保持生成文本与原始LLM的文本分布一致**
        EAGLE提供的加速是无损的：不涉及目标LLM的微调，生成的文本保持与普通自回归解码相同的分布。EAGLE框架甚至在生成非贪婪解码（temperature=1设置）的文本时仍维持输出分布与原始LLM一致。

#### 实现与部署
EAGLE 在MT-bench上的评估结果表明，在自回归解码方面，EAGLE是目前已知最快的框架，并且在模型加速度方面，它比Lookahead快两倍，比Medusa快1.6倍。EAGLE训练成本低，对于LLaMA2-Chat 70B模型，EAGLE训练的解码器层的参数少于10亿，并且训练时间只需1-2天。此外，EAGLE框架易于生产环境中部署，不涉及对原始LLM的任何微调，并且应用简单，仅为LLM添加了一个轻量级的插件（一个Transformer解码器层）。

#### 总结
文章提出了一个名为EAGLE的新框架，以提高大型语言模型（LLMs）自回归解码的速度，同时保证生成文本与原始LLMs的文本分布一致。EAGLE通过改进推测性采样方法，在减少时间开销和提高草稿的接受率方面取得了显著成效，对比Lookahead和Medusa实现了更快的加速效果，并且训练成本低，易于部署。