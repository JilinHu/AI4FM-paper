#### 背景
- **背景**       
    文章介绍了多变量时间序列预测的挑战，指出以往的大型预训练模型在该领域面临的难题，尤其在多样化和公开预训练数据稀缺的情况下。

- **已有的工作**
    已有工作在时间序列预测方面使用了Transformer和MLP-Mixer架构的高效替代品，如TSMixer，但由于数据集在不同应用中的多样性和公开预训练数据的有限性，无法实现预训练模型的通用转移学习。即使有些针对特定数据集的自监督预训练方法，它们也未能在不同数据集之间实现通用的转移学习能力。目前趋势是利用大型语言模型(LLMs)对时间序列进行预测，这些方法虽然在少数/零样本预测方面表现出色，但未能有效处理多变量预测中的可控变量和外部变量，且模型庞大，计算资源消耗巨大。

#### 核心贡献
- **提出了一个名为Multi-level Tiny Time Mixers (TTM)的小型模型**
    - **挑战1：在多样化且分辨率不一的公开数据集上进行有效预训练**
        TTM通过引入如自适应分块、通过下采样进行数据增强、分辨率前缀微调等创新增强，解决了在多个具有不同时间分辨率数据集上预训练的问题。

    - **挑战2：渠道间关联和外源信号的整合问题**
        文章提出了一个多级建模策略来显式建模渠道关联，并在微调阶段纳入外源信号，解决了现有基于LLM的TS模型缺乏的关键能力。

#### 实现与部署
TTM在几个/零样本的预测任务中优于现有基准，准确率提高12-38%，并且与现有的LLM-TS基准相比，在模型参数上减少了14-106倍，从而实现了54-65倍的训练/推理速度提升。在许多基准测试中，TTM的零样本结果甚至超过了少样本结果，突出了其方法的有效性。此外，代码和预训练模型将开源。

#### 总结
TTM展示了专门针对多样化时间序列数据训练的小型预训练模型在多变量时间序列零/少样本预测中的高效性和转移学习能力。