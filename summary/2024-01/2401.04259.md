#### 背景
- **背景**       
    文章介绍了现代大型语言模型（LLMs）如GPT-4在某些任务中能够与人类表现相媲美，但它们处理长文本和技术性强的文献，如科学论文的能力尚未充分探索。现有的模型通常只能处理有限长度的文本，且主要训练在非技术文本上，例如新闻文章和网站内容。

- **已有的工作**
    以往的研究尝试自动生成科学论文的同行评审反馈，但大多使用相对较小的模型且无法消化整个论文的文本，或者使用模板生成而不是生成细致的自由形式评论。即便是更先进的尝试，如使用GPT-4来验证作者清单，也受限于生成评论类型的多样性。之前的工作未尝试解决GPT-4的输入大小限制，且没有构建针对不同评论类型的专门提示和"专家"。

#### 核心贡献
- **提出了一个名为MARG的反馈生成方法**
    - **挑战1：输入长度限制**
        使用多个GPT实例（代理），将论文文本分布给不同代理并让它们相互交流，MARG允许处理超出基础LLM输入长度限制的完整论文。通过将代理分工专业化，并结合专门针对不同评论类型的子任务（如实验、清晰度、影响力），MARG提高了反馈的帮助性和特异性。

    - **挑战2：生成特定类型的反馈**
        引入针对特定方面的“专家”GPT代理，分别协助生成有关实验、清晰度和影响力的评论。MARG-S作为MARG的专业化变体，能够比单个代理同时生成所有类型反馈的方法表现得更好。

#### 实现与部署
在用户研究中，MARG-S平均每篇论文生成了3.7个“好”的评论，而单个代理生成所有评论的简单基线只生成了1.7个好评论，Liang等人（2023）最近提出的方法产生了0.3个。此外，尽管用户将基线生成的评论的大部分视为泛化评论，但MARG-S的评论中有71％被评定为具体。最后，作者分析了MARG-S的缺点，包括高成本和内部沟通错误，例如在某些消息中未能包含关键信息，并为未来的工作提出了方向。

#### 总结
本论文提出了一个创新的多代理评论生成方法（MARG），可以跨越基础模型的上下文大小限制，生成高质量的科学论文同行评审反馈。通过用户研究和自动化度量，MARG的反馈质量对比基线有显著提高，生成的有用评论数量提高了2.2倍，同时生成了更加具体的评论。