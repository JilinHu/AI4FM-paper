#### 背景
- **背景**       
    论文中指出，大型语言模型（LLMs）在处理长文本序列时的能力有限，这往往是因为预训练时的上下文窗口大小有限。以往的工作显示扩展上下文窗口需要对大型模型进行fine-tuning，但这个过程既费时也计算资源密集。

- **已有的工作**
    虽然RoPE（Rotary Position Embedding）等技术已被证明可以在没有fine-tuning的情况下有效扩展上下文窗口，但通常这些方法还是需要通过fine-tuning来适应更长的序列长度。

#### 核心贡献
- **提出了一个名为Self-Extend的方法**
    - **挑战1：如何在不经过fine-tuning的情况下扩展上下文窗口**
        本文提出的Self-Extend方法通过设计两种注意力机制，即针对长距离token的分组注意力以及针对近邻token的正常注意力，来实现在LLMs中未见位置映射，无需fine-tuning。

    - **挑战2：如何保证性能不下降**
        Self-Extend在合并两种注意力时做出了特别设计，保证了正常注意力范围内的平滑过渡到分组注意力范围，并通过实验表明其在无需额外训练的情况下有效提升了模型在长文本处理上的性能。

#### 实现与部署
Self-Extend的实验展示了在不同的基准测试上，即使与需要fine-tuning的方法对比，Self-Extend的性能也是可比的或者更优。这表明该方法在增长上下文窗口同时避免了fine-tuning的高计算成本。具体表现在不同模型如Llama-2-7b-chat和Mistral-7B-instruct等上的性能提升。

#### 总结
这篇论文成功展示了一种无需fine-tuning即可扩展LLMs上下文窗口的方法，这对于在计算资源受限情况下提升大型语言模型处理长文本的能力具有重要意义。