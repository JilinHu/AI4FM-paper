#### 背景
- **背景**       
    论文中主要介绍了在数学问题求解领域内大型语言模型(LLMs)的应用现状。这是一个复杂的任务，因为它不仅涉及到正确理解问题，还要生成正确的解决步骤和最终答案。目前的方法大多依赖于监督式微调，即使用正确的推导步骤(Chains of Thought, CoT)训练LLMs。尽管这些方法取得了一定的成效，但它们存在一个主要局限性：依赖于单一正确的CoT注释，而没有充分利用训练数据中的多个CoT注释探索正确答案的潜力。

- **已有的工作**
    已有的工作通过监督式微调(SFT)固化了一个从问题直接到答案的路径，但没有在生成过程中进行探索(optimize a non-differentiable objective)，因此存在潜在的过拟合和泛化能力不足的问题。在数学问题求解的背景下，特别是在考虑到要求算法不仅要找到正确答案，还要生成解决问题的步骤时，这一局限性就变得尤其明显。

#### 核心贡献
- **提出了一个Reinforced Fine-Tuning (ReFT)**
    - **挑战1：改进LLMs在数学问题求解中的推断能力及其泛化能力**
        通过应用增强学习，特别是Proximal Policy Optimization (PPO)算法，ReFT对非可微目标(正确答案)进行了优化。与监督学习不同，ReFT探索了多种CoT，而不仅仅是一个固定的解决路径。从而克服了已有方法的局限性，即无法充分利用训练过程中的多样性。凭借强化学习，ReFT提升了模型的泛化能力，使其在数学问题求解任务中取得了更好的性能。

    - **挑战2：提高训练效率和避免奖励函数的操纵**
        ReFT需要更多的训练迭代次数才能达到收敛，主要是因为其需要探索生成空间以找到正确答案。此外，现行的奖励函数仅依赖最终答案来确定奖励，存在被操纵的可能性，特别是在可能答案空间受限的情况下。论文提出，未来的工作将包括开发无需预热的方法来提高训练效率及性能、探索基于流程的奖励(including utilization of offline reinforcement learning techniques and the implementation of process-based rewards)来强化奖励函数，以及将ReFT应用到更广泛的推理任务中。
    
#### 实现与部署
ReFT在三个数学问题数据集GSM8K, SVAMP, 和MathQA上进行了试验，并与SFT以及自我训练方法进行了比较。使用了两种基础模型Galactica-6.7B和Codellama-7B。结果表明，ReFT在各个数据集上都取得了更好的性能。在训练过程中，ReFT通过对生成的CoT进行探索，实现了更高的性能和泛化能力。论文还探讨了ReFT性能上的局限性，为未来的研究提供了方向。

#### 总结
ReFT通过利用强化学习优化非可微目标，显著提高了大型语言模型在数学问题求解任务中的性能和泛化能力。它超越了传统的监督式学习方法，展现了在更复杂推理任务中的潜力。