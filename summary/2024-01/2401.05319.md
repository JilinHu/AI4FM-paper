#### 背景
- **背景**       
    这篇论文讨论了在处理复杂数据结构和算法的编程问题时，现有的大型语言模型（LLMs）在代码生成任务方面的性能仍有改进空间。

- **已有的工作**
    现有方法常使用如rubber duck debugging，它们没有能力获取实时变量值或追踪执行流，这是处理复杂算法代码调试中非常重要的。同时，这些方法也不能充分利用测试用例，只是向LLMs提供失败的测试用例，很难帮助定位bug。

#### 核心贡献
- **提出了一个“print debugging”的方法**
    - **挑战1：追踪执行流和实时变量值**
        现有的方法缺乏实时监控和执行流的追踪能力，这篇论文通过引入print debugging方法，指导LLMs使用打印语句来追踪和分析日志，从而修复程序中的bug。

    - **挑战2：有效利用测试用例反馈**
        仅向LLMs提供失败的测试用例反馈并不足以定位bug。论文方法通过在代码中添加打印语句并比较输出日志和测试用例说明的不一致性，帮助模型识别并修复bugs。

#### 实现与部署
在对GPT-4进行的实验中，使用Leetcode问题集进行评估，结果显示，相比rubber duck debugging，print debugging在简单和中等难度的Leetcode问题上分别提高了1.5%和17.9%的性能。尽管如此，在高难度的问题上，这种方法并没有带来性能提升，准确率仅为5%，表明对于复杂的问题，单纯的调试方法可能不足以解决潜在的算法问题。

#### 总结
本文提出了一种利用print debugging方法指导LLMs进行代码生成和调试的方法，并且在Leetcode问题集上验证了其有效性，特别是在简单和中等难度的问题上。尽管在高难度问题上效果有限，但这项工作仍然是LLMs在代码调试方面的一个重要进步。