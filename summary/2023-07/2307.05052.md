#### 背景
- **背景**       
    文章介绍了大型语言模型在自然语言处理任务中的上下文学习（ICL）能力。ICL通过使用少量的输入-标签对示例来指导模型，而无需对模型参数进行微调。然而，每一部分示例如何驱动预测，特别是当在上下文中提供给模型时，仍是一个开放的研究问题。以往的工作对此有混合的发现，例如，尽管人们可能会假设真实标签会对ICL有类似于有监督学习的影响，但Min等人（2022）发现真实标签与输入之间的对应关系对最终任务的性能影响很小。此外，标签的顺序对结果有很大影响，只有规模较大的LLM能学习反转的输入-标签映射。

- **已有的工作**
    已有的研究不能完全解释ICL在示例的不同部分--如标签的翻转、输入的变化以及补充性解释的添加--上的效果。例如，大型语言模型在小规模（如所有的GPT-3模型）时不能通过上下文中的示例来覆盖先前从预训练中获取的知识，即，当示例中的真实标签被翻转时，模型的预测不会翻转。但是Wei等人（2023）显示更大的模型，如InstructGPT和PaLM-540B，在同样的设置中表现出覆盖先前知识的新能力。此外，输入分布的效果也未得到充分研究，Min等人（2022）将整个输入更改为随机单词，Wei等人（2023）则完全没有研究输入分布。

#### 核心贡献
- **提出了一个对比示例和显著图的研究方法**
    - **挑战1：如何理解不同部分的示例对ICL预测的贡献**
        此研究使用了NLP中的对比示例对构建的示例部分进行了定性和定量分析，以理解这些部分对预测的贡献。通过对不同构建方式的示例进行翻转、中和和补充性解释，然后通过显著图对这些对比示例进行对比。研究发现，在情感分类任务中，示例中的真实标签在标签翻转后变得不那么显著。

    - **挑战2：量化输入分布和补充性解释的影响**
        研究对输入文本的不同部分进行了细致的编辑，以符合特定任务的目的。对于情感分析任务，研究调整了示例输入文本中的情感表达术语，将其变为中性术语，发现这种输入干扰（中和）没有像改变真实标签那样的大影响。此外，研究发现补充性解释并不一定对情感分析任务有益，尽管显著图表明解释令牌与原始输入令牌一样显著。

#### 实现与部署
研究采用了三种构建对比示例的方法，并利用显著图来理解ICL的不同部分对模型预测的影响。实验结果表明，标签翻转后对真实标签的显著性降低，而输入文本的情感词汇中和对模型预测的影响较小，可能是因为模型依赖于预训练时获得的知识来做出较好的预测。补充性解释虽然显著，但没有明显提升情感分析任务的效果。这些发现提醒我们在寻求提高ICL表现时，需要仔细生成和评估补充性解释，并考虑它们是否确实惠及目标任务。

#### 总结
本研究使用对比示例和显著图分析法来探究大型语言模型中上下文学习的内在机制，揭示了标签翻转、输入变化、和补充性解释对预测的不同影响，并为实践者提供了如何策划示例的洞见。