#### 背景
- **背景**       
    论文中提到，文本嵌入在自然语言处理（NLP）任务中有着广泛的应用，如信息检索、问答等，它们的作用是编码自然语言的语义信息，以实现高效准确的内容检索。然而，现有的基于BERT的方法虽通过对NLI数据集的微调来学习文本嵌入，但仍不能很好地抓住语言的丰富上下文。同时，面向高性能和鲁棒性的E5和BGE等方法则采用更复杂的多阶段训练范式，先用大量弱监督文本对预训练，再在几个标注数据集上微调。

- **已有的工作**
    现有多阶段训练的方法存在几个问题：首先，它们需要构建复杂的多阶段训练流水线，这需要大量的工程努力去策划关联性数据对；其次，它们依赖人工收集的数据集，通常受限于任务的多样性和语言覆盖度。

#### 核心贡献
- **提出了一个基于LLMs生成合成数据的文本嵌入新方法**
    - **挑战1：简化训练流程，减少对人工标注数据集的依赖**
        论文提出的方法不依赖于构建复杂的训练流水线或人工收集的数据集，而是使用专有的大型语言模型(LLMs)来生成用于百万级文本嵌入任务的合成数据。该方法可以在没有任何标注数据的情况下，通过少于1千步的训练，实现与当前竞争性文本嵌入基准相媲美的表现。

    - **挑战2：增强跨语言和任务类型的多样性**
        利用两步提示策略，首先提示LLM发散思维产生候选任务池，然后再基于给定任务生成数据，以此来涵盖各种应用场景，设计不同的提示模板提升数据的多样性。同时，选择对开源的大型解码器风格的LLMs进行微调，以适配不同语言和任务类型。

#### 实现与部署
对Mistral-7B模型仅用合成数据进行微调后，在BEIR和MTEB基准上达到了有竞争力的性能，当进一步结合合成数据和标注数据微调时，该模型在这些基准上取得了新的最高水平，超过以前方法2%。此外，还证明了模型能有效执行针对长达32k tokens的个性化passkey检索，通过改变位置嵌入的旋转基准，超越了传统512 token的上下文长度限制，在多语言方面表现突出，尤其对资源丰富的语言。

#### 总结
本文采用最新的大型语言模型和合成数据，提出一种新颖的文本嵌入方法，能够在无需人工标注数据且训练步骤少于1千的情况下，达到与竞争性基准相匹配的性能，为进一步提升文本嵌入技术提供了有力证据。