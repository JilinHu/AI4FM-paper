#### 背景
- **背景**       
    文章着眼于深入探讨大型语言模型（LLMs）的In-Context Learning (ICL)能力，特别是在文本分类任务中对于多数类标签偏差的鲁棒性。过去的研究表明了LLMs在ICL任务中易受样本选择偏差（如多数类标签偏差、近期偏差等）的影响，导致模型倾向预测那些在上下文示例中频频出现的标签。

- **已有的工作**
    已有工作未能彻底检验不同比例的类别分布在文本分类任务的上下文提示中对模型性能的影响。现有研究也未全面探讨针对多数类偏差下模型性能的鲁棒性边界。

#### 核心贡献
- **提出了对多数类标签偏差鲁棒性边界的全面研究**
    - **挑战1：如何评估多数类偏差对ICL的影响**
        研究关注于不同LLMs在多种多样的类标签比例上下文示例中的ICL表现。挑战在于确认这些模型是否能够在面对标签分布变化时保持性能稳定。

    - **挑战2：了解模型大小和指示性指令对鲁棒性的影响**
        研究提出了包含模型大小对比和增加信息指示性指令对模型抵抗多数类标签偏差的影响的消融实验。

#### 实现与部署
该研究使用公共可获取的开源模型进行实证研究，以确保透明性和可复现性。实验选取了BoolQ 和 RTE-1/2/3 这些具有二元标签的数据集，以及现实中的多元类别数据集COVID-5G Conspiracy，通过人为地创建上下文示例，生成不同程度的多数类标签偏差。文中提到，某些LLMs在ICL中表现出约90%的对多数类标签偏差的鲁棒性。实验通过改变在上下文示例中的Yes/No比例（从0%Yes-100%No到100%Yes-0%No，以10%为步长），模型以权重F1分数评估结果，整个实验过程中重复此比例变化以评估模型处理多数标签分布偏差的鲁棒性。

#### 总结
本文对LLMs在面对ICL中多数类标签偏差时的鲁棒性进行了全面研究，通过实验发现某些模型在处理这种偏差时显示出显著的稳定性。