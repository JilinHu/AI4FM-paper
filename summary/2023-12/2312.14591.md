#### 背景
- **背景**       
文章调研了在人际互动中，自然语言反馈帮助我们反思行为、维持适当行为以及纠正错误的情况，并探讨了是否也可以使用语言反馈来调整大型语言模型（LLM）。

- **已有的工作**
以往的研究主要通过奖励或偏好数据来对LLM进行校准，但这些方法并不能充分利用反馈。此外，现有方法仅使用评判性语言反馈作为提示，促使模型生成改进后的响应然后作为新示范进行监督训练，这种间接利用评判的方式无法充分学习错误。

#### 核心贡献
- **提出了一个名为Contrastive Unlikelihood Training (CUT)的新框架**
    - **挑战1：提高对评判性反馈利用**
        CUT框架能够基于评判性反馈进行细粒度的不恰当内容检测和修正。通过对比真实评判（可能包含负面意见）和理想化评判（表现完美）的生成概率，检测并惩罚响应中的不恰当内容，这允许模型直接从评判中学习并执行修正。

    - **挑战2：离线和在线的模型对齐**
        CUT方法在离线和在线对齐实验中展示了其有效性。在离线环境中，即使只有1317条现有的评判数据，CUT也能使LLaMA2-13b在AlpacaEval上超越175B DaVinci003模型，并且比最好的基线强52.34点。在在线对齐实验中，CUT表明它可以使用根据模型特定评判数据进行迭代精细化，使LLaMA2-chat-13b在AlpacaEval上的性能从81.09点稳步提升到91.36点。

#### 实现与部署
CUT框架的实现与部署通过多个实验证明了其有效性。在离线对齐实验中，CUT使用1317个现成的评判数据，在AlpacaEval基准中取得62.56的胜率，超过了175B大小的DaVinci003模型，且比最好的基线模型高出了52.34分。在在线对齐实验中，通过四次CUT迭代，在AlpacaEval上的性能持续提升，LLaMA2-chat-13b从81.09分提升到91.36分。研究还发现，与奖励相比，评判在对齐LLM方面表现出了更大的潜力。

#### 总结
论文提出了一个新的通过直接利用语言反馈来对齐LLMs的框架Contrastive Unlikelihood Training（CUT），并且证明了其在多种场景下的有效性，包括离线对齐和在线对齐，以及从未对齐的模型（冷启动）和已对齐的模型（热启动）进行进一步优化。研究显示，与奖励相比，评判性反馈在对齐LLMs方面具有更大的潜力，值得进行进一步研究。