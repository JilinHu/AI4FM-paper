#### 背景
- **背景**       
    论文指出，预训练的大型语言模型（LLMs）需要进行微调以提高对自然语言指令的响应性。尽管之前的参数节省技术（parameter-efficient fine-tuning, PEFT）对LLMs调整也有贡献，但这些方法可能无法达到与全参数调整（full-parameter tuning）同等的性能水平。目前在分布式数据的隐私保护背景下，联邦学习（Federated Learning, FL）提供了一种利用终端设备上的丰富数据进行模型微调的方法。然而，全参数微调对服务器和客户端的通信开销巨大，对于数十亿大小的LLMs而言几乎是不切实际的。

- **已有的工作**
    以往全参数微调大型语言模型在设备层面上操作的技术解决方案并不现实，因为它们通常涉及巨大的通信成本，这对于数亿大小的LLMs来说代价非常高昂。比如使用反向传播（backpropagation, BP）和大部分无BP方法如零阶优化（zeroth-order optimization, ZOO）会导致与模型大小成比例的通信成本。此外，基于BP的方法还需要大量内存，这对大多数终端设备来说是不现实的。

#### 核心贡献
- **提出了一个名为FedKSeed的方法**
    - **挑战1：如何降低通信成本？**
        已有的ZOO方法以牺牲其他性能因素为代价来减低通信成本，而FedKSeed方法采用有限组随机种子，这种理论上的种子复用范式有效降低LLMs全参数微调的通信成本到不到18千字节每轮，并且内存使用效率与推理所需内存相同。

    - **挑战2：如何提高模型准确性和计算效率？**
        FedKSeed进一步开发了一种评估ZOO扰动重要性的策略，允许按概率区别种子采样，优先考虑对模型精确度影响更大的扰动，通过这样的方式筛选种子池中的种子，可以加快与最新模型状态同步的过程，从而进一步提高计算效率和模型准确度。

#### 实现与部署
FedKSeed的主要实现包括了使用ZOO方法和一组随机种子来进行联邦全参数微调，使得在设备上直接调整数十亿大小的LLMs成为可能。在进行实验研究时，相较于现有的联邦LLM微调方法，研究者通过六种不同的场景，包括不同LLMs数据集和数据分区，验证了该方法在通信效率和新任务泛化方面的优越性。

#### 总结
论文提出了一种新颖的联邦全参数微调方法——FedKSeed，通过ZOO与有限组种子结合，显著降低了数十亿大小LLMs全参数微调所需的通信开销，同时实现了较高的模型精确度和计算效率。