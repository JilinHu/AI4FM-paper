#### 背景
- **背景**       
文章指出，回答复杂的自然语言问题通常需要多步骤的推理和整合外部信息。尽管已经有数个系统尝试通过结合知识检索和大型语言模型（LLM）来回答这类问题，但是这些系统通常会遇到各种故障案例，而且由于与外部知识的互动是不可导的（non-differentiable），无法直接对它们进行端到端的训练以修正这些失败案例。

- **已有的工作**
现有的系统在与外部知识互动时遭遇不可导的问题，限制了模型在处理这些失败案例上的自我改进能力。

#### 核心贡献
- **提出了一个具备推理和对外部知识做出行动能力的ReAct风格LLM代理**，并通过ReST类似的方法对代理进行精炼，该方法使用了AI反馈的增长式批量强化学习进行持续的自我改进和自我蒸馏。作者证明，从一个提示过的大型模型开始，仅经过两次算法迭代后，可以生成一个经过微调的小型模型，在具有挑战性的组合式问答基准测试中取得与大型模型相当的表现，而所需的参数数量却小两个数量级。
    - **挑战1：如何改进LLM代理的推理和外部知识互动以解答复杂的自然语言问题**
        作者定义了一个具备推理和对外部知识做出行动能力的ReAct风格LLM代理，通过ReST类似的方法进行自我改进，这种方法利用AI反馈和增长式批量强化学习，实现了与传统方法不同的接口和交互模式。

    - **挑战2：如何提高精确度并减少模型大小**
        通过算法迭代和自我蒸馏的方法，在保持表现的同时大幅度减少了模型所需的参数数量，使得小型模型能够在复杂问题上与大型模型相媲美。

#### 实现与部署
使用了不同大小的PaLM 2“base”模型以及内部的Google Q&A API来执行搜索代理。利用了四个不同的数据集来为搜索代理提供初始问题，并通过将完成的搜索代理轨迹拆分成推理步骤来构建微调混合。对于排名“奖励”模型，采用指令微调的PaLM 2-L，并利用LLM基于指令挑选最佳样本进行微调。在自我改进算法中，自底向上训练新模型，并验证其性能是否有所提高。同时，使用小型模型进行自我蒸馏。主要的评估方法是使用Bamboogle数据集和BamTwoogle数据集，以避免过度拟合，并通过LLM基础的自动评估和人工评估来验证答案的正确性。

#### 总结
本论文通过定义一个能够进行推理和外部知识互动的LLM代理，并采用自我改进算法，实现了在合成问答基准测试中小型模型与大型模型相媲美的表现。提出的方法不仅提高了模型的推理能力，也大大减小了模型所需的参数数量。