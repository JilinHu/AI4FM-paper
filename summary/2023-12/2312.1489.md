#### 背景
- **背景**       
    本论文提出了，虽然现有的基准测试（benchmarks）能够评估大型语言模型（LLMs）的推理能力，但这些基准测试存在一些限制。目前这些基准静态且公开可用，使得模型可能专门针对这些基准量身打造回答，这可能导致它们的性能表现被夸大。

- **已有的工作**
    现有的评估方法主要依赖于含有人类生成问题的数据集及其标准答案，例如MMLU和GAOKAO，以及法国国家数学考试等。然而，这些评估通常缺乏一个量化度量难度水平或所需推理程度的标准，导致我们对LLMs逻辑推理能力的理解受限。

#### 核心贡献
- **提出了一个名为NPHardEval的新基准**
    - **挑战1：如何进行更全面和动态的评估？**
        NPHardEval 设计了动态更新机制，数据点每月刷新，以减少模型过度拟合基准的风险，提供了一个更准确和可靠的评估方法。

    - **挑战2：如何评估LLMs在各种复杂度类别问题上的推理能力？**
        NPHardEval 包含了900个扩展到NP-Hard复杂性类别的算法问题，这些问题经过精心挑选，以代表NP-hard以下的各种复杂性类别，提供了对LLMs推理能力的严密衡量。

#### 实现与部署
基准测试NPHardEval对10个不同的LLMs进行了评估，包括GPT-4 Turbo、Claude 2、GPT-3.5 Turbo等，使用了零次（zero-shot）提示作为性能的基础衡量。实验结果显示，当面对NP-Hard问题时，所有模型性能出现明显下降，平均加权准确度在P和NP-Complete问题上为0.26，在NP-Hard问题上下降到0.03。关闭源代码的模型（close-source models）一般比开源模型表现得更好。在不同复杂度类别的问题上，测试结果表明，LLMs在面对P和NP-Complete问题时表现更好，而NP-Hard问题则表现明显更差。此外，在上下文中学习的实验中，闭源模型在从示例中学习算法技能方面显示出一致性，表明它们正在学习，而不只是模仿解决方案。

#### 总结
本论文通过NPHardEval基准测试提供了一种新的评估LLMs推理能力的方法。该基准测试广泛涵盖了从多项式时间到NP-Hard复杂性级别的问题，并设计了动态数据更新机制以防止模型过拟合，从而确保了评估结果的可靠性和真实性。这些发现极大地促进了对LLMs当前能力的理解，并为提高这些模型的推理能力铺平了道路。