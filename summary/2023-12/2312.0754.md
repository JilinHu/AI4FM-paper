#### 背景
- **背景**       
    论文着重分析长期背景下的语言模型的决策代理的挑战和解决方案。作者指出现有的模型在处理如NetHack这类复杂游戏时，由于观察和交互历史的长度超出了模型可以处理的范围，导致对策略进行有效学习存在困难。

- **已有的工作**
    现有工作无法有效处理长时间跨度内的连续决策任务，原因在于模型的输入受到固定的上下文长度限制，而复杂的决策环境中交互历史迅速累积处理困难。

#### 核心贡献
- **提出了一个解决长交互历史问题的方法**
    - **挑战1：长交互历史的处理**
        为了应对长时序依赖关系，论文提出了一种新的称为diff历史的方法，它通过使用Unix diff运算来计算连续观察结果之间的差异，并以此代替原始观察，从而有效压缩交互历史的长度。

    - **挑战2：动作理解的表示**
        为了区分动作与观察数据，引入了特殊标记 ((action-start)) 和 ((action-stop))，这不仅帮助模型区分交互历史中的动作和观察，还简化了推理阶段文本动作解码的过程。

#### 实现与部署
根据LangHack数据集进行实验，该数据集包含使用AutoAscend机器人生成的50万个游戏片段作为训练和评估数据。论文在不同的实验设置下对模型进行了评估，发现使用diff历史的语言模型在动作预测上有显著提升。例如，在使用diff历史的语言模型经过16个epochs的训练后，其平均NetHack得分高达1885分，而原始交互历史的语言模型得分仅约20分。此外，diff历史方法使得模型在训练时可用的历史范围平均增加了4倍。

#### 总结
论文提出并验证了使用diff历史来提高对长交互历史的模型处理能力。这一方法显著提升了模型在复杂决策任务中的表现，并能有效扩大模型可处理的历史长度，为长时间序列决策代理的设计提供了新思路。