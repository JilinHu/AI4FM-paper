#### 背景
- **背景**       
    文章提出的问题是开源大型语言模型（LLMs）在处理长文本相关任务时存在的限制。大多数现有的开源模型，如长文本会话模型，在长文本的处理上准确性不佳。主要原因是这些模型通常有一个不超过4k的上下文窗口，不足以处理涉及长文本的复杂问题。

- **已有的工作**
    已有的开源模型如Llama-yarn和LongAlpaca虽然声称能够处理长达32k的文本，但在长文本聊天的精确度上仍不尽人意。至于更强大的模型如GPT-4-128K和Claude-2-200K，它们由于高昂的训练成本和训练数据的保密性，没有开放源代码。现有的一些尝试，如线性位置插值(PI)、"NTK-aware"插值以及Yarn，通过插值ROPE并随后用少量的长文本数据微调模型来扩展上下文窗口。

#### 核心贡献
- **提出使用原文释义任务改进上下文窗口**
    - **挑战1：长文本数据未被有效利用**
        现有模型在处理长文本时往往丧失对文中部分重要信息的把握，只侧重处理文本开头和结尾的信息，这是因为人类语言有 "临近更相关" 和 "在起始和结束处强调" 两个特点。论文提出，要训练处理长上下文的模型，需要的不是简单的长文本数据，而是 "有效" 的数据，例如"W形数据"，以解决 "丢失在中间" 的问题。

    - **挑战2：现有模型在长文本场景下精确度低**
        之前模型在长文本场景下精确度不高的原因在于输出的有效性不足。论文提出通过使用有效的指令微调数据，可以通过Qlora方法高精度地训练出长文本问答的LLM。
  
#### 实现与部署
通过上述方法，文章成功地将Qwen-14B-chat模型的上下文窗口从2k扩展到了32k，并在多文档问答任务中达到了非常高的准确度，超过了同等规模的所有现有开源模型。这个模型和训练数据已经在HuggingFace和WiseModel上开源。

#### 总结
论文主要通过理论证明和实验验证，提出了一种低成本且高效的方法，通过原文释义任务和有效的指令微调数据扩展现有语言模型处理长文本的能力，显著提高了长文本问答的准确性。