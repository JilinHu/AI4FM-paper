#### 背景
- **背景**       
    论文指出在计算机教育研究中需要课程教师开发和维护有效的最新评估方法。已有的研究利用大型语言模型（LLMs）生成编程练习，但用于生成编程多项选择题（MCQs）的研究却罕见。本文分析了GPT-4生成与高等教育Python编程课程特定学习目标（Learning Objectives，LOs）一致的MCQs的能力，为此开发了一个基于GPT-4的MCQ生成系统。

- **已有的工作**
    以往的研究主要关注使用小型LLMs模型处理MCQ的单一步骤，或者在缺乏大规模评估框架的情况下利用LLM生产阅读理解型问题。尽管有使用GPT-3和GPT-4生成MCQ的尝试，但这些研究没有自动生成新的高等教育编程类MCQ的尝试。

#### 核心贡献
- **提出了一个基于GPT-4的自动化MCQ生成系统**
    - **挑战1：与特定学习目标的对齐**
        论文利用BERT（双向编码器表示转换器）模型的分类器预测提供的LO的Bloom's Taxonomy级别，从而确定合适的问题类型，确保生成的MCQ与LO保持良好的一致性。

    - **挑战2：生成质量与人类设计的MCQ相近**
        研究了GPT-4生成质量如何与经验丰富的课程设计者设计的MCQ相媲美，通过多项检验标准，包括是否存在多个正确选项、干扰项的质量等，对自动生成的和人工设计的MCQ进行了比较。评价结果表明，自动生成的MCQ在对齐LO方面比人工设计的更好，81.7%的自动生成的MCQ通过了所有评价标准，但仍有一些在质量上的差距。

#### 实现与部署
对于评估自动生成的MCQ，本研究生成了与246个LO对应的651个自动生成的MCQ。通过开发一套六项评估标准的量表，让7名学生和6名计算机科学教师注释器（文中所有作者）对这些MCQ及449个人工设计的MCQ进行了评分，得出了651个自动生成的MCQs的分布情况。通过Fisher精确测试对自动生成的MCQs和人工设计的MCQs在六个类别上进行了比较。结果显示自动生成的MCQs在清晰语言提供足够信息（RQ1.i）和包含语法和逻辑上正确代码（RQ1.iv）上与人工设计的MCQs无异。然而，当涉及到单个正确答案（RQ1.ii）和高质量干扰项（RQ1.iii）时，自动生成的MCQs稍显不足。

#### 总结
本文的主要贡献是开发了一个基于GPT-4的自动化MCQ生成系统，通过专门的弹性构架和精确的LO对齐机制，成功生成与高等教育Python课程LO一致的MCQs。研究结果表明，自动生成的MCQ在大多数情况下与LO保持良好的一致性，质量接近人工设计的MCQ，但在拥有单一正确答案和高质量干扰项方面略显欠缺，未来工作应该集中在减轻这些问题上。