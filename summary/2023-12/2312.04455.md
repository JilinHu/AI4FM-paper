#### 背景
- **背景**       
    论文针对大型语言模型（LLMs）在工具使用性能方面存在的问题进行探讨。作者发现，模型在分配注意力时出现的波形模式会对工具使用性能产生影响，尤其是当关键信息位于注意力分配波形的低谷区域时，LLMs的工具使用性能会下降。

- **已有的工作**
    现有的LLMs工具使用框架虽然取得了一些成就，但作者在实验中观察到，这些模型对上下文中不同位置的认知水平存在变化，这可能会影响它们在工具使用任务中的表现。具体来说，当关键信息恰好与注意力波形的低谷重合时，模型可能会忽略这一信息，从而降低精度。

#### 核心贡献
- **提出了一个名为Attention Buckets的新推理方法**
    - **挑战1：波形模式中的关键信息遗漏**
        论文通过几个并行执行的流程来处理上下文，每个流程都具有一个独特的旋转位置嵌入（RoPE）角度基础，这些角度基础形成了不同的注意力波形。通过交错不同运行中的注意力波峰，可以补偿特定流程的注意力低谷，从而降低LLMs遗漏关键信息的风险。

    - **挑战2：增强上下文的认知**
        通过在不同波峰之间进行交错，使得LLMs在每次运行中都能关注上下文中的不同位置，这些共同提供了一个全面的理解。论文将所有这些并行执行的输出分布聚合起来，并计算它们的加权和，然后解码以产生最终的预测标记，从而增强了模型对上下文的认知。

#### 实现与部署
根据论文的广泛实验结果，使用Attention Buckets增强的7B参数开源模型在广泛认可的工具使用基准上达到了与GPT-4相当的最新状态水平。这一结果证明了作者所提方法的有效性。

#### 总结
该论文针对LLMs在工具使用时对上下文认知的不足提出了Attention Buckets方法，通过处理不同的RoPE角度基础来强化对上下文的关注，显著提升了LLMs在工具使用任务的性能。