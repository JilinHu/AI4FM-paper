#### 背景
- **背景**       
    论文指出，大型语言模型（LLMs）作为研究领域的重要性不断提升，随着它们能力的增强，评估它们的性能和了解它们的局限性变得日益关键。然而，传统针对生成模型的评估指标（如BLEU、ROUGE等）通常只能捕捉模型能力的一部分或几部分。

- **已有的工作**
    尽管近期研究开始从更加综合的视角探索LLMs的评估，但这些工作主要聚焦于评估"what"和"where"，即评估时应该交给LLMs哪些任务和它们应对的知识类型。而关于评估中的第三个问题"How to evaluate"——关于使用哪些标准、评估类型、评分方式和排名系统，目前讨论较少。

#### 核心贡献
- **提出了一个新的评估数据集LLMEval并对20个LLMs进行了评估**
    - **挑战1：应该考虑哪些评估标准来评估LLMs？**
        论文对五个标准进行了比较：准确性、信息量、流利性、逻辑一致性和无害性。结果显示，在不同的评估标准中，现有的LLMs在无害性方面都显示出值得注意的性能。区分因素在于信息量和准确性的指标。

    - **挑战2：应使用哪种注释方法来注释LLMs的输出？**
        论文使用了现场众包与公共注释者组合手工注释，并结合GPT-4进行自动评估。实验表明，现场评估在手动评估中展示了更高的准确性和一致性，同时发现现场注释者与GPT-4之间有更高的一致性级别。

#### 实现与部署
在评估中，共有2186名个体参与，产生了243337个手动注释和57511个自动评估结果。论文进行了不同设置的比较和分析，并得出了10个结论，为未来的LLM评估提供了洞见。此外，论文还比较了竞技运动中常用的两种排名系统：象棋游戏中使用的Elo评分系统和足球比赛中使用的积分评分系统。论文发现Elo评级系统在LLM评估任务中表现出不稳定性，当考虑不同的比赛序列时，结果显示出显著方差，并且对手动注释难以避免的噪声数据高度敏感。所有注释数据将在匿名期结束后发布到Github。

#### 总结
论文针对如何评估大型语言模型（LLMs），对多种评估标准、不同类型的评估者、评分方法和排名系统进行了比较和分析，提出了新的评估数据集LLMEval，对20个LLMs进行了评估，生成了大量的手动和自动评估结果。该研究为未来的LLM评估提供了有益的洞见和结论。