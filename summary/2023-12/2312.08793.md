#### 背景
- **背景**       
    论文探讨了大型语言模型（LLMs）在面对相互竞争的压力（例如有用性与无害性）时如何解决冲突，以Llama-2-chat 模型为研究对象试验 "禁止事实"任务。

- **已有的工作**
    已有工作主要集中在描述一个具有独特任务的电路，但现实生活中常常会遇到目标冲突。现有的研究手段未能解决模型如何在目标之间进行权衡的问题。

#### 核心贡献
- **提出了一个禁止事实任务**
    - **挑战1：如何评估模型在竞争目标下的行为**
        通过构建数据集并筛选Llama-2可以正确回答的事实，以此来测量禁止正确答案对模型行为的影响。
        
    - **挑战2：如何解构Llama-2以理解其抑制行为**
        将Llama-2分解为1000多个组件，并通过首次修补法给每个组件赋予重要性等级。

#### 实现与部署
利用构建的'禁止事实数据集'对模型进行了测试。测试结果表明，平均来看，禁止正确答案比禁止错误答案降低了超过1000倍的正确答案出现的概率。另外，通过分解和排序Llama-2的1057个组件，发现仅约35个组件足以可靠地实现完全的抑制行为。这些组件存在着较大异质性，许多组件操作基于错误启发式规则。作者还发现了一种通过手工设计的对抗性攻击（称为"加州攻击"）可以利用这些启发式规则。这些发现凸显了成功解释先进机器学习系统所面临的一些障碍。

#### 总结
这篇论文通过研究模型在禁止事实任务下的行为，解析了Llama-2-chat模型如何处理相互竞争的目标，并对它的分析提出了新的手法。