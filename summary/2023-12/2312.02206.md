#### 背景
- **背景**       
    文章讨论了长篇问答系统中如何更好地模拟人类偏好，指出了现有数据不能充分捕获人类偏好微妙差异的问题。

- **已有的工作**
    现有工作无法充分捕获人类在长篇问答中对答案的偏好细微差异，因为现有的模型训练方法常常忽略了这一点。

#### 核心贡献
- **提出了一个基于公理的框架**
    - **挑战1：捕获人类偏好的细微差异**
        提出的公理化框架可以生成或增强训练对，更好地反映人类的实际偏好，即使在现有数据中这些细微差异未必体现出来。

    - **挑战2：在同一标准上评分人类和LLM生成的答案**
        训练了独立的偏好模型（PM），参数规模介于2.2亿至70亿，能够在相同的标准上对人类和LLM生成的答案进行评分，同时消除文本长度和风格等杂讯。这比仅基于人类点赞进行训练要好。

#### 实现与部署
根据文献，实验结果表明训练时纳入适当的公理化信号能够提升偏好模型(PM)与人类点赞和专业人类评注者的一致性，甚至超过了GPT-4的能力。这暗示对于偏好打分来说，GPT-4可能是过度设计的。

#### 总结
本文提出的基于公理的框架为长篇问答偏好模型提供了一种新方法，通过细致审视人类偏好，并优化了偏好打分的准确性与效率。