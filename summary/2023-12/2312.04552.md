#### 背景
- **背景**       
论文提出了一个名为"生成插图说明"的新任务，即根据用户需要定制的视觉指令。这个任务需要生成一系列图像和文本，共同描述如何实现一个目标。

- **已有的工作**
对于学习和生成指令性内容，现有的工作包括在WikiHow等网站上学习的文本领域的语言模型，这些工作在汇总、常识程序知识、问答和层次化推理等任务上取得了进展。多模态环境中，学习多模态数据的表示已经被证明是零拍摄识别、文本图像和文本视频检索、时间段划分、活动定位和预期等多个任务中的一个强大信号源。但现有的文本到图像（T2I）模型无法直接从用户查询中生成视觉效果，即使提供详细的指令性文本，现有的T2I模型也不能生成对目标、步骤均忠实且彼此一致的图像。

#### 核心贡献
- **提出了一个名为StackedDiffusion的新模型**
    - **挑战1：生成一致且有效的指示性视觉效果**
        现有T2I模型无法直接根据用户查询生成视觉效果。StackedDiffusion模型结合了预训练的大型语言模型和微调的文本到图像扩散模型，采用一些技术（如空间平铺、文本嵌入串联以减少长指令信息丢失以及新的“步骤定位编码”）来生成完整的指导性文章和一系列图像，这些图像独特地描述了这些步骤。这些技术使得StackedDiffusion能够在有限的指导性数据训练下，生成有用的插图。

    - **挑战2：超越现有方法的性能**
        与各种基于即用工具的基线方法相比较，包括最近推出的多模态语言模型，StackedDiffusion在自动评估和人工评估中都显示出显著的性能优势。模型甚至在30%的情况下超越了人类生成的文章，显示了该方法的巨大潜力。

#### 实现与部署
StackedDiffusion模型对WikiHow等网站的指导性图像栈进行了训练，生成了完整的教学文章，并为这些文章生成了独特的一系列图像。模型利用了大规模预训练的大型语言模型和微调的文本到图像扩散模型，并采用了技术手段来完成这一任务，而无需引入任何新的可学习参数。模型的实现包括空间平铺来同时生成多张图像、文本嵌入串联以减少长指令中的信息丢失，以及新的“步骤位置编码”来更好地区分说明中的不同步骤。在进行全面的消融实验和人类评估后，结果表明StackedDiffusion模型在所有指标上均表现出色，并且相较于现有方法，人类评估者更偏爱该模型，甚至在一些情况下超越了真实图像。

#### 总结
本论文介绍了一种名为StackedDiffusion的新方法，用于生成插图说明，这是一种将文本和图像结合起来描述如何实现某一目标的任务。该方法通过结合大型语言模型和文本到图像扩散模型，并引入一些新颖的建模技巧，解决了现有T2I模型无法直接从用户查询中生成视觉效果的问题，并在人类评估中超越了现有技术水平。