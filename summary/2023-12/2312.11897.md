#### 背景
- **背景**       
    近年来，视觉-语言模型（VLMs）的发展呈指数级增长。这些模型预训练时规模越来越大，不仅参数数量增加，训练集的大小也持续扩张，使得它们在多个任务上不断推动着技术的最前沿。

- **已有的工作**
    已有的视觉采样技术因视频数据源的高度冗余而面临挑战，通常使用预处理模块或多模态注意力机制来指导模型选择视频中的相关部分。而视频语言模型则通过特征重采样革新了计算机视觉领域。不过，这些模型通常只能处理有限数量的视频帧，例如一些模型最多只能处理8帧。

#### 核心贡献
- **提出了一个文本条件重采样器（TCR）**
    - **挑战1：处理长视频序列**
        TCR通过桥接已经预训练的视觉和语言模型来解决长视频序列处理中存在的挑战，其训练方法使之能够处理与任务条件化的长视频。设计了一个基于变换器的采样架构，能够选取最相关的帧特征传递给大型语言模型(LLM)。
  
    - **挑战2：长视频的时空理解**
        随着大规模长形态视频的数据集发布，如EGO4D，研究人员开始关注长视频内容的时空理解。解决这一挑战需要开发新模型和基准测试，例如EgoSchema数据集，它是首个真正的长视频问答尝试，因为每个问题的回答都需要观看3分钟的视频。
        
#### 实现与部署
TCR的设计轻量并且采用了交叉注意力机制，能够一次处理超过100帧视频，这允许模型使用比先前工作更长的视频片段。在多种评估任务上的实证验证表明，TCR在NextQA、EgoSchema和EGO4D-LTA挑战中设定了新的最佳性能状态。此外，TCR还揭示了需要更长视频上下文的任务，从而可以有效地用于对长程视频模型的进一步评估。

#### 总结
本论文提出了一个名为TCR的新型架构及预训练方法，能够处理与文本条件相结合的长视频。它有效地桥接了预训练的视觉编码器和LLM，实现了长期视频理解的问题，并在多个评估任务上取得了最佳性能。