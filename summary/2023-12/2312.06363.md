#### 背景
- **背景**       
    论文介绍了大型语言模型（LLMs）的最新发展，重点关注增长模型规模如何增强模型的语言理解能力。与此同时，研究者们尝试通过融合额外的模态如图像和视频，发展多模态大型语言模型（MM-LLMs）。

- **已有的工作**
    现有研究领域的一个主要进展是在上下文中学习（In-Context Learning，ICL），它指的是基于上下文中的少数示例进行学习和预测的能力，而不更新任何LLM的参数。尽管ICL可以给MM-LLMs带来显著的性能提升，但其改进程度仍然落后于针对下游任务的微调。

#### 核心贡献
- **提出了一个称为多模态上下文调整（MMICT）的新范式**
    - **挑战1：结合ICL和微调以增强MM-LLM的性能**
        现有方法中，ICL虽然增加了模型的性能，但是与在下游任务上进行微调相比，依旧有较大差距。研究人员看到这一点，提出了结合这两种学习范式的可能性，以此达到进一步增强在下游多模态任务上微调性能的效果。

    - **挑战2：设计一个能够捕获不同输入和目标的多模态特征的单一模块**
        论文介绍了一个名为“多模态中心”（M-Hub）的统一模块，这是一项能够根据不同的输入和目标捕获多种多模态特征的创新。这个模块使MM-LLMs能够从上下文中的视觉引导的文本特征中学习，并随后基于文本引导的视觉特征生成输出。M-Hub的灵活性还让研究人员设计了多种上下文示范。

#### 实现与部署
多次广泛的实验表明，MMICT在一系列多模态下游任务上显著优于传统的微调策略和直接输入不同模态信息混合的原始ICT方法。MMICT通过在微调时学习示例中的视觉引导文本特征，进一步在视觉输入和文本指导下预测文本标签。通过对多种示范格式的探索，研究发现了一些有趣的关键发现，并为该领域未来的研究指明了方向。

#### 总结
这项工作通过提出MMICT，展示了在大型多模态语言模型上运用上下文学习能力以增强微调性能的新范式。通过设计M-Hub这一多功能模块并通过各种上下文示范实验，研究揭示了上下文学习在改善多模态任务性能中的潜力。