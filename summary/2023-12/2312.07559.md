#### 背景
- **背景**       
    此论文探讨了大型语言模型（Large Language Models, LLMs）作为科学研究代理的应用能力，即通过检索相关文献回答问题的能力。当前基准测试主要局限于评估大型语言模型根据已知或广泛可用的信息回答特定上下文中的问题。对于需要基于检索信息回答的问题，这些基准测试不足以进行评估。因此，本研究引入了专门用于评估检索增强型代理PaperQA的LitQA数据集。

- **已有的工作**
    现有工作未能充分评估代理的能力，因他们的问题基于模型可能已经在训练中接触到的信息，而没有特别设计来测试模型检索和基于检索信息生成答案的能力。

#### 核心贡献
- **提出了PaperQA**
    - **挑战1：数据集设计**
        现有的语言模型训练数据集通常包含旧的或广泛可用的信息，可能已经被模型在训练中看到。为应对这个挑战，LitQA数据集由专家组成，包含50个多项选择题目，所有问题均来自生物医学领域并且是从2021年9月之后的文献中选取的，所选问题难以或不可能在不检索相关论文的情况下回答。

    - **挑战2：性能评估**
        需要一个能够基于检索信息精准回答问题和体现在不确定时能表达不确定性的度量。PaperQA与现有商业工具及人类专家的性能进行了比较，在LitQA数据集上展示了它的重要性，并且调查了它检索相关论文的能力和幻觉引用的比率。

#### 实现与部署
PaperQA的所有实验都在LangChain代理框架内实现。PaperQA运用了四个不同的LLM实例——作为代理的LLM使用GPT-4，总结用的LLM使用GPT-3.5，回答问题的LLM使用GPT-4，提问的LLM也使用GPT-4。作为搜索引擎，它使用Google Scholar收集前5篇可获取的论文。评估结果表明，PaperQA在LitQA数据集上优于其他LLM、AutoGPT代理和使用检索增强生成网络（RAG）的商业产品，并且在人类专家水平之上。此外，PaperQA的不正确回答率低于所有工具，并且与人类不确定性表达相匹配，强调PaperQA在实际不确定时更好地表达了不确定性。

#### 总结
该论文提出了PaperQA，一个基于检索的生成型代理，用于科学研究。PaperQA可以准确回答基于最新科学文献的问题，并且与人类专家的回答相当，甚至在某些方面表现更好。论文展示了PaperQA的有效性，并通过与人类专家和其他商业工具的对比，证明了其优越性。