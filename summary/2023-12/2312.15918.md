#### 背景
- **背景**       
    论文指出，大型语言模型（Large Language Models, LLMs）通过提示工程（prompt engineering）展现出不断发展的上下文内学习（In-context Learning, ICL）能力。然而，如何在自然语言理解（Natural Language Understanding, NLU）和问答（Question Answering, QA）任务中提高LLMs的泛化能力和事实性，仍然是一个未被充分探索的挑战。

- **已有的工作**
    目前的研究主要集中在如何增强模型遵循用户的具体指令和质量期望，并避免不必要的输出，但几乎没有工作探索在推理阶段使用特定任务微调的语言模型（SLMs）来改进LLMs的上下文内学习的方法。

#### 核心贡献
- **提出了一个名为SuperContext的框架**
    - **挑战1：如何改善LLMs在处理分布外（Out-Of-Distribution, OOD）数据时的泛化能力**
        SuperContext框架整合了SLMs输出的监督知识以增强LLMs的可靠性，具体体现在提升模型对OOD数据的泛化能力方面。它通过把微调模型的预测结果和置信度作为补充信息加入LLMs的提示中，实现了在不同任务的OOD设置中的性能提升。
        
    - **挑战2：最小化生成任务中的幻觉（hallucination）现象**
        该框架还聚焦于通过SLMs加强LLMs的准确性，以减少生成任务中的幻觉现象。在涉及无法回答问题的QA任务中，SuperContext展示了通过增强的判别模型来减少幻觉的能力。

#### 实现与部署
SuperContext在NLU和QA任务的零样本（zero-shot）和少样本（few-shot）设置中进行了实验验证。在综合基准测试GLUE-X和QA数据集SQuAD 2.0上的实证结果显示，该方法在9个不同任务的OOD设置中，相比LLMs和SLMs都显示出显著的性能提升。这表明了SuperContext作为系统地整合SLMs到LLM推理决策中的先驱方法，特别是在处理OOD数据和减轻幻觉方面，显著提升了LLM的性能。

#### 总结
论文提出的SuperContext框架通过利用特定任务微调的SLMs的监督知识，显著提高了LLMs在自然语言理解和问答任务中的泛化能力和事实性。它代表了将小型模型的强大功能融入LLMs，以处理分布外数据和最小化幻觉现象的一种创新做法。