#### 背景
- **背景**       
    文章介绍了在Minecraft环境中，传统的基于强化学习的代理通常依赖于稀疏奖励，这使得学习复杂任务变得充满挑战。由于奖励的极端稀疏性和决策空间的巨大复杂性，即使是通过行为克隆的预训练策略，也仍然需要数十亿次的环境互动才能有效学习。

- **已有的工作**
    其他研究者提出了一系列密集奖励信号以实现有效的学习，但其在Minecraft这种复杂且时长跨度长的任务中的适用性仍是一个开放性问题。此外，现有的密集奖励方法往往无法显著提升在Minecraft中的任务成功率。这突显了Minecraft的难度和现有方法的局限性。

#### 核心贡献
- **提出了一个名为Auto MC-Reward的学习系统**
    - **挑战1：奖励信号的稀疏性和环境与任务的复杂性**
        Auto MC-Reward使用大型语言模型（LLMs）自动设计密集的奖励函数，通过奖励设计者、奖励评论家和轨迹分析器，基于环境信息和任务描述编码执行Python函数，并通过自我验证、错误检查和反馈再迭代优化奖励函数来解决这一挑战。

    - **挑战2：现有方法的局限性**
        利用LLMs的任务理解和经验总结能力来提供即时和详细的学习指导奖励。Auto MC-Reward通过分析训练代理收集的轨迹，并自动辅助奖励设计者改进奖励函数，从而解决了现有方法的局限性。

#### 实现与部署
Auto MC-Reward展示了在复杂的Minecraft任务中，如效率地获取钻石、避开熔岩和在平原生物群落中探索树木和动物方面的显著改善。实验结果显示，与原始稀疏奖励和现存的密集奖励方法相比，Auto MC-Reward在这些任务上达到了更好的结果，展现了其在提升稀疏奖励任务学习效率方面的先进能力。Auto MC-Reward能够让智能代理有效地学习对应任务有益的新行为，例如避免熔岩，这大大提高了任务的成功率。此外，Auto MC-Reward还实现了36.5%的成功率仅使用原始信息挖掘钻石的高成就。

#### 总结
Auto MC-Reward是一种先进的学习系统，利用LLMs以自动方式设计针对Minecraft任务的密集型奖励，通过LLMs的理解和经验总结能力，有效地提高了代理在复杂环境中学习新行为和完成长期任务的能力。