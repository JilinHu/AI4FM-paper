#### 背景
- **背景**       
    论文提出，虽然强化学习（RL）在开发能够通过环境互动来发展决策能力的人工智能（AI）智能体方面至关重要，但直接将感知映射到行动的单一RL策略面临着诸多问题，尤其是它在多任务中的通用性不足以及对大量训练数据的需求。

- **已有的工作**
    目前的RL方法使用单一映射函数定义策略在复杂环境中通常显得不足。此外，RL引入的先验通常是任务特定的，需要大量的工程化和领域专业知识。尽管近期研究转向将大型语言模型（LLMs）集成进智能体框架，但现有框架通常假设固定的推理结构，且往往缺乏针对新行为和技能的微调能力。

#### 核心贡献
- **提出了一个名为Pangu-Agent的框架**
    - **挑战1：缺乏结构性推理**
        论文认为，直接从感知中学习出行动的标准强化学习RL流程可能在任务跨度扩展时成为一个重大瓶颈，因为标准梯度提供的监督可能不足以支持所有深层网络体系结构。为了解决这个问题，研究者引入了内在函数（intrinsic functions），使得AI智能体的内部思考过程被形式化为结构性推理。引入这些内在函数可以重构典型RL目标，以支持多个“思考”阶段。

    - **挑战2：缺乏微调能力**
        旧有模型在面对超出预训练LLM能力范围的任务时存在风险。针对这一点，Pangu-Agent 旨在使智能体能够通过监督学习和RL进行微调。论文展示了如何利用内在函数进行微调，从而使智能体能够学习并适应新环境。 

#### 实现与部署
Pangu-Agent 引入内在和外在函数，添加对推理结构的先前理解，并提供了能够根据认知过程的模块结构在每个模块或函数内部学习模型的自适应能力。通过执行一系列实验，文章探讨了Pangu-Agent框架的实际应用，并展示了这种方法的有效性。实验结果表明，当嵌入了组织化推理和先验知识时，AI智能体的表现和适应能力大大提升，这为更具韧性和通用性的AI智能体系统敞开了大门。论文还进行了与现有AI管道和框架的深入比较。

#### 总结
本论文提出了Pangu-Agent框架，目标是解决标准RL方法在多任务环境中所面临的挑战。Pangu-Agent通过内在函数引入结构性推理，并通过监督学习和RL实现智能体的微调，提高了智能体适应多环境交互的能力。