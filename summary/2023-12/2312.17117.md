#### 背景
- **背景**       
    论文介绍了在长视频中进行时序句子定位（TSG）的问题，这是一个需要模型理解自然语言查询并定位视频中对应时刻的任务。传统的针对短视频的TSG方法在长视频中遇到了挑战，主要包括复杂情境下的时序推理和含有文本语言的多模态信息处理。

- **已有的工作**
    现有TSG方法在长视频数据集上需要高计算成本才能进行训练，并且通常存在特定数据集拟合偏差，导致泛化能力不足。此外，现有方法未能捕捉到文本语音中的丰富语义，使得它们难以处理含有大量对话的长时间视频。尝试利用多模态大型语言模型（MLLM-V）实现的方法虽取得一些进步，但仍未能很好地将TSG任务与大型语言模型（LLM）对齐，因此在长视频的时序推理能力方面表现不佳。

#### 核心贡献
- **提出了一个名为 Grounding-Prompter 的方法**
    - **挑战1：在长视频中的时序推理与多模态理解**
        论文提出将TSG任务转化为长文本任务，并通过使用压缩任务文本化以及界限感知引导策略来加强LLM在复杂背景下的时序推理能力。

    - **挑战2：高效稳健的TSG性能**
        该方法通过设计多尺度去噪链式思考（CoT）、设置合理性原则和引入单次示例中的学习（ICL）来约束LLM，以便产生合理的预测，并增强模型对TSG任务的理解和时序推理能力。

#### 实现与部署
实验结果表明，Grounding-Prompter在VidChapters-mini数据集上取得了最先进的性能，与其他基线方法相比有明显的优势。通过消融研究进一步验证了该方法的有效性，证明了该策略从文本语音和视觉模式中获益，能够处理长视频中复杂和嘈杂的内容。

#### 总结
本论文提出了Grounding-Prompter方法，针对长视频中的TSG问题，将LLM与时序推理和多模态信息结合起来，证明了通过多模态提示LLM的有效性，并通过实验验证了其在长视频TSG任务中的优越性。