#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）使用基于语言的行为评估来评估其认知能力的方法论考虑。作者通过三个文献案例研究（一个常识知识基准测试、一个心理理论评估和一个句法协议测试），描述了在将认知测试应用于LLM时可能出现的常见障碍。

- **已有的工作**
    尽管对LLMs进行认知测试在技术上看似直接，但结果的解释却并非如此。当前AI心理学领域面临许多方法论问题，比如考虑哪些因素来评估模型性能、如何调整刺激物以减少模型可能使用的“黑客”即启发式策略来获得高任务性能而不需使用被评估的认知技能等。作者对这些问题提出了见解。

#### 核心贡献
- **提出了一个对大型语言模型进行认知评估的方法论**
    - **挑战1：解释结果**
        许多论文评估了LLMs的各种认知技能和特征，例如个性特征、工作记忆能力、逻辑推理、计划能力、社会推理和创造力等。然而，尽管这些评估的执行相对简单，但解释结果却不是。作者探讨了如何正确解读这些评估结果的问题。

    - **挑战2：避免方法论上的问题**
        作者将首先描述三个案例研究，这些案例凸显了在评估LLMs时需要注意的一些问题，然后提供一个关于运行认知评估的DO's和DON'Ts的（非全面的）列表。最后，作者将强调一些DO's和DON'Ts仍在形成中的讨论区域，如提示灵敏度、文化与语言多样性、使用LLMs作为研究助手，以及对开放式和闭合式LLMs进行评估的比较。

#### 实现与部署
作者并未讨论具体的实现和部署，但是总结了认知评估的指南和开放讨论的领域，并对如何更好地对LLMs进行评估做出建议。具体的评估结果和与相关工作的比较将体现在后续部分的案例研究中。

#### 总结
这篇论文为大型语言模型的认知评估研究方法提供了指导性的建议，探讨了在方法论上如何避免在运行认知评估时可能出现的问题。论文的目标是贡献于AI心理学领域最佳实践的更广泛讨论。