#### 背景
- **背景**       
    文章介绍了多模态大型语言模型（MLLMs）最近在多模态理解、推理和交互方面表现出的强大能力。然而，现有的MLLMs普遍存在严重的幻觉问题：产生的文本并不真实地基于相关图片，甚至包括描述不存在的视觉内容和描述错误，使得这些模型在现实世界中成为不可信赖且不切实际的应用。

- **已有的工作**
    现有MLLMs的指导性微调（instruction tuning）没有包括正负人类反馈，使得模型难以学习精确的行为边界来排除幻觉。传统的强化学习人类反馈（RLHF）方法在语言模型中的运用虽然涉及人类评注者对模型回应的排序，但面临标注模糊和学习效率两大挑战。人类标注者在定位优先响应方面面临困境，由于语言复杂性和回应的变化，令到需要大量标注数据来学习期望行为，这样可能导致错误的信用分配以至于行为退化。

#### 核心贡献
- **提出了一个名为RLHF-V的框架**
    - **挑战1：幻觉问题**
        RLHF-V框架通过从人类反馈中学习来对齐MLLM行为。数据层面，提出了收集细粒度段落级校正的人类反馈，通过让人类注解者直接纠正模型响应中的幻觉段落，提供清晰、密集和细粒度的人类偏好以及最优响应，以此策略避免了语言差异和非稳健偏见，确保反馈准确分配到期望的行为，从而提高学习效率和防止奖励黑客问题。

    - **挑战2：学习效率**
        方法层面，提出了密集直接偏好优化（Dense Direct Preference Optimization, DDPO），这是DPO的一种新变体，以一种等效简单高效的监督方式解决传统RLHF的目标。DDPO直接面对细粒度段落级偏好对政策模型进行优化，让幻觉段落接受更强的反馈以事实为基础。

#### 实现与部署
在五个基准测试中的全面实验显示，RLHF-V可以显著提高MLLMs的可信度，同时保证有希望的数据和计算效率。使用1.4k个首选数据，RLHF-V将基础MLLM的对象幻觉率降低了34.8%，超过了训练有10k首选数据的同期LLaVA-RLHF。RLHF-V在防止过度泛化引起的幻觉方面，表现出比强大的GPT-4V更好的鲁棒性。

#### 总结
RLHF-V是一个通过细粒度校正型人类反馈校正MLLM行为的新框架，通过收集高质量的人类偏好数据为MLLMs提供人类对齐的学习信号，并通过全面的实验验证了该框架的有效性。该研究可能在提高大型多模态语言模型在各种任务中的可靠性和实用性方面取得重要进展。