#### 背景
- **背景**       
    论文介绍了大型语言模型（LLMs）在生成回应时可能包含不准确或虚假信息，这种现象通常称为“幻觉”。这可能限制了LLMs在实际场景中的应用。

- **已有的工作**
    现有的研究工作中，提出了不同的方法来减少LLMs的幻觉现象，包括选择高质量的训练数据、源自外部反馈的强化学习、检索增强生成和模型不确定性的利用。但这些方法的共同挑战在于优化LLMs去减产生成幻觉，这是一个困难的目标。

#### 核心贡献
- **提出了一个简单的诱导-然后对比解码（ICD）策略**
    - **挑战1：如何有效减少幻觉**
        论文首先通过从原始LLMs诱导幻觉来构建一个事实上较弱的LLM。然后，在解码过程中惩罚这些诱导出来的幻觉，通过对比解码来增强生成内容的事实性。这种方法是通过放大来自原始模型的预测，并通过对比解码来降低错误预测。

    - **挑战2：如何在不改变现有训练目标的前提下解决问题**
        论文中指出，以最大似然为基础的下一令牌预测可能是LLMs幻觉的原因之一。然而通常情况下不改变这个训练目标，考虑到其它的良好特性如简单性和泛化能力。论文的ICD方法不需要改变训练目标，而是通过在现有LLMs的基础上诱发出可比较的带有幻觉倾向的弱模型，然后在生成阶段减去这些幻觉信息。

#### 实现与部署
论文通过在诸如TruthfulQA和FACTSCORE等基于歧视的和基于生成的幻觉评估基准上的实验结果表明，提出的ICD方法可以有效地提高LLMs的事实性。例如，配备了ICD的Llama2-7B-Chat和Mistral-7B-Instruct在TruthfulQA上的表现与ChatGPT和GPT4相当。此外，在FACTSCORE的文本生成中，ICD允许Llama2-7B-Chat在事实精度方面超越其70B模型对手。

#### 总结
论文提出一个新颖的减少LLMs幻觉的方法，通过构建一个事实上较弱的LLM并在生成过程中减去其知识，改进了模型在生成事实性内容方面的表现。