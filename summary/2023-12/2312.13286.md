#### 背景
- **背景**       
    论文指出，现有的多模态系统在模仿人类通过少量示例或简单指令即可轻松解决多模态任务方面存在挑战。

- **已有的工作**
    当前的多模态模型无法像人类那样通过上下文解决任务，通常依赖于特定任务的体系架构和大规模监督训练集。

#### 核心贡献
- **提出了一个名为 Emu2 的多模态生成模型**
    - **挑战1：如何增强多模态模型的上下文学习能力并有效地推广到未见过的任务**
        通过扩大模型规模，训练了一个有 37 亿参数的 Emu2 模型，并展示了通过使用统一的自回归目标（预测下一个多模态元素）在多模态序列上的有效预训练，这个模型能够显著提高上下文学习能力，包括即时推理、视觉提示和物体基础的生成等能力。

    - **挑战2：为特定指令优化并微调模型以执行挑战性的任务**
        Emu2 在经过针对具体指令的微调后，能够处理交错的文本-图像-视频输入和输出，使其成为执行多种多模态任务的强大和多功能的基础模型，并在若干视觉问答数据集中展示了前沿的少样本性能。

#### 实现与部署
Emu2 在多个视觉语言任务中的少样本设置中取得了先进的成绩，并在视觉问答任务中取得了最先进的结果，超越了以前更复杂设计的模型。此外，Emu2 可以被微调成为一个高质量的可控视觉生成模型，能够接受文本、位置和图像的混合作为条件，并生成精确的图像。这表明，随着上下文中示例数量的增加，模型的表现也会有所改善。

#### 总结
本论文通过扩大模型规模，成功提升了多模态生成模型 Emu2 在上下文学习能力上的表现，并在一系列多模态理解任务中取得了突破性的效果，尤其在基于指令微调后的视觉问答和可控视觉生成方面。