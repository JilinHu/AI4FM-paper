#### 背景
- **背景**       
    论文概述了LLMs在NLP领域特别是代码生成方向的突破，提出了一个挑战：如何平衡有效的测试用例生成与代码片段生成，因为两者往往难以兼顾。

- **已有的工作**
    针对代码生成效果的增强，已有工作如Self-Edit和CodeCoT提出了自我完善优化的路径，例如利用用户编写的测试用例来验证和提升生成代码的准确性。但这些方法存在局限性，特别是涉及对测试用例的依赖，这对用户来说可能是一个问题，尤其是对于非领域专家而言。此外，这些方法往往是在代码生成和测试用例生成过程中进行平衡，可能会影响到测试效果的独立性和有效性。

#### 核心贡献
- **提出了一个AgentCoder**
    - **挑战1：代码生成与测试用例生成间的平衡**
        论文提出了AgentCoder框架，通过创建三个相互协同的智能体（程序员智能体、测试设计智能体和测试执行智能体）来解决代码生成与测试用例生成间相互影响的问题。通过这种多智能体系统，能够确保强大的代码生成能力，并解决了传统方法的局限性。

    - **挑战2：代码生成的质量与迭代优化**
        程序员智能体采用连贯思考（Chain-of-Thought）的方法在生成代码的同时进行迭代优化，这种方法既能生成初始代码片段，又能在得到测试执行智能体反馈后进行自我修正，以确保最终代码片段满足所有测试用例要求。

#### 实现与部署
AgentCoder的评估表明，其生成的代码质量超过了现有的SOTA方法。例如，在HumanEval-ET和MBPP-ET数据集上，AgentCoder分别获得了77.4%和89.1%的通过率，超过了现有最佳模型分别只有69.5%和63.0%的通过率。这些成果证实了AgentCoder解决提出的挑战的有效性，同时它的模块化设计可确保模型适应性和可扩展性，允许将来的增强和与更高级模型的集成。

#### 总结
本论文提出了一个新颖的基于多智能体的代码生成解决方案AgentCoder，通过特定的智能体聚焦于代码生成、测试设计和测试执行，有效地解决了代码生成与测试之间的平衡问题，并实现了优于现有SOTA方法的代码生成质量。