#### 背景
- **背景**       
    论文介绍了在压缩大型语言模型（LLMs）时，模型会变得更高效，占用更小的内存，并能够在本地部署。可是，这种压缩行为会与模型的质量之间形成一种权衡。以往的研究主要集中在使用如困惑度（perplexity）或下游任务准确率等通用指标来衡量压缩效果，但这些研究往往忽略了更为细微的指标，例如衡量模型的参数知识。为弥补这一差距，作者对不同的模型家族（编码器（ENCODER），编码器-解码器（ENCODER-DECODER）和解码器（DECODER））进行了全面分析，使用LAMA和LM-HARNESS基准来量化常用压缩技术对模型性能的影响。

- **已有的工作**
    现有的研究对于模型的压缩通常只考虑单一模型或特定类型的模型，并且量化指标太过通用，而与实际任务指标相去甚远。此外，大部分压缩研究着重于解码器（DECODER）模型，对于大型编码器和编码器-解码器模型的研究还远远不够。

#### 核心贡献
- **提出了一个对LLMs压缩对参数知识影响的全面分析**
    - **挑战1：保存参数知识的重要性**
        本文特别强调了对参数知识即在预训练中获得并储存在模型权重中的知识的保存的重要性。这对于涉及推理的任务和专门的应用至关重要。文章考察了不同的压缩方案对参数知识的影响，包括修剪（pruning）和量化（quantization）技术，以及这些技术在下游推理任务上的表现。

    - **挑战2：对多种模型家族的综合分析**
        通过对多个模型家族应用修剪和量化方法，文章分析了这些模型在不同的推理任务上的表现。这一分析突破了现有研究的局限性，为实践者提供了务实见解，助其在模型压缩中做出明智决策。

#### 实现与部署
通过综合研究，论文为LLMs提供了几项重要观察结果：当同时对所有模块进行修剪时，对参数知识的影响最显著；在修剪水平> 50%时，所有模型的参数知识急剧下降；相比量化前馈网络，量化注意力模块对所有模型的影响较小；相比结构化修剪，所有模型在最终层的非结构化修剪对性能的影响较小。这些结论基于广泛的模型家族和具体的压缩技术，为实际操作提供指导。

#### 总结
这项研究首次大规模考察了LLMs的压缩技术对模型参数知识的影响，并为实际应用提供了重要见解，特别是在关于修剪和量化技术相关的决策方面。