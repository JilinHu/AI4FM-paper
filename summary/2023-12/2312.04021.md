#### 背景
- **背景**       
    文章指出，基于变换器(Transformer)的语言模型，尽管在多种用例中能生成连贯且与上下文相关的文本并展现令人印象深刻的性能，但这些模型偶尔也会产生错误或过于自信的输出，引起了人们对模型校准性能的担忧。这一问题在采用称为“上下文内学习”(In-context learning, ICL)的新范式来构建性能强大的预测器时显得尤为紧迫，特别是在安全关键领域中的应用。

- **已有的工作**
    预先工作显示，虽然可以通过缩放技术（如温度缩放）重新校准每一步生成中的熵，但论文展示了在ICL中的校准问题不可能仅依靠这些依赖额外验证数据的传统再校准方法轻易解决。

#### 核心贡献
- **提出了一个关于ICL校准的评估和分析**
    - **挑战1：ICL的校准问题**
        论文发现，包括GPT-2和LLaMA在内的语言模型（LM）校准表现很差。随着上下文样本数量的增加，预测准确性和校准误差都会增加。特别是模型尺寸增大或使用特殊数据进行微调时（如策划指令，对话或人类偏好数据），这种校准退化会变得更加严重。通过Bayesian的视角评估这种token级别的校准能够让我们定量地测量模型感知和实际性能之间的差距。

    - **挑战2：推理任务中的原因分析与预测**
        论文深入研究了校准如何影响推理任务中的性能，尤其是通过提供解释生成解答的任务。结果表明在生成长篇上下文用于推理并解释最终答案后，校准会下降。

#### 实现与部署
论文使用了传统的自然语言理解(NLU)任务和推理问题回答任务，执行了k-shot设置的上下文内学习实验，并记录了在错误校准和重新校准后的性能和校准误差。展示了模型进行复杂推理时的准确性和预期校准误差，包括调整模型的大小和微调对模型准确性和校准误差的不同影响。在不同的实验中采用了不同的提示生成策略，探讨了这些策略对模型性能和校准的影响。发现包括标签在内可以减少不确定性并促进更有效的推理，而仅重复上下文无法带来性能改善。实验结果还显示，提示构建的多样性在提升性能特别是较大语言模型性能上有显著影响。研究发现微调（如vicuna和alpaca模型）使模型更准确但校准性变差，尤其在推理任务上显著。

#### 总结
该论文深入研究了上下文内学习(ICL)在语言模型(LMs)中的校准准确性问题，并提出了评估和分析方法。它揭示了校准误差与模型大小和微调过程中的变化关系，以及校准在推理任务生成中的降低。