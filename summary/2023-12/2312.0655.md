#### 机构&分类
None

#### 背景
- **背景**       
    该论文提出了一个名为LLM360的框架，旨在提升大型语言模型（LLM）发布的透明度和开源协作。在趋向于更封闭的LLM发布的当下，LLM360试图在维持高透明度的同时，发布现代化且高质量的模型。

- **已有的工作**
    作者指出，大型预训练模型（LLM）的研究和应用发展迅速但存在透明度低、难以复现和验证的问题。尤其是在数据训练集、训练代码、超参数配置以及模型检查点等方面，缺乏开源共享使得第三方难以评估模型性能和风险，同时也限制了学术界和小型组织在LLM研究方面的参与。

#### 核心贡献
- **提出了一个xxx**
    - **挑战1：提升LLM发布的透明度**
        作者发现现有的LLM在发布时缺少透明性，难以被复现和验证。LLM360框架提倡发布预训练使用的数据集，以及完整的训练参数和模型结构的细节，提供了所有的训练代码、超参数、配置和模型检查点，以及训练期间的度量和统计数据。

    - **挑战2：支持开源协作研究**
        现有的LLM通常缺乏共享部分或全周期的模型状态，LLM360针对每个训练阶段发布了涵盖模型权重和优化器状态的检查点以及与数据块和评估指标相对应的完整度量数据，支持从任意点继续训练，便于在不同训练情境下进行研究，公开了项目日志和评估代码以增进可复现性。

#### 实现与部署
在实现和部署方面，LLM360项目包括了两个新预训练的LLM——AMBER和CRYSTALCODER。它们分别在1.25万亿和1.4万亿tokens的数据集上训练，并使用了混合精度训练策略。此外，LLM360提供了丰富的评估规范和训练日志，以及内存动态跟踪等信息，使研究者可以不必亲自进行LLM预训练就能理解训练过程中所发生的现象。最终，AMBER和CRYSTALCODER在Open LLM Leaderboard上的表现与其他类似时间开发的LLM（如LLaMA等）相比具有竞争力，尽管有些方面（如ARC评估）仍稍逊一筹。项目发现的问题也在之后的LLM预训练努力中得到了解决，比如通过数据清洗和混合、细分数据类别调度等策略提高预训练的质量。

#### 总结
LLM360项目通过提供全面公开透明的预训练LLM框架，旨在促进LLM研究的开源协作和可复现性，特别是对于那些资源有限的小型研究组织。它为后续研究提供了完整的预训练模型、训练代码、评估工具和部署指南，有助于推动学术共享和知识共享，让研究者更深入地了解和利用LLM。

#### Institution&Category
None

#### Background
- **Background**
This paper introduces LLM360, a framework for releasing LLMs that encourages open-source transparency, reproducibility, data/model provenance, and collaborative research. In a trend towards more closed LLM releases, LLM360 aims to deliver both modern and high-quality models while maintaining a high degree of release transparency.

- **Existing Work**
The authors point out that the rapid development of large pre-trained models (LLMs) in research and application has faced the issues of low transparency, difficulties in reproduction and validation. Especially in terms of training datasets, training code, hyperparameter configurations, and model checkpoints, the lack of open sourcing makes it difficult for third parties to assess the models' performance and risks, which restricts participation in LLM research by the academic community and small organizations.

#### Core Contributions
- **Introduced a framework for LLM transparency**
  - **Challenge 1: Enhancing transparency in LLM releases**
    The authors found existing LLMs lack transparency and are difficult to replicate and validate. The LLM360 framework advocates for the public release of the datasets used for pretraining, as well as the complete details of training parameters and model structure, providing all training code, hyperparameters, configurations, and model checkpoints, as well as metrics and statistics collected during training.

  - **Challenge 2: Enabling open collaborative research**
    Current LLMs often lack sharing of partial or full-cycle model states, whereas LLM360 releases checkpoints including model weights and optimizer states and full metrics corresponding to data chunks and evaluation metrics for each training stage, supporting continued training from any point and facilitating research in various training contexts by opening project logs and evaluation code to enhance reproducibility.

#### Implementation and Deployment
In terms of implementation and deployment, the LLM360 project includes two new pretrained LLMs—AMBER and CRYSTALCODER. They have been trained on datasets of 1.25 trillion and 1.4 trillion tokens, respectively, and utilized a mixed precision training strategy. Additionally, LLM360 offers a wide range of evaluation standards and training logs, along with memory dynamic tracking, enabling researchers to understand what happens during LLM pretraining without doing it themselves. Ultimately, AMBER and CRYSTALCODER show competitive performances on the Open LLM Leaderboard compared to other LLMs developed around the same time, such as LLaMA, although they still lag in some assessments (such as ARC). Issues discovered in the project were subsequently addressed in later LLM pretraining efforts, such as the improvement of pretraining quality via strategies like data cleansing and mixing, and scheduling of various data category chunks.

#### Summary
The LLM360 project, by offering a comprehensive and transparent pretrained LLM framework, aims to foster open-source collaboration and reproducibility in LLM research, especially for resource-limited small research organizations. It provides complete pretrained models, training codes, evaluation tools, and deployment guidelines for subsequent research, promoting academic sharing and knowledge sharing, and enabling researchers to understand and utilize LLMs more deeply.