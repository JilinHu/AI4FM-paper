#### 背景
- **背景**       
    文章指出，大型语言模型(LLMs)通过上下文学习(ICL)展示了令人印象深刻的少样本泛化能力，但随之而来的是更大模型在复杂性和计算需求方面的显著提升，导致了前所未有的计算需求和部署挑战。

- **已有的工作**
    对此，研究人员尝试将大型模型的强大能力转移到更高效、更紧凑的小型模型上，通常通过将小型模型的输出与大型模型的输出对齐来实现。现有方法要么在大模型生成的输出上训练小模型，要么模仿它们的token级别概率分布。然而这些蒸馏方法几乎没有注意到输入部分，在 ICL 中也起着至关重要的作用。

#### 核心贡献
- **提出了一个双向对齐(BiAlign)**
    - **挑战1：输入的选择对 ICL 性能的影响**
        根据对 ICL 性能高度敏感的演示示例选择的发现，文章提出了双向对齐(BiAlign)。双向对齐不仅对输出分布进行对齐，还引入了一种新颖的排名损失来实现输入偏好之间的对齐。

#### 实现与部署
通过大量的实验和分析，研究表明，BiAlign 在包括语言理解、推理和编码在内的多种任务上，能够一致地胜过现有的基线。

#### 总结
本文通过引入新颖的排名损失以及对输出分布的对齐，提出了双向对齐(BiAlign)，有效提高了小型模型的 ICL 能力。