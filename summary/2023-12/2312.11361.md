#### 背景
- **背景**       
    论文指出检索式增强生成（Retrieval-Augmented Generation, RAG）是通过利用外部知识源来增强大语言模型（LLM）输出，从而减少事实幻觉的一个有前途的方式。然而，先前的研究缺乏对不同语言家族的全面评价，这使得对LLM在外部检索知识错误场景下的稳健性进行评估变得具有挑战性。

- **已有的工作**
    已有研究没有全面评估LLM在多语言中的推理能力，因此尚未清楚LLM及其对外部信息的利用是否能稳健地抵抗在高资源或低资源语言中进行首次检索时的错误。

#### 核心贡献
- **提出了NoMIRACL数据集**
    - **挑战1：评估LLM在不同语言的检索稳健性**
        NoMIRACL是一个人工注释的数据集，用于评估LLM在RAG中对18种语言学多样语言的稳健性。NoMIRACL包含非相关子集和相关子集，其提供了能够评估模型在非相关子集产生幻觉倾向和在相关子集识别相关通道的不准确性的度量指标。

    - **挑战2：建立GPT-4基线模型**
        使用NoMIRACL数据集，研究人员构建了一个GPT-4基线模型，用于初步评估。结果表明GPT-4在非相关子集上平均幻觉率高达33.2%，而在相关子集上错误率为14.9%，这表明提高LLM对RAG中非相关信息拒绝能力的研究具有重要性。

#### 实现与部署
NoMIRACL数据集以及GPT-4基线模型已经开源并可供获取。实验结果显示，GPT-4在面对非相关子集时倾向于将非相关通道误判为相关，且在所有测试语言中的平均幻觉率高达33.2%。而在识别具有已知答案的相关通道方面，GPT-4的平均错误率降低到14.2%，表现出较强的能力。研究还发现，GPT-4的幻觉率与语言资源大小（即维基百科语料库大小）之间存在正相关（斯皮尔曼ρ = 0.39）。

#### 总结
这项工作通过引入NoMIRACL数据集，为评估LLM在检索式增强生成中的稳健性提供了一个多语言的评估工具，并通过建立GPT-4基线模型展示了LLM在识别相关与非相关检索结果中存在的挑战，突出了未来研究提高LLM稳健性的必要性。