#### 背景
- **背景**       
    文章讨论了大型语言模型（LLMs）利用其内部知识图谱即在预训练期间学习的知识图谱进行推理的能力。提出了两个研究问题，探讨LLMs在回忆预训练知识图中的信息以及从上下文中推导出知识图关系的准确性。

- **已有的工作**
    已有的研究还没有检验LLMs利用其内部知识图推出知识的能力。主流LLM（如text-davinci-003和ChatGPT）的训练过程和数据未披露，不清楚它们是否在知识图数据上进行预训练。因此，本文调研了LLMs从内部知识图中回忆信息的能力，尤其是推导直接关系（指知识图中的一跳连接）。研究还探索了LLMs从给定文本中提取上下文信息的能力。本文解决了两个主要问题：LLMs准确回忆知识图信息的能力（RQ1），以及LLMs从上下文推断知识图关系的能力（RQ2）。

#### 核心贡献
- **提出了一个方法来探索LLMs利用内部知识图进行推理的能力**
    - **挑战1：如何准确回忆知识图中的信息**
        通过执行两个非上下文关系生成任务（尾实体预测和关系预测）来评估LLMs的知识回忆。这需要LLMs记忆并利用预训练期间学习到的知识图结构。此外，研究中定义了两种知识推理中可能出现的幻觉：内容幻觉和本体幻觉。

    - **挑战2：如何从上下文中推断知识图关系**
        通过执行两个任务（关系抽取（RE）和上下文路径生成（CPG））来生成使用LLMs的上下文关系。这是一个更具挑战性的任务，需要推断关系并在知识图中形成路径。

#### 实现与部署
根据文章，使用三个基于文本的大型语言模型（text-davinci-003、ChatGPT和GPT-4）对LLMs进行了测试。这些实验是在零射击环境下进行的，即模型在给定任务之前没有接受过特定训练。研究使用了两种评估指标，硬准确性（H-ACC）和软准确性（S-ACC），基于DBpedia的100个实体关系对随机样本进行测试，并且结果指出LLMs可以成功处理从记忆中回忆信息以及从输入上下文中推断信息的简单和复杂的知识图推理任务。

#### 总结
研究结果显示，LLMs能够通过其内部知识图成功处理知识图推理任务，并能从上下文中推断出知识图关系，展示了LLMs在知识图推理中的潜力及应用价值。