#### 背景
- **背景**       
    文章指出，凭借大型语言模型（LLMs）作为评估者，自动文本评估取得了显著进展。然而，现有的按样本进行评估的范式存在诸多问题：敏感于提示设计、抗噪声能力差，以及在静态参考下的综合性能较差。人类评估者在评估时会同时考虑标准定义和样本间比较，而现有的LLM评估者只依赖于标准。

- **已有的工作**
    由于人工评估工作量大且耗时，研究人员探索了自动化方法来补充人工评估，其中包括基于规则、基于嵌入和基于学习的评估方法。这些方法虽取得了进展，但与人类判断的一致性仍存在显著差距。而且，由于现有LLM评估者通常对每个样本单独评估，缺乏样本间的比较，导致评估结果依赖于提示设计，并且评分之间的区分度不足。

#### 核心贡献
- **提出了一个叫做BATCHEVAL的新范式**
    - **挑战1：强化提示设计的鲁棒性和评分的区分度**
        BATCHEVAL采用了批量处理的方式进行评估，通过将所有样本分成几批，然后将每批样本编译成一个提示输入给LLM。这种方法通过在批次中引入样本作为标准之外的额外参考，既可以减少对提示设计的依赖，又可以通过批内比较提高样本间评分的区分度，从而改善评估的鲁棒性。

    - **挑战2：提升集成评估中的多样性和性能**
        BATCHEVAL的迭代更新批次组合为LLM提供了多样的评估参考，增强了评估的多样性和集成性能。实验研究表明，这种方法不仅能提高评估的精准度，而且还降低了API成本。

#### 实现与部署
实验在4个文本评估任务上进行，主要使用GPT-4：对话轮次响应评估、对话文本总结和故事生成。实验结果显示，BATCHEVAL在与人类评估的皮尔逊相关系数上，超过了最佳性能的LLM评估者10.5%。通过共享单一提示和较少的迭代轮次，BATCHEVAL的API成本仅为其他方法的64%。研究还验证了BATCHEVAL在更多LLM上的泛化能力，抗提示设计和噪声的鲁棒性，并通过进一步实验分析了超参数选择，并探讨了BATCHEVAL的工作机制。

#### 总结
论文提出了一种新的LLM评估范式——BATCHEVAL，解决了自动文本评估在鲁棒性和与人类判断一致性方面的问题。通过批量评估和迭代处理，BATCHEVAL在准确性和成本效率方面显著超越了现有方法。