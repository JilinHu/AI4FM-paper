#### 背景
- **背景**       
    文章指出，大型语言模型（LLMs）在语言处理任务中显示出了转换能力，但与此同时，它们可能带来的风险及负面社会影响也引起了担忧，现有对齐技术如监督式微调（SFT）和基于人类反馈的强化学习（RLHF）在将LLMs与人类价值观和意图保持对齐方面存在挑战。为此，论文提出了一种新的实时对齐方法。

- **已有的工作**
    已有工作存在问题，例如监督式微调需要大量的计算资源，强化学习不稳定且对超参数敏感，而且高质量的人类反馈数据收集困难和成本高昂。同时，现有的对齐方法很难处理跨越时间和地点的多变的人类社会规范和价值观的内部化。

#### 核心贡献
- **提出了一个On-the-fly Preference Optimization（OPO）方法**
    - **挑战1：如何构建一个广泛覆盖、反映权威价值的规则模块？**
        这个问题通过构建一个包含特定人类价值的文本语料库的规则创建模块来解决。这个模块收集和确定法律和道德相关的规则，并作为外部记忆供LLMs 遵循。
        
    - **挑战2：如何设计灵活的评估算法？**
        为了解决现有对齐方法可能造成的基准测试泄漏问题并扩大对齐规则的测试范围，论文提出了一个可扩展的评估模块，能够基于规则自动生成新的法律和职业道德问题。

#### 实现与部署
该论文主要通过自动生成的和人类标注的文本来评估OPO方法的有效性。通过GPT-4自动生成基于给定规则的多项选择问题，并由受过良好教育的注释者审查质量，最终得到有效法律和专业道德问题的测试数据。此外，还展示了不同LLMs在五个基于来源和领域的评估数据集上的综合评估结果，其中包括封闭源变体如GPT-4和不同大小的开源模型，结果显示出论文提出的方法能够一致提高大多数LLMs在这些评估数据集上的表现。

#### 总结
该工作提出了一个动态的OPO方法，通过收集法律和道德规则作为外部存储器来限制LLMs的行为，无需进一步训练，并通过一个可扩展的评估模块来应对潜在的基准测试泄漏问题及扩大测试规则的范围。尽管该方法在推理效率方面存在局限性并且检索模型仍可进一步优化，但在多个评估数据集上的广泛实验表明了该方法的有效性。