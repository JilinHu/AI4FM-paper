#### 背景
- **背景**
  近些年，大型语言模型（LLMs）在多种自然语言处理（NLP）任务中表现卓越，但它们的部署仍然面临着高计算和内存需求的挑战。研究者们通常通过从LLMs中提取知识来改善开源的小模型以减少计算资源成本，这种做法被称为知识蒸馏，虽然获得了一定的效果，但这些小模型往往在需要高级推理的任务上难以达到LLM的表现。

- **已有的工作为什么解决不了**
  已有的知识蒸馏方法在提升模型的推理能力方面的成效有限。对于依赖于推理的任务，尤其是数学推理的任务，现有的蒸馏方法未能实现产生连贯和可执行的推理路径，从而限制了小模型在这些任务上的性能。

#### 核心贡献
- **提出了Mixed Distillation框架**
    - **挑战1：已有蒸馏方法无法提高小模型的高级推理能力**
      为了解决这个挑战，论文提出了Mixed Distillation框架，该框架结合了LLMs中的Program-of-Thought (PoT) 和 Chain-of-Thought (CoT) 推理能力，并将这些能力蒸馏到小模型中。PoT 专注于增强小模型生成的推理结果，而CoT 有助于优化这些结果。

    - **挑战2：现有方法中小模型的单路径推理能力有限**
      为了提高小模型的推理能力，Mixed Distillation框架集成了PoT和CoT，支持多路径推理学习。驱使这些模型通过联合使用LLMs的推理能力来提升性能。

#### 实现与部署
论文中的实验结果显示，Mixed Distillation框架在SVAMP数据集上的应用，7亿参数量的Llama2和CodeLlama模型不仅在单路径推理蒸馏方法上获得了显著的提升，而且在推理准确度方面还超越了LLM（GPT-3.5-turbo）。通过多路径推理采样，模型分别实现了85%和85.5%的令人印象深刻的准确率性能，超越了以往的蒸馏方法。

#### 总结
Mixed Distillation框架通过整合LLMs中的PoT和CoT能力到更小的模型中，显著改善了它们的高级推理能力，特别是在数学推理任务上的表现。