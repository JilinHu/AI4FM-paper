#### 背景
- **背景**       
    近年来，大型语言模型（LLMs）的进步极大地推动了机器学习和自然语言处理（NLP）领域的发展。在这种发展趋势下，监督式微调（Supervised Fine-Tuning，SFT）和In-Context Learning（ICL）作为两种主要的学习范式备受青睐，特别是在只有少量数据的情况下也展现出了很好的效果。然而，这两种范式都存在过分自信（overconfidence）的问题，即在有限数据环境下容易产生误差较大的预测结果。

- **已有的工作**
    尽管之前的研究已经集中在LLMs的性能对比上，特别是在分布外（OOD）数据的性能，同时已有研究逐步扩展了监督式训练过程中的In-Context示例的使用。然而，这些工作主要关注模型在任务特定模型上的表现，而在任务性能和校准间的行为之间的关系以及如何在性能和校准之间取得平衡这一研究问题尚未被充分探讨。这是因为一个适用于现实世界的系统应当同时提供准确和校准（值得信赖的）的预测。

#### 核心贡献
    - **挑战1：性能与校准的平衡**
        该论文深入分析了不同学习方法的性能与校准行为及其相互作用，发现在资源有限的情况下，要同时提高任务性能和校准是困难的。

    - **挑战2：数据有限的环境下的模型校准**
        论文探讨了在数据有限的环境下，使用自组装（self-ensembling）技术来改进模型的校准性，实验结果显示自组装可以显著减少校准误差。

#### 实现与部署
论文中提出的方法包括在不同的建模阶段（例如，不同的上下文例子的变化，提示的变化或不同的集成策略）应用自组装技术。作者通过控制实验发现，在七个分类数据集上使用自组装对性能和校准都有积极的影响。自组装方法不但降低了模型预测的校准误差（平均下降了43%），还提高了模型性能，并且还简化了资源的利用和时间消耗，通过对单一模型的修改以适应不同的任务。

#### 总结
本文提供了在资源有限的情况下不同学习方法的性能和校准的全面分析。这表明虽然提高性能和校准同时达成是困难的，但通过自组装技术能够在不影响性能的前提下增强模型的校准，对于未来LLMs的应用提供了重要的实践指导。