#### 背景
- **背景**       
    论文关注如何在内存有限的设备上高效地进行大型语言模型（LLM）的推理。现有的方法难以应对推理时的内存需求，尤其是在模型大小超过可用计算内存的情况下，这限制了在各种设备上部署模型的能力。

- **已有的工作**
    已有工作主要集中在压缩技术（如剪枝和量化）以及选择性执行等方面。但这些方法对减少从闪存到DRAM（动态随机存取存储器）的数据传输无能为力，且无法直接解决推理时闪存的高延迟问题。

#### 核心贡献
- **提出了一个高效的大型语言模型推理策略**
    - **挑战1：数据传输的高延迟**
        提出了减少数据负载、优化数据块大小和高效数据管理等策略，旨在减少与闪存I/O操作关联的延迟，提高数据传输的吞吐量，并优化已加载数据的管理。通过迭代地只传输必要的非稀疏数据从闪存到DRAM。

    - **挑战2：有限的DRAM容量**
        通过滑动窗口技术管理神经元数据，只保持用于最新一组输入tokens的神经元数据，从而可以有效利用内存资源。同时，我们驻留在RAM中保持转换器的注意力机制中的嵌入和矩阵，并且只有当需要时，才动态地加载FFN的非稀疏部分到DRAM中。

#### 实现与部署
提出的方法能让LLM在RAM内存可用空间是模型大小的一半甚至更少的设备上运行，相较于传统方法，我们在CPU和GPU上分别实现了4-5倍和20-25倍的推理速度提升。研究成果显著，远超过传统的加载方法，为资源受限环境中部署先进LLM提供了可能。

#### 总结
这份研究提供了一个创新且实用的解决方案，不仅能有效降低在内存受限设备上运行大型语言模型时的数据负载，还能显著提升推理速度，在实际应用中具有重要意义。