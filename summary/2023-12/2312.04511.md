#### 背景
- **背景**       
    文章提出了大型语言模型（LLM）在协调多功能调用方面存在的问题。目前的大型语言模型，比如ReAct，经常需要顺序地推理和行动以处理每个函数调用，导致高延迟成本和有时的不准确行为。

- **已有的工作**
    现有的工作如ReAct在复杂的应用程序中，尤其是需要多个函数调用的场合，效率低下，因为它们需要顺序地执行每个函数调用，然后在进行下一个函数调用之前对每个观察结果进行推理。这种方法可能导致高延迟和令牌消耗，因为每个步骤的结果都需要连接起来并发送回LLM进行进一步的推理。

#### 核心贡献
- **提出了一个名为LLMCompiler的系统**
    - **挑战1：大规模功能调用的高延迟成本**
        LLMCompiler通过引入三个组件：LLM策划者、任务获取单元和执行器，将函数调用并行化以有效协调多功能调用。这种并行执行减少了在执行多功能调用时所需的时间，降低了成本，并提高了精确度。

    - **挑战2：智能规划和任务之间的依赖性管理**
        LLMCompiler允许用户指定工具和可选的上下文示例，并自动计算函数调用的优化协调。它可以对具有复杂依赖关系的任务进行管理，并支持基于中间结果的动态重规划。

#### 实现与部署
LLMCompiler与开源模型如LLaMA-2以及OpenAI的GPT模型兼容，并已开放源代码。通过对不同类型的并行函数调用任务进行基准测试，包括需要动态重规划的情况，它在延迟速度上可达到3.7倍的提升，成本节省高达6.7倍，准确性提高约9%。与OpenAI的最近并行函数调用特性相比，LLMCompiler在延迟上有高达1.35倍的获益，同时准确度相似。

#### 总结
论文提出了一个名为LLMCompiler的系统，解决了大型语言模型在执行多功能调用时的高延迟成本和效率低下的问题，通过并行化函数调用和优化协调来提高速度，节省成本并提升准确率。