#### 背景
- **背景**       
    论文讨论了如何训练使用音乐数据集进行分解（分为独立的声音部分，即stems）的音乐生成模型。文章提到了当前大多数音乐生成模型都是文本驱动的，而文中则侧重于创建可以综合上下文信息生成音乐的模型，这种类型的模型在应用中更为实用。

- **已有的工作**
    现有工作集中于文本条件生成音乐，而并未充分考虑音乐的上下文信息和多声道的特性。

#### 核心贡献
- **提出了一个非自回归的语言模型风格的音乐生成架构**
    - **挑战1：处理多音频通道和上下文信息**
        该模型经设计可以支持多音频通道，通过对模型的修改适应了音乐的多声部特性。此外，模型使用了新颖的采样方法，以提升音乐的质量和与上下文的一致性。
    
    - **挑战2：提高音乐质量和上下文一致性**
        通过内部数据集上的训练以及采用Frechet Audio Distance(FAD)和Mean Opinion Score(MOS)测试来验证模型生成音乐的质量和上下文一致性，实验证明，采用最佳采样参数的模型能够产生质量上与现有最先进模型相当的音乐。

#### 实现与部署
论文中用到的两个主要评估指标分别是Frechet Audio Distance(FAD)和Mean Opinion Score(MOS)。前者用以评估音乐质量，后者用以评估音乐的感性质量。实验发现，模型在内部数据集上生成音乐的质量大大提升，并且音乐质量与现有顶尖的音乐生成模型相当。MOS测试结果表明，最佳采样参数的模型能够创造出听起来合理的音乐成果。

#### 总结
该论文提出了一个新的非自回归的语言模型方法用于音乐生成，优化了多声道的处理和音乐与上下文信息的一致性，并通过客观和主观评估证明了模型生成的音乐质量和与上下文信息的契合程度。