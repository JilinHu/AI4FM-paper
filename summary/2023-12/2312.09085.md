#### 背景
- **背景**       
    本篇论文研究了大型语言模型（LLMs）在与错误信息的劝说性对话中的易感性，特别关注在事实问题上模型的信仰变化。先前的研究主要集中于单轮对话中的模型易感性，但在多轮劝说性对话中，信仰可能会发生改变。

- **已有的工作**
    现有工作之所以无法解决问题，是因为它们大多集中于单次响应，没有考虑信念如何在多轮对话中通过劝说而变化，这是现实交流中的常见现象。

#### 核心贡献
- **构建 Farm 数据集和测试框架**
    - **挑战1：如何跟踪LLMs在劝说性对话中的信念变化**
        研究者首先构建一个名为 Farm 的数据集，包含了与系统性生成的劝说性错误信息配对的事实问题。创建一个测试框架来跟踪LLMs在劝说性对话中的信仰变化。
    - **挑战2：如何量化和评估LLMs的信仰变化**
        实验通过对比初始信念检查与最终信念检查的回答，量化LLMs的信仰变化。实验揭示了LLMs在面对错误信息时可能很容易受到各种劝说策略的影响。

#### 实现与部署
使用该测试框架，实验发现模型正确的事实知识信念很容易被各种劝说策略操纵。例如，ChatGPT的信念可以被改变49.8%，而GPT-4的信念可以被改变20.5%。此外，还强调了不同模型在相同错误信息面前的表现可能有显著差异。

#### 总结
本论文为首次全面研究LLMs面对事实错误信息在劝说性对话设置中的鲁棒性，并揭示了LLMs对劝说性错误信息的易感性。