#### 背景
- **背景**       
    论文探讨了大型语言模型（LLMs）如何从它们训练数据中的抽象申明性陈述中泛化。例如，当一个LLM被引导去生成2050年伦敦的天气报告时，它可能会根据2023年的数据生成报告（即匹配预训练的统计特征），也可能会纳入科学论文中关于气候变化的申明性陈述，预测更高的温度。

- **已有的工作**
    尽管现有研究已经表明LLMs有时能从训练数据中的声明性陈述中泛化，即使这些陈述在上下文中并不存在，但这种泛化能力在申明性陈述与统计模式冲突的情况下并没有被测试。例如，在气候变化的情况下，科学论文中对未来温度的预测与2023年的BBC天气报告之间存在冲突。

#### 核心贡献
- **提出了一个新的研究视角**
    - **挑战1：申明性陈述与程序信息冲突**
        论文创造了三项简化任务来研究申明性陈述对泛化的影响。比如，在第一个任务中，模型被训练在AI聊天助手总是拒绝给予医疗建议的聊天互动场景中。研究的重点是测试当同时训练含有在某些情况下提出医疗建议的申明性陈述时的影响。

    - **挑战2：模型规模对泛化的影响**
        第二个任务设计来测试模型规模对泛化的影响。它涉及基于训练集预测人口统计特征，其中统计模式和申明性信息发生冲突。研究发现，即使规模从330百万到1750亿个参数，申明性陈述对模型可能性的影响在绝对数值上较小，并且随模型规模的增加而增加的幅度出人意料的小。
    
#### 实现与部署
在三项任务中，研究发现申明性信息对LLM的泛化有微妙但系统性的影响。影响虽然在绝对值上较小，但统计学上显著。通过一系列消融实验表明，这种影响不能简单解释为模型匹配提示中的词语与记忆中申明性事实的词语。对于模型规模的影响测试发现，仅有330M参数的模型就以系统的方式受到申明性信息的影响，而随着模型规模的增加，申明性信息的影响确实增加了一些，但这种增加远小于许多其他实际LLM基准的增加。

#### 总结
本文研究了培训数据中声明性陈述与统计模式或“程序”示例相冲突时模型的泛化情况。所得结果对于AI风险（关于“背叛转折”）和公平性有重要影响。