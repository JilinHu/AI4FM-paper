#### 背景
- **背景**       
    文章介绍了当前大型语言模型 (LLMs) 在小样本学习和零样本学习中由于文本生成的通用性而成为主流选择。尽管如此，许多用户在只需要执行分类任务时，并不需要LLMs的广泛能力。

- **已有的工作**
    尽管BERT等小型模型在资源效率上优于LLMs，其能够学习通用任务而无需针对性微调，但现有工作并未充分利用这些模型在无样本和少样本学习的潜力。

#### 核心贡献
- **提出了一个基于自然语言推断(NLI)的通用分类任务框架**
    - **挑战1：如何在没有大量微调的情况下完成通用和有效的文本分类**
        该论文通过将NLI作为一个通用的分类任务，它类似于对生成性LLMs进行指令微调，在效率和通用性之间找到了一个新的平衡点。模型基于NLI能够推广到未见过的任务，从而克服了该挑战。

    - **挑战2：如何简化用户构建自己的通用分类器的过程**
        作者提供了一份详细的逐步指导并分享了复用的Jupyter笔记本，是构建通用分类器过程变得简单可行。

#### 实现与部署
该论文分享了一个在33个数据集上，包含389个不同类别训练得到的通用分类器。这个分类器的代码部分已被用于训练通过Hugging Face Hub已下载超过5,500万次的旧零样本分类器。该新分类器的零样本性能比仅基于NLI数据模型提高了9.4％。

#### 总结
这篇论文提供了一种利用自然语言推断进行通用文本分类的新方法，并且提供了实现该方法的详细步骤和工具，能够在不牺牲性能的前提下显著提高模型的效率。