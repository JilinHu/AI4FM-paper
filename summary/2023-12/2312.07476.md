#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）如何运用少量示例进行上下文学习（ICL）。然而，示例数量的限制会导致示例偏倚，即LLM推断出的输入-标签映射误解了任务的本质。作者受人类经验的启发，从示例之间的相互关系的角度出发，尝试减轻这种偏倚。

- **已有的工作**
    目前关于ICL的研究还未充分理解其工作机理。尽管已有研究尝试从不同角度探索ICL机制（如输入-标签格式的重要性、标签空间的作用、示例分布的影响），但ICL中示例间关系的潜在影响仍未被广泛讨论。

#### 核心贡献
- **提出了Comparable Demonstrations (CDs)**
    - **挑战1：示例偏倚**
        特指在ICL中，由于示例数量有限，LLM可能诱导出与任务本质不符的输入-标签映射。文章通过构造CDs—最小程度编辑文本以翻转相应的标签—以减少偏倚。这样的编辑强化了示例间的对比，突显了任务的本质，并减少了潜在的伪相关。

    - **挑战2：手动注释的成本**
        尽管ICL只需要少量示例，但手动注释仍然代价昂贵，而且目前还无法自动生成CDs。此外，CDs目前仅考虑示例间一对一的关系，未充分利用示例间的多对多关系。

#### 实现与部署
通过一系列实验展示了CDs的有效性，特别是在分布式场景（OOD）中，CDs能够带来性能提升。实验结果示，在数字推理任务中，CDs相较于其他基于语义相似性的策略显示出更高的稳健性。结论中提及，未来将在更复杂的场景（例如数学推理、多跳推理）中探索ICL，并从示例间关系的角度严格分析ICL的机制。

#### 总结
本文从示例间关系的角度研究ICL，提出通过最小化编辑文本以构造Comparable Demonstrations（CDs）来减轻潜在的示例偏倚，实验证明了其在OOD情形下的性能增益，表明了CDs在简化任务中尤其必要，并展示了其相对于示例数的稳健性。