#### 背景
- **背景**       
    论文介绍了在目标任务上从头开始训练大型变换器模型需要大量数据和计算资源。常规的转移学习实践通过使用同等大小和规格的预训练模型权重进行初始化以增加收敛性和训练速度，但如果没有可用的所需大小的预训练模型怎么办？

- **已有的工作**
    对于“如何有效实现这种知识转移”，已有工作包括知识蒸馏（Gou et al. 2021; Lin et al. 2022; Park et al. 2022）、权重共享（也称为超网络训练）（Wang et al. 2021b; Cai et al. 2019; Yu et al. 2020; Wang et al. 2021a）以及剪枝（Blalock et al. 2020; Han et al. 2015; He et al. 2017）。然而，这些方法普遍存在训练时间长、模型过大涉及训练、或与训练小模型的预训练大模型不具备超网络属性这样的问题。

#### 核心贡献
- **提出了一个用较大预训练模型初始化较小变换器的简单有效技术，称为 weight subcloning**
    - **挑战1：如何有效初始化较小目标网络？**
        要解决这个问题，文章提出了神经元重要性排名，这一操作能在预训练模型中减少每层的嵌入尺寸，并删去变换器模型中的模块以匹配缩小网络的层数。

    - **挑战2：是否可以通过从大模型中转移权重来提高小模型的训练速度并可能获得更好的精度？**
        文章证明了通过 weight subcloning 初始化网络能显著提升目标模型的训练速度，并通过权重直接转移的方法，避免了涉及大规模教师模型进行训练的额外复杂性，使训练循环保持不变，提高了各种训练任务的适应性。
    
#### 实现与部署
论文提出的方法在实现上不需要改动训练流程或进行额外学习以传递参数，相比于其他技术（如知识蒸馏和权重共享），weight subcloning 不涉及训练过程中使用大型教师模型，且能直接从父模型转移权重至目标模型，无需额外的转换学习。这种直接初始化方法显著提高了基于视觉变换器的图像分类和用于下一个标记预测的语言模型的训练速度。

#### 总结
本论文提出了一种有效的权重子克隆（weight subcloning）技术，用以从较大的预训练模型初始化较小的变换器模型，显著提高了训练速度，并使得全新的模型即使在低计算资源条件下也能得到高效训练。