#### 背景
- **背景**       
    论文介绍了如何通过增强型学习从人类反馈中对大型语言模型(Large Language Models，简称LLMs)进行对齐，目的是使其生成具有高奖励的输出，使用了一个基于人类偏好训练的奖励模型。目前存在的问题是，为了防止遗忘预训练知识，RLHF通常使用KL约束来保持策略与其经过监督微调初始化的接近度，这会阻碍奖励优化。

- **已有的工作**
    已有研究通过将KL正则化与奖励优化结合，使用监督微调(Supervised Fine-tuning, SFT)初始化作为锚点，来限制奖励的优化。然而，使用SFT模型作为锚可能会导致奖励的欠拟合，因为在降低KL和最大化奖励之间存在根本的张力。

#### 核心贡献
- **提出了一个名为WARP（Weight Averaged Rewarded Policies）的新策略**
    - **挑战1：如何改善KL和奖励之间的权衡**
        论文提出WARP利用权重平均（WA）来改善这一权衡，综合利用线性模式连接，该方法通过在细化模型之间找到性能较高的线性路径。WARP合并了由不同阶段的权重平均策略。
    
    - **挑战2：如何迭代优化性能与对齐**
        WARP使用三种WA的变体：首先，使用策略的指数移动平均作为动态可更新的KL锚；其次，通过球面线性插值合并独立细化的策略；最后，向初始化插值。这一过程迭代应用，以每次迭代的最终模型作为下一次迭代的高级初始化，逐渐精炼KL-奖励前沿，实现在固定KL下获得更高奖励。

#### 实现与部署
实验表明，WARP能够改善Gemma策略的质量和对齐，超越了其他开源的大型语言模型。WARP是一种简单的策略，旨在优化解决方案的KL-奖励帕累托前沿，并通过三种不同的权重平均(WA)阶段在对齐过程中使用三种不同的WA变体，分别基于它们的独特效益。而实验结果显示，WARP比起其他策略在固定KL下可达到更高的奖励。

#### 总结
本文提出了WARP，一种新的LLM对齐策略，通过权重平均合并模型以解决RLHF过程中的挑战，改善KL与奖励之间的权衡。实验证明，WARP能够提升模型性能和与人类价值的对齐度。