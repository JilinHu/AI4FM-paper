#### 背景
- **背景**       
    论文指出了目前评估大型语言模型（LLMs）推理策略时存在的一个主要问题。传统评估方法只注重性能指标，而忽略了由于额外计算资源带来的有效性提升，从而可能呈现出对策略效率的片面观点。

- **已有的工作**
    由于推理策略的计算要求不同，公平而全面地比较这些策略一直是一个挑战。例如，树状思考（ToT）策略需要分支出多个序列并纳入自我评估，使其比其他策略更为计算密集。只考虑性能指标的评估框架可能会忽视像计算成本这样的关键实用因素。

#### 核心贡献
- **提出了一个预算意识型评估框架**
    - **挑战1：如何平衡推理策略的性能和计算资源消耗？**
        论文介绍了一个将计算预算纳入不同推理策略性能测量的框架。这种预算意识型比较提供了一个更加平衡的视角，考虑了输出的质量和消耗的计算资源。研究发现复杂的推理策略往往并非由于算法灵巧而超越简单的基线，而是因为分配了更多的计算资源。对于一种像连贯思考自洽（CoT SC）这样的简单基线策略，只要与其他复杂方法拥有可比的计算资源，它常常能在性能和预算之间取得最佳平衡。

    - **挑战2：评估自我评估策略在不同模型和数据集上的表现**
        对自我评估策略进行深入探究，发现其性能实际上取决于模型和数据集。通过实证和理论证据，进一步调查了答案生成预算和评估预算两种特定类型的预算对性能的影响，并确认正确性预测代理的校准与利用自我评估的推理策略的成功强相关。

#### 实现与部署
该研究通过五种模型（包括GPT-4）对七种LLM推理策略进行了全面评估，跨越五个数据集。结果揭示了传统的评估指标常常忽略一个关键方面：通过额外的计算资源可获得的性能提升。这一观察得到了简单连贯思考自洽策略在有效性上可以匹敌，甚至超过更复杂策略的有力支持。此外，该论文还收录了对树状思考（ToT）和反思（Reflexion）进行预算分离的消减研究，并提出了新策略SC2以探索自我评估能力。

#### 总结
论文提出了一个考虑计算预算的LLM推理策略评估框架，并展示了简单策略在同等计算资源下可超越复杂策略的能力。通过揭示自我评估的重要性，为更加高效的预算利用和更有效策略的开发奠定了基础。