#### 背景
- **背景**       
    论文介绍了 WILDBENCH，这是一个面向大型语言模型（LLMs）的评估基准。该基准利用真实用户场景中的任务挑战，来测试和区分不同LLMs的性能。

- **已有的工作**
    已有工作缺乏反映真实用户使用案例的基准评估，且在长上下文和复杂任务中的表现不够分辨能力。

#### 核心贡献
- **提出了一个评估基准：WILDBENCH**
    - **挑战1：如何获取真实世界中对LLMs的多样化要求和评估它们的性能？**
        通过采用真实用户与聊天机器人的会话数据集WildChat，实施基本过滤，难度注解，以及人工审查来精选出代表现实世界使用案例的任务。

    - **挑战2：如何创建一个既可解释又具有评估标准的自动化评估过程？**
        通过为WILDBENCH中的每个测试查询生成一个由5-10个问题组成的清晰易验证的检查表来解决这个挑战。使用GPT-4-Turbo和Claude-3-Opus等LLMs生成检查表，从而缓解了使用单个LLM作为评估者的偏见。

#### 实现与部署
论文提出了两种自动化评价指标：WILDBENCH-Score和WILDBENCH-Reward。这两种指标通过与人工评价结果的关联验证了有效性。研究表明这些评价方法与人类评价具有很高的相关性（0.98和0.95的皮尔逊相关系数），超过了其他基准的评价结果。此外，通过引入结构化清单和成对评价，论文提出了一个减少评价中长度偏差的方法。

#### 总结
WILDBENCH 作为一个评价基准，提供了一个结合了真实用户任务挑战、自动化指标和解释性清单的评价框架，能够更准确地评估和区别大型语言模型在复杂任务中的表现。