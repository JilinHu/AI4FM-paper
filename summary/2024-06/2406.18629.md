#### 背景
- **背景**       
    论文开始讨论了大型语言模型（LLMs）在进行数学推理时面临的显著挑战，这主要是因为准确性要求对推理链进行广泛和精确的推理。确保每一个推理步骤的正确性非常关键。为了解决这个问题，该论文旨在通过学习人类反馈来提升LLMs的鲁棒性和事实性。然而，直接偏好优化（Direct Preference Optimization, DPO）在长链数学推理方面的效果有限，因为采用DPO的模型很难识别错误答案中的详细错误。这种限制源自于缺乏细粒度的过程监督。

- **已有的工作**
    已有工作中，DPO表现出在长链数学推理过程的限制。采用DPO的模型很难识别错误答案中的详尽错误。这主要是因为在评估答案的过程中缺少了对每一步推理的具体反馈。

#### 核心贡献
- **提出了一个名为Step-DPO的方法**
    - **挑战1：优化长链推理的准确性**
      此方法将个别的推理步骤作为偏好优化的单元进行处理，而不是整体性评价答案。论文发现，自生成的数据比人类或GPT-4生成的数据更有效，因为后者具有分布外的特性。论文通过Step-DPO证实使用少至1万组偏好对和不到500个训练步骤，可以在70亿参数以上的模型上，使MATH数据集的准确率提高近3%。

    - **挑战2：创建高质量数据集**
      开发了一种用于Step-DPO的数据构建流程，使得能够创建包含1万组逐步偏好对的高质量数据集。数据集的质量对于模型的性能和适应性有直接影响。

#### 实现与部署
论文中的方法，Step-DPO，应用于Qwen2-72B-Instruct之后，在MATH和GSM8K测试集上分别达到了70.8%和94.0%的准确率。这些结果超越了一系列闭源模型，包括GPT-4-1106、Claude-3-Opus和Gemini-1.5-Pro。码、数据和模型均可在其公开的GitHub仓库中获得。这项研究证明了在数学推理这一关键任务中，通过采用有针对性的改进方法（如Step-DPO），可以显著提升大型语言模型的性能，甚至可以超越当前最先进的闭源系统。

#### 总结
这份论文提出了一种新的优化方法Step-DPO，它通过对单个推理步骤进行优化而非整体评估答案，提升了LLMs在长链数学推理上的准确性和鲁棒性。