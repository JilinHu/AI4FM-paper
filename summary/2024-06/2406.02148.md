#### 背景
- **背景**       
    文章讨论了跨文档事件共指消解（Cross-document event coreference resolution, CDECR）的挑战，重点是如何通过聚类跨多个文档中提到的事件来引用相同的现实世界事件。以往的方法主要依赖于对小型语言模型（如BERT)的微调来处理事件提及之间的兼容性，但由于上下文的复杂性和多样性，这些模型容易学习到简单的共现关系，而不是真正与共指相关的术语。

- **已有的工作**
    目前的工作尝试使用小型语言模型（SLMs）来编码事件提及，通过监督共指消解来获得它们的嵌入。然而，这些方法存在缺陷，特别是当涉及到上下文多样性和复杂性时，它们倾向于学习到伪特征。另外，大型语言模型（LLMs）展现了卓越的上下文理解能力，但在适应特定的信息提取（IE）任务时遇到挑战。

#### 核心贡献
- **提出了一个协作方法**
    - **挑战1：事件提及的上下文相似性**
        由于不同文档中不同事件可能以非常相似的方式描绘，特别是对于同类型的事件，模型需要从多样化的上下文中提取相似的共指证据来作出判断。文章提出的方法通过利用LLMs本身的知诈配对和上下文理解能力，来全面建立事件提及和它们所对应的上下文元素之间的连接，更全面的解决了上下文的相似性挑战，对性能提升做出了重大贡献。

    - **挑战2：相同事件在不同文档中的描绘差异**
        相同的事件在不同文档中的描述可能有很大的差异。文章通过设计两步工作流程使用LLM来概括事件提及，并使用不同的通用提示指导其理解每个提及的上下文，而不是任务特定的上下文学习或微调。然后，通过联合表示学习将原始文档和生成的摘要整合进SLM，从而在微调过程中增强SLM对事件提及的理解，使得它能基于更加聚焓的上下文作出共指判断。

#### 实现与部署
作者提出的协作方法在三个CDECR数据集上进行了实验。通过与仅依赖LLM或SLM的方法相比，协作方法显示出显著改进，实现了性能上的互补优势。整体上，该方法在ECB+，GVC和FCC数据集的CoNLL F1得分分别增加了1%，2.7%和7%，从而达到了最先进的性能。

#### 总结
文章提出了一种新颖的协作方法以解决跨文档事件共指消解任务。通过将LLMs的普遍能力与任务特定的SLMs结合，显著提高了模型性能。