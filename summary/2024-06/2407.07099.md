#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）在解决复杂问题上的推理能力，尤其是Chain-of-Thought（CoT）提示技术的使用。CoT技术通过引导模型执行逐步推理来增强其推理能力，尤其是在类似算术、符号推理和常识问答等任务上。然而，CoT的一个变种——自身一致性（self-consistency），虽然提高了推理效率，但也导致了更高的部署成本。

- **已有的工作**
    已有的自身一致性方法通过生成多个推理路径并通过投票选择最常出现的答案来增强LLMs的推理能力。尽管取得了改善，这一方法由于需要多次推理，导致部署成本增加，并且不能充分利用单一推理路径下LLMs的推理潜力。

#### 核心贡献
- **提出了一个名为Nash CoT的方法**
    - **挑战1：如何保持自身一致性带来的性能好处同时降低推理成本**
        针对挑战1，该研究提出了Nash CoT，将语言解码视为一个偏好共识游戏，通过建立每个本地路径内部的双人游戏系统，并实现偏好均衡，从而在不同路径上实现Nash均衡。这种方法允许在使用更少的推理路径的情况下，在各种推理任务上达到与自身一致性相当或更好的性能。

    - **挑战2：如何选择对问题上下文相关的模板来指导输出**
        对挑战2，研究者利用LLM自动选择与问题上下文相关的模板来生成输出，旨在每个路径中实现Nash均衡和正常生成。这种基于模板的引导方式增加了LLM正确解决问题的概率，并且通过引入偏好均衡概念，平衡了由模板引导与常规生成的偏好，实现了对给定问题上下文的可靠匹配。

#### 实现与部署
Nash CoT通过生成两个答案的多路径推理，选择一个达到偏好均衡的答案，并最终采样预测。本研究在各种推理任务上与自身一致性方法进行了性能对比，包括阿拉伯推理、常识问答和符号推理，结果表明Nash CoT在使用更少的推理路径的情况下取得了相当或更好的性能。它还显著降低了本地部署的大型语言模型的推理成本，最高达到了50%的降低。这些实验使用了两种LLMs——Mistral-Instruct和GLM4，并在不同的推理任务中进行了测试和验证。

#### 总结
本文提出了一种新颖的方法Nash CoT，利用偏好均衡概念通过减少推理路径的数量在保持性能的同时降低了LLMs的部署成本。