#### 背景
- **背景**       
    文章介绍了针对理解长篇多模态内容这一实际应用中的基础能力，现存的多模态大型语言模型（MLLMs）评估范式并不充分，尤其在视觉中心评伜方面存在明显的不足。长篇多模态文档的理解是一个尚未充分探索的领域。

- **已有的工作**
    虽然现存MLLMs在多模态图文任务上取得了显著进步，但由于上下文窗口大小的限制，这些模型仍难以有效理解长篇图文多模态文档。此外，缺乏适当的评估基准限制了MLLMs在长篇多模态理解方面的进一步发展。

#### 核心贡献
- **提出了一个名为Needle In A Multimodal Haystack (MM-NIAH)的基准评测**
    - **挑战1：缺乏长篇多模态文档理解的评估**
        随着MLLMs的快速发展，现有模型的评估趋向全面化。然而，长篇多模态内容的理解能力在实际应用中至关重要，但该领域的研究仍然不足。MM-NIAH是首个专门设计用于系统性评价现有MLLMs理解长篇多模态文件能力的基准。它包括三种类型的评估任务：多模态检索、计数以及推理。在完成这些任务中，模型需要根据分散在整个给定多模态文档中的不同关键信息来回答问题。评价领先的MLLMs在MM-NIAH上的表现时，发现现有模型在这些任务上仍有显著的提升空间，尤其是在视觉中心评估方面。

    - **挑战2：构建用于长篇多模态文档理解的评估基准**
        MM-NIAH是首个旨在系统性评估现有MLLMs理解长篇多模态文档能力的基准。构建这一基准时，将OBELICS中的多个交错的图文文件串联成一个长篇文档，在文本或图像中注入关键信息“针”。所提出的MM-NIAH包括两种类型的“针”（即文本针和图像针），来覆盖文本和图像两种模态。出于全面评估的目的，设计了包括检索、计数和推理在内的三种任务类型。研究表明，现有MLLMs在图像“针”上的表现比文本“针”差得多；在图文交错的数据上预训练的MLLMs在MM-NIAH上的表现并不优于只在图文对数据上预训练的模型；MLLMs未能维持其底层LLMs的长篇上下文能力；虽然RAG提升了文本针的性能，但对图像针在MM-NIAH基准上无效。
#### 实现与部署
基于MM-NIAH，对开源和非开源的MLLMs进行了实验评估。实验结果表明，无论是开源还是非开源的MLLMs，在理解长篇多模态文件方面都存在挑战。模型在图像任务上的表现比文本任务更加困难，并且预训练在图文交错数据上的模型，并没有在MM-NIAH基准上展现出比在图文对数据上预训练模型更好的性能。MLLMs不能保持其底层LLMs的长篇上下文能力，而且尽管RAG增强了模型在文本任务上的性能，但在图像任务上效果不佳。这些结果表明长篇多模态文档理解仍是一个有待解决的挑战。

#### 总结
该论文提出了MM-NIAH，首个长篇多模态文件理解的评估基准，旨在考验和提升MLLMs的性能。通过不同的评估任务，论文指出了现有MLLMs在长篇多模态文档理解方面的局限和挑战。进一步的，该基准为MLLMs的长篇多模态文档理解研究提供了有效的平台。