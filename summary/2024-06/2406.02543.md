#### 背景
- **背景**       
    文章探讨了大型语言模型（LLMs）中的不确定性量化问题，目的是识别出LLMs在回答询问时的不确定性何时会很大。研究同时考虑了两种不确定性:认知不确定性和偶然不确定性。前者源于对真理（例如事实或语言）缺乏了解，而后者来自于难以减少的随机性（如对同一查询可能有多个有效答案）。

- **已有的工作**
    现有的方法主要针对存在单一正确响应的问题，它们旨在检测一个响应是否占主导（或具有相同意义的多个响应），即预测中的不确定性很小。然而,在存在多个正确响应（即真实问题存在偶然不确定性）的情况下，仅估计LLM输出的不确定性量是不够的，完美的（真实）预测器可能具有较大的偶然不确定性和没有认知不确定性，而一个完全无用的预测器可能只具有较大的认知不确定性。

#### 核心贡献
- **提出了一个信息论度量**
    - **挑战1：分离认知不确定性与偶然不确定性**
        研究提出使用迭代的提示过程来构建一个LLM产生的多响应的联合分布，以此量化LLM与真实基础事实之间的差距。这种差距对偶然不确定性不敏感，因此即使在多个有效响应的情况下也能量化认知不确定性。这个过程可以显著地检测到认知不确定性。

    - **挑战2：计算可行的不确定性下界**
        研究者推导出一个可计算的不确定性度量的下界，并提出了有限样本的互信息（MI）估计器。这个估计器有时即使在可能有无限支持（语言中的所有可能字符串）的LLMs及其派生联合分布下也只会有微不足道的误差。

#### 实现与部署
通过一系列实验，论文展示了新公式的优势。实验在闭包开放域问答基准数据集上进行，如TriviaQA, AmbigQA以及一个从WordNet合成的数据集，结果表明当数据主要由单标签或多标签查询组成时，基于MI的幻觉检浴方法超过了一个基于响应可能性的天真基线，并实现了与一个基于输出熵的更高级基线相似的性能。

#### 总结
本论文重点研穴并提出了一个新的信息论度量方法以在大型语言模型中量化不确定性，特别是针对LLMs生成响应时的幻觉现象。这项研究为如何识别和处理LLMs中的幻觉提供了新的理解和解决方案。