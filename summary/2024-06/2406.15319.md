#### 背景
- **背景**       
    文章介绍了在传统的检索增强生成（Retrieval-Augmented Generation, RAG）框架中，检索单元通常较短，例如DPR通常使用的100词的维基百科段落。由于这种设计，检索器需要在大型语料库中寻找相关信息单元，而读者只需要从检索到的短文本中提取答案。这种“重”检索器与“轻”读者的设计可能导致性能不理想。

- **已有的工作**
    现有的RAG模型需要回调大量的信息单元，例如top-100或更多，并且需要额外的复杂的重排器（re-ranker）才能取得很好的性能。此外，短信息单元也可能导致由于文档截断而产生的语义不完整性，进而限制最终性能。这种设计选择是在NLP模型处理长文本能力受限的时代做出的。

#### 核心贡献
- **提出了一个名为LongRAG的框架**
    - **挑战1：检索单元太短，导致性能不佳**
        为了解决因语义不完整而造成的信息丢失和模型性能受限问题，LongRAG将整个维基百科文档处理成超过4K标记的单位，相比之前增长了30倍。这大大减少了总的信息单元数量，从2200万减少到60万。这显著降低了检索器的负担，并显着提升了检索得分。

    - **挑战2：现有RAG的读者模块不适合处理长文本**
        LongRAG通过“长检索器”和“长读者”的设计提供了一种解冤方案。长检索器负责识别给定查询的相关信息，然后将头部的检索单元拼接起来，作为下一步的长文本上下文。随后，长读者会从这些检索到的长文本（通常约为30K标记）中进一步提取答案。LongRAG只需要向现有的长文本LLM（如Gemini或GPT-4）输入问题，就能不经任何训练产生答案。
  
#### 实现与部署
在NQ和HotpotQA数据集上的实验结果显示，LongRAG将NQ数据集大小从2200万减少至60万文档单位，并将答案召回率提升至71%，同样的，将HotpotQA数据集大小从500万减少至50万，将答案召回率提升至72%。此外，LongRAG在NQ实现了62%的精确匹配率，在HotpotQA实现了64%的精确匹配率。这些结果可以与最强的经过微调的RAG模型相媲美，例如Atlas和MDR。

#### 总结
LongRAG是一个针对开放领域问答任务的新框架，它通过增大检索单元和利用长文本语言模型来解冤传统RAG框架的限制。通过减少检索单元和提升检索器效能，以及使用长文本LLMs进行零次学习的答案提取，LongRAG在性能上取得了显着的改善。