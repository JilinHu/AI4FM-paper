#### 背景
- **背景**       
    论文介绍了大型语言模型（LLMs）在聊天对话中的不透明性问题，用户往往很难理解系统是如何以及为什么生成特定的响应。这种不透明性在考虑到偏见和真实性问题时尤其受关注。
     
- **已有的工作**
    已有工作无法有效解决这个问题，因为简单地询问系统来解释其推理过程通常不可行。LLMs在描述自己的输出原因时通常产生看起来有说服力但实则不实的解释，或者干脆回避这些问题。

#### 核心贡献
- **提出了一个看板原型**
    - **挑战1：如何设计一种用户体验**
        研究者面临的挑战是如何设计一种界面，让用户能够了解并控制AI系统内部模型的表现。通过开发一个能够实时展示AI系统对用户的内部模型的看板界面来回应这个挑战。
    
    - **挑战2：如何提高透明度对抗偏见**
        另一个挑战是如何提高系统的透明度，使用户能夀察觉到并对抗潜在的偏见。这通过用户修改系统对他们的内部模型进而改变系统行为的能力来解决。
    
#### 实现与部署
实验结果表明，用户欣赏这个设计，看板为用户提供了洞察聊天机器人回应的方式，提高了用户对偏见行为的认识，并给了他们控件来探索和减轻这些偏见。研究者还报告了用户对偏见和隐私问题的反应以及建议，这将有助于未来的设计和机器学习研究。
与已有的工作相比，该研究通过可见化内部状态和允许用户控制这些状态，提供了一种更适合普通用户需要的方法来增加透明度和理解。

#### 总结
这篇论文致力于增加LLMs在对话AI系统中的透明度，并通过设计一个可视化的用户界面—一个与聊天机器人接口相配套的看板—实现了这一点。用户能够实时看到系统的内部用户模型，并可以通过界面更改这些模型。基于用户反馈，看板还有助于揭露并且对抗模型的偏见行为。