#### 背景
- **背景**       
    文章讨论了利用大语言模型（LLMs）解决数学问题时遇到的挑战。已有的研究表明直接偏好优化（DPO）能够提高LLMs在诸如推理和对齐等下游任务的表现。但是数学问题求解具有多样性，即便是错误的推理路径也可能偶然得到正确答案，这使得仅基于最终答案判断模型生成文本的好坏变得不够精确。

- **已有的工作**
    现有的工作通常需要人工或人工智能反馈来指导模型，比如进行过程监督，它需要大量精细且昂贵的人工标注，并且只适用于传统的强化学习算法。

#### 核心贡献
- **提出了一个自动化步骤控制优化的方法（Step-Controlled DPO, SCDPO)**
    - **挑战1：自动生成特定步骤开始出错的解答样本**
        传统的优化方法可能无法捕捉到数学问题求解过程中的复杂性。SCDPO通过在特定步骤开始产生错误的解答样本的自动生成，提供了明确的分步反馈监督，不需要额外的人工标注。

    - **挑战2：提升数学问题的求解表现**
        使用SCDPO可以帮助模型学习更详细的推理能力。研究通过对比传统的DPO和SCDPO在不同SFT模型上的表现，证实了SCDPO能够有效提高大语言模型在数学问题求解方面的性能。

#### 实现与部署
论文提出的SCDPO方法已应用于包括一个现有的SFT模型和两个微调过的SFT模型上。实验结果表明，相比于传统的DPO方法，SCDPO在三个不同的SFT模型上均取得了一致的性能提升。此外，作者还用SCDPO对一个InternLM2-20B模型进行了微调，该模型在GSM8K和数学（MATH）数据集上分别取得了88.5%和58.1%的高分，与其他开源的大模型相比表现出色。

#### 总结
本论文提出了一种新的数学推理优化方法——SCDPO，通过在特定步骤监督错误的方式，自动化地生成训练样本，显著提升了LLMs在数学问题求解方面的性能，证明了该方法的潜力。