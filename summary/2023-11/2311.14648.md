#### 背景
- **背景**       
    文章介绍了预训练语言模型（LM）具有产生虚假但听起来合理的文本的神秘倾向，这种“幻觉”现象对于基于语言的AI系统的可用性构成了障碍，并且可能会对依赖它们输出的人造成伤害。研究指出，对于无法从训练数据确定真伪的“任意”事实，符合统计校准条件的语言模型必然会产生幻觉。

- **已有的工作**
    已有的工作无法解决幻觉问题，因为它们没有识别到这是一个内在的统计原因。AI生成的幻觉并不仅仅与LM架构或数据质量有关，而是与LM中实现的统计校准条件有关。尽管进行了优化以降低幻觉产生的比率，但这远未触及问题的根源，因为已有的校准方法不能区分可靠和虚假信息。此外，幻觉问题也源于训练数据中信息的丰富程度——某些事实可能只在训练集中出现一次，因此导致模型在生成时产生幻觉。

#### 核心贡献
- **提出了一个统计校准条件**
    - **挑战1：如何在必要产生幻觉的同时，构建一个优秀的预测语言模型**
        由于良好的预测性能意味着模型必须能够在没有明确线索的情况下生成具有高概率的事实，因此幻觉是不可避免的。文章提出，即使在理想的训练条件下——也就是训练数据是独立同分布（i.i.d.）且没有事实错误——良好的预测表现仍将导致幻觉产生。通过对模型实现校准，文章展示了这种统计校准条件与幻觉生成之间的联系。

    - **挑战2：如何量化语言模型应产生的幻觉程度**
        文章提供了对幻觉产生程度的量化方法，即与训练数据中只出现一次的事实（“Good-Turing”估计）所占的比例紧密相关，即使是假设理想的训练数据没有错误的情况。这说明了即使训练数据质量极高，模型为了保持统计校准，也会自然而然地生成一定比例的幻觉。

#### 实现与部署
文章通过理论证明，并通过实验验证了他们的统计下界模型。结果显示，即使在理想化的假设下（例如，没有训练数据中的事实错误），幻觉仍然是生成模型不可避免的产物。特别是在模型经过预训练以精确预测文本之后，需要进行后训练以减少对那些在训练集中只出现一次的任意事实的幻觉，然而分析也表明对于在训练数据中出现不止一次的事实（例如引用出版物例如文章和书籍）以及系统性事实（如算术计算等）并无统计理由产生幻觉。这意味着，可能有其他的架构和学习算法可以减轻这些类型幻觉的产生。

#### 总结
该文章展示了预训练语言模型在充分校准的条件下，必然产生幻觉的统计根源，并介绍了预测性能良好的模型固有的幻觉产生机制。同时，文章还提供了幻觉产生率的下界估算，并探讨了不同类型事实产生幻觉的可能性，指出了未来减轻特定类型幻觉的可能方向。