#### 背景
- **背景**       
    文章介绍了在使用语言模型实现问答任务中，对于某些应通过在关联上下文中推理来回答的问题，这些问题是否是通过记忆来回答的这一持续讨论的问题。

- **已有的工作**
    已有的工作将潜在的记忆判定为训练样本中存在的 n-gram 与评估样本中的 n-gram 重叠（n ≥ 8），这种方法只能检测连续 token 的精确匹配，而无法捕捉到间接相似或连续性不连续的 token 重叠。此外，在先前的工作中，通常需要比较不同的评估样本群以得出结论。

#### 核心贡献
- **提出了一个新方法**
    - **挑战1：识别无法通过记忆回答的问题**
        文章提出了一种基于输入 token 和标签 token 之间的语义相似性来识别模型不太可能记住答案的评估样本的方法。这种方法比之前的方法优越，因为它能够发现包含连续或不连续的 token 序列重叠的评估-训练样本对。

    - **挑战2：提高未记忆样本的表现**
        文章设计了一个多任务的训练策略，并增加了两个设计用来引入简单数字推理策略的额外数据集，这些策略已知能够提升某些评估数据集上的表现。文章展示了在未记住子集上，引入额外训练数据集后两个模型之间的表现得到了改善。

#### 实现与部署
通过将句子嵌入向量的余弦相似度用于评估训练样本之间的相似性，研究者发现这种方法能揭示测试-训练样本的连续或不连续重叠。此外，研究者通过识别不可能由新增加的训练数据集（干预）所记住的评估样本，并在干预前后使用同一子集来评估性能差异。在实验中，研究者训练了两个语言模型，其中第二个模型在训练时增加了两个额外数据集，设计用来引入简单数字推理策略。通过这种方法，作者发现对于DROP和ROPES这两个评估数据集的未记忆子集，性能分别提高了9.0%和25.7%，而其他评估数据集的表现没有显著变化。

#### 总结
本论文提出了一种新方法以评价小型语言模型在问答任务中答案的生成是否为记忆或概括能力的结果。通过语义相似度分析，确定了不太可能被模型记住答案的评估样本，并用增加额外训练数据集的方式，针对特定评估子集进行了模型性能的优化。最终，研究结果显示增加了数据集的模型在特定评估数据集上有了显著提升，并推断这种改善与模型的泛化能力有关。