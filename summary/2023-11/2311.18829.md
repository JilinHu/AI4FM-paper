#### 背景
- **背景**       
    文章介绍了当前文本到视频生成领域存在的问题，主要是如何从文本提示出发直接生成视频存在着困难，包括对细粒度的外观细节处理不足以及运动动态学习效率低下。

- **已有的工作**
    已有的工作尝试通过大规模的文本到视频扩散模型直接进行学习，这种方法虽然能生成高质量视频，但需要大量的GPU资源和训练数据。部分工作提出了成本效益更高的策略，即在文本到图片模型中引入时间层后，在配对的文本和视频数据上进行微调，创建文本到视频模型，但生成的视频可能遇到外观和时间一致性问题。

#### 核心贡献
- **提出了一个名为MicroCinema的框架**
    - **挑战1：外观保持和时间一致性**
        为了解决外观保持和时间一致性的挑战，文章提出了一个分而治之的策略，即将文本到视频的生成过程分为两个阶段：文本到图片的生成和图片&文本到视频的生成。通过这种策略，先利用文本到图片模型生成一个关键帧图片，再基于该图片和文本生成视频。此外，文章提出的Appearance Injection Network能够加强给定图像的外观保持，使模型专注于运动学习。

    - **挑战2：利用先进的文本到图像模型**
        文章利用了如Stable Diffusion、Midjourney和DALL·E等先进的文本到图片模型生成逼真细致的图像。这使得MicroCinema在后续的视频生成过程中可以分配更少的注意力于细节的外观，而更多地关注于有效的运动动态学习。同时引入的Appearance Noise Prior新机制旨在维持预训练的2D扩散模型的能力。

#### 实现与部署
通过大量实验，MicroCinema展示出其框架的优越性。具体来说，MicroCinema在没有额外训练数据、零样本学习（zero-shot）的条件下，在UCF-101数据集上取得了342.86的FVD（Fréchet Video Distance）得分，在MSR-VTT数据集上取得了377.40的FVD得分，两者均为当前的最佳成绩。

#### 总结
MicroCinema以其创新的文本到视频生成两阶段流程和有效的Appearance Injection Network及Appearance Noise Prior机制，在视频生成质量上实现了新的突破，为后续工作提供了可借鉴的范例。