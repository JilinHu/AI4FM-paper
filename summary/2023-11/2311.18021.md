#### 背景
- **背景**       
    文章分析了视觉语言模型(VLMs)的**背景**学习(ICL)性能，当模型需要根据上下文中提供的几个示例(称为示范)来回答关于新图片的问题时，ICL发挥作用。研究表明，示范中的文本信息对模型在回答生成中的影响至关重要，而随机选择的示范并不总是为不同查询提供必要的上下文信息，且基于视觉相似性检索的示范会忽略文本上下文。

- **已有的工作**
    已有研究未能充分考虑选择用于ICL的示范时必须综合考虑图像和文本信息的需求。仅基于视觉相似性的示范选择方法会遗漏对于特定查询更有信息量的文本内容，因为视觉相似的图片所关联的问题并不总是相互关联的。

#### 核心贡献
- **提出了一个名为混合模式上下文示例选择（MMICES）的方法**
    - **挑战1：如何有效选择有信息量的示范**
        作者讨论了挑战：合适的示范选择对于VL模型的ICL性能至关重要，但存在许多挑战。例如，仅基于文本信息的检索会面临自身的挑战，例如那些通常的查询（如“这张图片中有什么？”）并不能提供足够的信息。为了解决这一挑战，该论文提出了MMICES方法，将基于图像相似性的初步示范选择和基于文本相似性的重排序相结合，以挑选出对特定查询更有信息量和相关性的示范。

    - **挑战2：如何在示范图像影响微小的情况下提高选择质量**
        论文指出，尽管示范图像在上下文中的影响被发现是微小的，但在初步的示范检索中，选择与查询图像视觉相似的示范更有可能带来含相关信息的文本，这是MMICES采用混合模式方法的原因。MMICES通过混合模式方法以确保提供高质量的上下文示范，提高模型生成正确回答的能力。

#### 实现与部署
论文中分析了包括OpenFlamingo和IDEFICS在内的7种视觉-语言模型，并涉及其架构、模型大小、预训练数据集和指示性微调。论文采用了包括VQAv2、OK-VQA、GQA和MSCOCO在内的四种VL任务和数据集进行评估，并展示了实验设置中使用的模型和度量方式。性能表现部分显示，MMICES在比较不同实验方式下的性能时，其性能指标在多次评估循环中均较高，这些结果突出了其优越性。

#### 总结
本文提出了一个用于视觉-语言模型在背景学习中选择示范的新方法MMICES，并通过一系列实验展示了其在不同模型和数据集上的良好性能。