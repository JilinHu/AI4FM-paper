#### 背景
- **背景**       
    论文讲述了大型语言模型（LLMs）在解决数学、逻辑和常识推理问题时，当指示它们使用“思维链条”（CoT）提示逐步推理出答案时，可以更准确、更可解释地工作。

- **已有的工作**
    尽管CoT方法取得了显著的成就，但是仍有许多情况下，生成的理由是不正确的。过去的改进方法包括对模型进行微调以生成更好的理由。如果可以获得黄金标准的理由，例如通过众包或自动方法，则可以应用监督学习方法，但这种数据往往难以获取。一种有吸引力的替代方法是从仅包含问题和正确答案的数据集出发，这些数据集更容易获取，并在学习过程中引导理由生成。作为这种策略的一种版本，自学的推理者（STaR）生成了由LLM提出的理由，并随后对那些产生正确答案的理由进行微调。

#### 核心贡献
- **提出了一个xxx**
    - **挑战1：采样**
        论文提出的核心挑战是从条件于正确答案的理由后验分布中采样。文章介绍了一种名为TRICE的学习算法，通过一个简单的马尔可夫链蒙特卡罗（MCMC）期望最大化（EM）算法来解决这个问题，并结合了一种新颖的控制变量技术，该技术可以在模型改进时将梯度估计的方差降至零。

    - **挑战2：处理不正确的理由**
        与STaR相比，TRICE算法有一个主要优势，即它不太可能忽视那些因理由生成过程失败而变得难以处理的例子。这不仅稳定了算法的收敛性能，而且还提高了性能表现，并且还可以从不正确的理由中学习。

#### 实现与部署
论文的评估结果表明，TRICE算法显著提高了模型在保留样本集上的准确性，并且比使用STaR、直接调整有或没有CoT的模型以及甚至是使用人类生成的理由进行监督微调的模型具有优越的性能。

#### 总结
本论文开发了一种基于MCMC-EM的微调策略，通过平均理由帮助LLMs生成正确的答案，具有潜在的推广应用的潜力。