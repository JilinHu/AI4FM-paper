#### 背景
- **背景**       
    文章介绍了在软件开发中安全性问题的重要性，尤其是代码中的安全缺陷，它们可以被攻击者利用来进行恶意活动。为了自动化地解决这些安全问题，研究人员转向了大型语言模型（LLMs），如OpenAI的GPT系列，用作代码审查和修复的工具。尽管LLMs在自然语言处理（NLP）任务中显示出了很高的能力，但它们在应对特定领域的任务，如代码安全性问题，挑战依旧存在。

- **已有的工作**
    已有的研究展示了LLMs在理解和生成代码方面的能力，但在应用于解决安全问题时，这些模型往往缺少对应的专业知识和精确指导，导致不能可靠地识别和修复安全缺陷。此外，现存的方法通常不包括对模型输出的细致审查过程，使得这些模型不能有效地从反馈中学习并提高其安全性问题解决的能力。

#### 核心贡献
- **提出了一个名为FDSS的新方法**
    - **挑战1：如何有效地使LLMs理解并修复代码中的安全缺陷**
        FDSS方法通过结合LLMs和静态代码分析工具Bandit的反馈，使LLMs能够接收到关于代码安全问题的详细指导，并生成可能的解决方案。这对于增强LLMs解决安全问题的能力是至关重要的。

    - **挑战2：如何提高LLMs在代码安全性问题上的表现**
        研究者的分析结果表明，与其他不包含来自Bandit的反馈的方法相比，FDSS方法显著提高了LLMs在处理安全问题时的效果。特别是在GPT-3.5和CodeLlama上得到的改进，超过了直接提供Bandit反馈或其它反馈机制的方法。

#### 实现与部署
使用PythonSecurityEval数据集进行评估，文章的分析结果表明FDSS方法在所有三个基准测试和三个模型（GPT-4, GPT-3.5, CodeLlama）中均优于基线方法。文章展示了不同方法在PythonSecurityEval数据集中生成的安全代码百分比，并通过图表展示了如何在不同的安全问题上进行比较，如CWE-259和CWE-20等。结果显示，在多数情况下，FDSS方法在生成安全代码方面比其他方法有显著的提高。

#### 总结
本文介绍了一种新型的代码修正方法FDSS，通过与静态代码分析工具Bandit集成，能显著提高LLMs解决代码中安全问题的能力。