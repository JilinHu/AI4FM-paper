#### 背景
- **背景**       
    文章介绍了Adapters——这是一个用于大型预训练语言模型（LLM）的开源库，它整合了多种参数高效和模块化的迁移学习方法。通过提供统一的接口，Adapters简化了研究人员和从业者使用和配置这些迁移学习方法的过程。该文献指出，由于LLM日益增大的规模，传统的全部参数微调学习范式变得非常昂贵，且不仅存在参数效率问题，还面临着负迁移、缺少正迁移、灾难性遗忘和泛化能力差等关键问题。

- **已有的工作**
    过去的研究尝试通过设计小型组件，即Adapter，用于在标注的任务数据上对语言模型进行微调。这些Adapter以其参数高效性和模块化而著称，体现在AdapterHub的初次发布中，但随着对Adapter方法的兴趣增长，现有的工作专注于参数效率，忽视了这些方法的模块性。为此，文章中提出了一个全新的库Adapters，它不仅在参数效率上进行了探索，还强化了模块化的特点。

#### 核心贡献
- **提出了一个统一的Adapter库**
    - **挑战1：参数效率**
        原有的工作或库如OpenDelta和HuggingFace的PEFT在处理参数效率问题上已取得一定成果，但Adapters库在此基础上，通过简化复杂适配器设置的定义，增强了复杂组合结构的灵活性。

    - **挑战2：模块化**
        在现有的工作中，特别是AdapterHub一代，模块化方面的设计选择在转移学习中的应用还未充分发展。因此，Adapters在“水平”和“垂直”方向上进行了扩展，不仅支持更多的预训练神经结构，还增加了新的组合和处理能力，实现了模块化的快速部署和适配能力。

#### 实现与部署
Adapters库提供了与HuggingFace Transformers库的紧密整合，允许通过最小化的代码修改来适配预现有的微调脚本。它支持多种模型结构，并通过一套统一的适配器接口，覆盖了与适配器工作相关的整个生命周期，包括添加、挂载、激活、保存、加载、聚合和删除适配器模块。作者通过实验展示了Adapters与传统全微调在多个NLP任务上的表现，并证明Adapters在参数效率和模块化传递学习上的有效性。

#### 总结
论文提出了一个统一的库——Adapters，它整合并扩展了参数高效和模块化迁移学习方法，实现了与Transformers库的紧密整合，通过多个NLP任务的对比实验，展示了其有效性。