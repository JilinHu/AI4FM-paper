#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）通过自然语言的人类反馈进行对齐的效率问题。传统上，LLMs使用人类偏好信号的排名反馈进行对齐，这些反馈采用增强学习从人类反馈（RLHF）的形式。然而，人类对LLMs输出的偏好可能以更丰富的形式表现，包括提供对响应优缺点详尽反馈的自然语言。该工作探究了使用自然语言模型人类反馈的数据效率。

- **已有的工作**
    RLHF在训练期间相当不稳定，且需要更多计算资源来同时支持政策模型和基础模型，特别是当模型大小超过50B参数时，给基础设施带来了巨大挑战。此外，RLHF通过单一奖励值传达人类偏好，但人类偏好可能要更详细和具体。例如，偏好的响应可能因为它简洁直接，或者在某些关键方面涵盖的更详细而得分更高。与基于奖励的模型相比，通过自然语言中详细的批评使LLMs学习到细粒度人类反馈是具有挑战性的。

#### 核心贡献
- **提出了一个Critique and Revise (CnR)方法**
    - **挑战1：人类反馈数据不足**
        使用更少的数据（如1000条记录或更少），通过在自然语言中提出批评和修正响应来教授模型人类偏好。CnR方法能够教授一个开源的LLM（例如Falcon-40B-Instruct）如何修正甚至是ChatGPT这样的强大LLM的响应。这显示了比起传统的偏好对比反馈，CnR方法在数据效率方面有着明显优势。

    - **挑战2：对人类反馈的高细节要求**
        CnR方法不仅要求LLM生成批评来概述响应的积极和消极方面，还需要它根据批评来修正最初的响应。这个方法允许更加精确和详细的人类反馈对齐，引导模型改正响应中的错误，并根据提供的批评相应修改相关回答。

#### 实现与部署
通过实际应用，论文展示了这个模型不仅能够产生有意义且针对特定响应的批评，并且还能够解决响应中的错误。应用于ChatGPT响应的修正在一个迭代中达到了56.6%的胜率，通过五次迭代后胜率可以提高到65.9%。此外，通过消减研究，作者发现使用更大的预训练模型和高质量指导调整数据可以提升修正的质量。还研究了CnR数据量对模型性能的影响。

#### 总结
文章提出了一个有效的CnR方法，它能够通过使用自然语言的精细反馈和响应修正，高效地校准LLMs以符合人类预期。通过相对较少的人类反馈数据，此方法可以显著改善即使是顶尖LLMs的响应质量，如ChatGPT。