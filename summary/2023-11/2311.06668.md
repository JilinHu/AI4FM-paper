#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）表现出的“在上下文中学习（ICL）”的能力，他们能基于示例演示适应新任务。但是，ICL在许多设置中的有效性是有限的，很难定量控制，并且会占用上下文窗口空间。

- **已有的工作**
    已有的工作存在的问题主要有：ICL的效果不均匀，高度敏感于模板、言辞者和演示的选择，导致难以实现既适应性强又鲁棒的LLM应用。此外，transformers的计算负担限制现有LLM处理扩展上下文的能力。

#### 核心贡献
- **提出了一个ICV**
    - **挑战1：**如何让LLM更有效地跟随示例演示，且能方便进行量化控制且不占用过多上下文空间。**论文方法**：提出了“在上下文中向量（ICV）”的新方法，通过利用LLM的潜在嵌入来创建ICV，然后在新查询中使用ICV来移动LLM的潜在状态，从而更有效地跟随演示示例，并减少了提示的长度。

    - **挑战2：**如何将多个示例演示有效集成到一种控制方法中，以便在新查询中引导响应生成的过程。**论文方法**：ICV方法通过分两部分实现这个目标：任务摘要和特征移位，先从演示示例中计算出“在上下文中”向量，然后在查询示例的前向传递期间应用这个向量，以在生成过程中整合上下文任务信息。使用单一向量避免了大量计算开销，并且对任务的调整和控制更为直接和简便。

#### 实现与部署
ICV方法在多种任务上验证了其有效性，包括语言模型解毒、风格转换和角色扮演等，在像Falcon和Llama这类LLMs上，ICV显著优于标准的ICL和LoRA微调这些任务。除此之外，研究还展示了一个适应组合任务的简单范式，这些任务围绕“在上下文中向量”的算术操作进行。ICV的简单性只在计算“在上下文中”向量时引入可忽略的计算开销，可以处理超出上下文长度限制的许多演示示例，而无需引入新的参数，从而成为对标准ICL和微调框架的实用增强。

#### 总结
本论文提出的ICV方法为大型语言模型的上下文学习提供了一种新颖且更加有效的替代方案。通过将演示示例的关键信息集成到一个可以控制的向量中，ICV方法提高了任务指导的精确度和效果，并显著优于现有的方法。实验结果表明，ICV在多项任务中展现了较高的性能，包括在不同的LLMs上进行语言模型解毒、风格转换和角色扮演。ICV方法的计算开销低，并且易于控制，有助于提升语言模型在实际应用中的适用性和弹性。