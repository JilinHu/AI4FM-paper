#### 背景
- **背景**       
    文章探究了现代大型语言模型（LLMs）在接受约束的环境中解决问题的创造性，特别是克服心理学中所称的“功能固定性”偏见，使用熟悉的物体以创新或非传统方式解决问题。

- **已有的工作**
    现有的自动化评估技术不能准确衡量解决方案的有效性，所以研究者们招募了人类评估员来评价GPT-4在MACGYVER数据集上的表现。

#### 核心贡献
- **提出了名为MACGYVER的数据集**
    - **挑战1：如何生成适合LLMs的创造性问题解决任务?**
        创建这类问题需要突破思维常规。研究者们通过结合GPT-4的生成能力和人类评审员的验证能力，设计了一个高效的流程来创建MACGYVER数据集。这个数据集包含1600个旨在触发功能固定性，需要创新思考的实际问题。

    - **挑战2：如何量化和评估LLMs在处理这些任务时的表现?**
        自动化评估的局限性导致了需人类注释者基于的评估系统来评价GPT-4提出的解决方案的可行性和效率。

#### 实现与部署
研究者们从七个不同的LLMs收集了机器解决方案，并对GPT-4进行了额外的评估以准确地与人类表现进行比较。对于每个问题，平均提取了四个GPT-4解决方案，并采用核聚变采样策略从每个API调用中返回最高概率的答案序列。人类评估者们通过更细致的分类系统评价给出的答案，以判断答案的正确性和解决问题的效率。评估手段的调整使得标注变得更为准确，反映了答案的实际效能。

#### 总结
本研究通过创造MACGYVER数据集，探索了LLMs在解决非传统问题上的能力，并通过人类评估员对GPT-4的表现进行了评价。研究结果展示了LLMs在这类任务上的局限性，同时提出了提高其表现的新方法。研究强调了创造性问题解决能力在日常生活中的重要性，并尝试通过LLMs补充人类的创造性思维，以期提高解决问题的能力和效率。