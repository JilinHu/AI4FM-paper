#### 背景
- **背景**       
    文章介绍了医学领域能否通过系统的提示工程（prompt engineering）来指导通用基础模型（如 GPT-4）发挥专家级别的能力，而无需进行专门的微调（fine-tuning）或依赖于人类专家的提示构建。

- **已有的工作**
    已有的研究主要侧重于使用通用的大型语言模型处理专业任务，并尝试通过微调或设计特定的输入提示来增强模型的能力。然而，这些方法尚未能有效地在没有专家监督的情况下，提升基础模型在专业任务上的表现至专家级别。

#### 核心贡献
- **提出了一个名为Medprompt的提示策略**
    - **挑战1：如何让基础模型适应特定领域的询问，而不依赖于细分域的训练数据或者人工设计的提示？**
        挑战在于如何高效地选择几个示例（few-shot examples），让基础模型快速适应特定任务的格式。Medprompt通过动态选择和自生成思考链的策略，使模型能够根据测试样本选择不同的提示示例，从而更好地适配各种任务输入。

    - **挑战2：如何量化和提升这种方法的通用性，确保在医学以外的其他领域也能发挥作用？**
        通过不同学科的能力评估（包括电气工程、计算机科学、哲学、会计、法律、护理和临床心理学等领域）来展示Medprompt不仅在医学问答数据集上有效，而且在其他多学科的竞赛问题上也表现出色，表明了方法的普适性和扩展能力。
      
#### 实现与部署
Medprompt的方法论结合了动态少量示例（dynamic few-shot）、自我生成的思考链（self-generated chain of thought）和选项洗牌组合（choice shuffle ensembling）三种主要技术。研究通过对GPT-4模型的探索实验证明，在医学挑战问题上，该策略能够显著提升专业能力，并且成功超越了之前通过专门微调和专家提示构建的专家模型。具体来说，Medprompt在医学问答基准数据集上实现了超过90%的准确率，创下了新的最佳表现记录。
  
Medprompt的独特之处在于其使用基础模型自创链式思考提示，这样的提示比传统的人类专家设计的提示更优。通过消融研究（ablation studies），文章进一步分析了Medprompt不同组成部分的相对重要性。此外，通过眼闭式数据集（eyes-off dataset）的评估避免了过度拟合的问题，并通过在非医学领域的多个学科中测试这些策略的性能，证明了其普适性和潜力。

#### 总结
本文通过系统的提示工程方法探讨了在无需专家监督的情况下，如何指导通用的基础模型在专业任务上发挥专家级别的能力，具体以医学领域为案例研究。所提出的Medprompt策略证明了其在增强基础模型专业能力方面的显著优势，并展示了广泛适用于多个学科的可能性。