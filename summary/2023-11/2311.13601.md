#### 背景
- **背景**       
    文章介绍了在大型语言模型（LLMs）中，上下文内提示（in-context prompting）已成为提高零样本（zero-shot）能力的普遍方法，但这一概念在视觉领域探索较少。现有的视觉提示方法集中于引用分割（referring segmentation），其只能分割出最相关的对象，未能解决如开放集分割（open-set segmentation）和检测等多种通用视觉任务。

- **已有的工作**
    已有的研究因只专注于引用分割，无法处理开放集分割和检测任务，这些任务需要模型能够理解和处理多种视觉提示并在没有先前见过的类别中发现和分割目标。此外，现有方法往往不能有效地利用无标签数据来提升模型性能。

#### 核心贡献
- **提出了一个全新的视觉上下文内提示框架**
    - **挑战1：支持多样化的视觉提示**
        已有模型无法处理多种形式的视觉提示，如笔画、框以及点。此论文通过开发一个多功能的提示编码器（prompt encoder）使得模型能够支持各种形式的视觉提示，并能接收任意数量的参考图像段作为上下文，从而应对这一挑战。

    - **挑战2：利用无标签数据提升性能**
        大多数视觉模型无法有效地使用无标签数据来提高性能。该研究建立了DINOv，一个统一的框架，通过视觉上下文内提示处理引用分割和通用分割问题。该框架简化了模型设计，并使模型能够同时利用语义标记的数据和未标记的数据，来提高性能，应对了这一挑战。

#### 实现与部署
DINOv 是建立在编码器-解码器架构之上，并提出了用于视觉提示的全新机制。模型首先对参考视觉提示进行编码，然后将其适配到目标图像上，从而提取出代表的视觉提示特征。实验结果显示，DINOv 在 COCO 和 ADE20K 数据集上展示了竞争力强的性能，分别达到了57.7 PQ 和 23.2 PQ，证明了其在引用分割和通用分割上的有效性。

#### 总结
本论文提出了DINOv，一个新的视觉上下文内提示框架，能够有效处理多样化的视觉提示，使用无标签数据，并在多个任务中达到很好的性能。