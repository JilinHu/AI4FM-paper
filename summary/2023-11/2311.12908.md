#### 背景
- **背景**
    该文章介绍了文本至图像扩散模型（text-to-image diffusion models）与语言模型（LLMs）在用户偏好学习方面的不同。与LLMs不同，这一领域内的扩散模型很少涉足用户偏好学习，常见的方法是用优质图像和文字说明预训练模型以改进视觉吸引力和文本对齐。

- **已有的工作**
    现有的工作没有充分探索在开放词汇环境中对大规模人类反馈进行泛化的方法，已有的RL方法在词汇扩大时效果下降。

#### 核心贡献
- **提出了一个Diffusion-DPO框架**
    - **挑战1：用人类比较数据优化扩散模型**
        扩散模型通常没有包含学习自人类偏好的阶段，作者提出一个解决方案——Diffusion-DPO，这是从直接优化偏好（Direct Preference Optimization）演化而来的方法，可以用来直接根据人类比较数据优化扩散模型，实现了将对人类偏好的直接优化应用于扩散模型，提供了一种新颖的扩散模型数据似然定义，并导出了一个可微分目标。

    - **挑战2：实现图像升级与文字提示对齐**
        新方法在人类评估者中得到了更高的评价，提升了图像的视觉吸引力和文字提示的对齐性。

#### 实现与部署
使用851K对人群外包偏好配对数据集(Pick-a-Pic dataset)来对最新的Stable Diffusion XL (SDXL)-1.0模型基础版进行微调。利用DPO微调的SDXL图像相比于SDXL基础和额外细节改善版模型在人类评估中表现更优，69%的时间内人类评价者更偏好DPO微调后的图像。本研究所提出的方法还包括一个利用AI反馈进行训练的变体，其性能与基于人类偏好的训练表现相当，能够打开使用AI反馈扩展扩散模型对齐方法的大门。

#### 总结
本文提出了一个名为Diffusion-DPO的方法，其通过直接优化基于人类比较数据的模型来实现对扩散模型与人类偏好的对齐。此外，文章也探索了基于AI反馈的训练，取得了与基于人类偏好训练相媲美的成绩。这明显提升了模型在视觉吸引力和文本对齐方面的性能，为利用AI反馈扩展扩散模型对齐方法提供了新的途径。