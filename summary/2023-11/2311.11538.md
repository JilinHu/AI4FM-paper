#### 背景
- **背景**       
    文章针对自定义的GPT模型可能存在的安全风险进行了探讨。这些风险包括利用输入提示（prompt）操纵这些模型的行为，进而导致未经授权的行动或数据泄露，这种攻击方法被称为提示注入（prompt injection）。

- **已有的工作**
    通过分析已有的工作发现，现有的防护措施尽管具有潜力，但仍然可以被精心设计的敌意提示所绕过。文章解释了为什么简单的提示可以在自定义GPT模型上获得惊人的成功率，凸显了目前自定义的GPT模型中存在的关键漏洞，以及为何需要解决这一提示注入的问题。

#### 核心贡献
- **提出了一个基于三步的方法来进行提示注入攻击**
    - **挑战1：怎样检测和评估自定义GPT模型是否容易受到系统提示提取和文件泄露的攻击。**
        文章通过开发一种方法，成功对200多个自定义GPT进行了测试，发现其中大部分都易受到这些严重风险的影响。

    - **挑战2：如何应对并加强模型对于提示注入攻击的防御。**
        文章还对近期提出的防御机制进行了红队评估，揭示了即使是设计用来预防提示注入的防御措施也可能被复杂的对抗性提示绕过。

#### 实现与部署
文章实施的评估结果表明，使用简单提示对自定义GPT的提示注入攻击获得了97.2%的系统提示提取成功率和100%的文件泄露成功率。这些结果强调了自定义GPT模型的关键漏洞，突出了紧急需要解决提示注入问题。即使存在一些攻击失败的实例，这些实例相对较少。在进行红队评估时，发现在带有代码解释器的GPT模型上，专家们通常需要更少的尝试来成功进行提示注入。而关闭代码解释器则能提高系统针对提示提取的健壮性。在四位专家中，有三位未能在十次尝试内提取系统提示，而成功的专家也需要进行九次尝试。

#### 总结
该论文着重研究了自定义GPT模型中的安全风险，尤其是提示注入攻击。研究者们提出了一个包含扫描、注入敌意提示和提取目标信息三个步骤的攻击方法，并通过实施评估发现自定义GPT模型存在严重的系统提示提取和文件泄露漏洞。这些发现突出了自定义GPT模型中的关键安全缺陷，并指出了提升这些模型安全性结构的必要性。此外，红队评估清楚地显示出，现有防护措施并不足够强大，甚至有时候明确指出不应该分享的信息也能被提取出来，这表明亟需进一步加强对抗提示注入攻击的防御机制。