#### 背景
- **背景**       
    研究人员发现传统的大型自然语言处理(NLP)基准评价并不能完全评估指令型大型预训练语言模型(LLMs)对指令的遵循能力，且自然语言生成(NLG)的实用性相关属性如连贯性、简洁性和响应的相关性通常会被忽略。人工注释虽然是评价这些模型的金标准，但它耗时且昂贵。

- **已有的工作**
    之前的研究主要关注在指令模板上微调(finetuning)模型，即通过提供任务指令模板来改善模型对不同问答格式的处理能力。现有的许多指令断言"数据越多越好"，但是当涉及模型对不同指令样式的适配时，这一断言并不总是成立。同时，较新的研究表明开源的LLMs如LLaMA-7B可以通过使用由先进GPT模型生成的高质量指令数据有效地进行微调。当前的评估机制，包括基于模型的评估，如用GPT-4作为裁判，展现了与人类注释者的评估结果强相关性，尽管在进行基于模型的评估时存在限制。

#### 核心贡献
- **提出了一个新的研究**
    - **挑战1：如何在不增大训练数据集的情况下提高LLMs的执行指令能力**
        本文提出通过仅使用1000个样本进行微调可以达到和使用完整56.2k样本数据集相同的性能。这表明在训练数据集的大小和样式上适当减少也能有效提高LLMs对指令的遵从能力。

    - **挑战2：能否实现一种适用于多种评估模式的微调方法**
        研究发现，将1000个LIMA样本和1000个Instruct样本结合起来微调，能够在传统的NLP基准和基于模型的评估范式上都得到性能提升。这表明结合来自不同来源且风格不同的少量样本可以产生更加鲁棒的LLMs。

#### 实现与部署
研究者在一系列指令类型的数据集上对开源MosaicML MPT-7B和MPT-30B预训练模型进行了微调。评估使用了两种流行的评估范式：传统的NLP基准评估和以GPT-4为裁判的基于模型的评估。研究结果表明，仅在LIMA样本上微调的模型在GPT-4评估中表现优于在Instruct数据集上训练的模型。另外，用1000个样本从56.2k微调训练集中进行微调，可以达到和完整数据集相同的性能。最后，组合1000个LIMA样本和1000个Instruct样本进行微调，可以在传统基准和基于模型的评估范式上都实现性能提升。

#### 总结
本论文的主要贡献包括：在开源模型上微调不同大小和风格的指令数据集，评估微调模型在不同的评估范式下的表现，并且发现较少的样本（特别是当这些样本结合了不同来源和风格时）足以在不同类型的评估中获得良好的性能。这表明在培养LLMs的指令遵从能力时，“少即是多”，且通过精心选择微调样本，可以使模型在执行指令能力上得到显著提升。这一发现对于如何有效地微调LLMs以及如何评估它们的实用性具有重要意义。