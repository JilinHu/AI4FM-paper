#### 背景
- **背景**       
    论文介绍了大型语言模型（LLMs）在领域特定知识（如法律、医疗等垂直领域）方面的不足。现有的方法主要通过连续预训练或检索增强来支持LLMs，但这些方法要么成本过高，要么在实际应用中效果不稳定。

- **已有的工作**
    现有工作尝试使用连续预训练和检索增强来提高特定领域LLMs的性能，但这些策略要么成本昂贵，要么在实用中不够可靠。

#### 核心贡献
- **提出了一个名为BLADE的新框架**
    - **挑战1：领域特定的知识不足**
        BLADE结合了一个黑盒大型语言模型和一个小型领域特定的LM，以增强领域特定知识的应用。小型LM用于保留领域特定知识和提供专业洞见，而大型LLM则贡献强大的语言理解和推理能力。该方法通过三个步骤实现：领域特定数据预训练小型LM、使用知识指导数据对模型进行微调、以及联合贝叶斯优化。

    - **挑战2：低效的域适应方法**
        BLADE通过知识指导调整(KIT)和贝叶斯提示优化(BPO)来提高小型LMs与通用LLMs的交互和协作能力。这些技术显著改进了小型LMs的交互能力，且有助于更好地保障私人数据。BLADE通过域特定预训练和贝叶斯提示优化，为小型模型提供更深入的问题特定知识的产生能力，这是现代密集型检索模型之间问题与文档的相对简单交互（如内积）所不具备的。

#### 实现与部署
BLADE在公开的法律和医疗基准测试上进行了广泛的实验。实验表明，相对于原始语言模型，BLADE连续在各种模型上增强了性能。例如，Baichuan-7B 和 ChatGPT 分别实现了28.4%和31.3%的性能提升，指出BLADE适用于不同大小的多种语言模型，并在中国法律问答任务上取得了最先进的结果。

#### 总结
这项研究提出了一个新架构BLADE，可以通过小型领域特定模型增强黑盒大型语言模型，并解决了大型模型在特定领域应用中的知识不足问题。BLADE证明了其在性能和成本上都是一个有效的解决方案。