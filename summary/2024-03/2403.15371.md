#### 背景
- **背景**       
    论文探讨了大型语言模型（LLMs）在没有训练干预的情况下，作为增强学习和决策过程中探索能力的应用。作者专注于原生表现，通过在多臂老虎机环境中部署LLM，完全使用LLM提示（prompt）来描述环境和交互历史。测试了GPT-3.5、GPT-4和Llama2，发现模型未能在没有重要干预的情况下稳健地进行探索。

- **已有的工作**
    已有的工作表明，当LLMs经过特定训练、例如使用来自增强学习的数据或相关任务上的专家演示时，可以展示包含探索在内的强化学习行为。但是这些训练往往是繁重、昂贵且可能特定于任务的。此外，这些发现并未阐明在标准训练方法下获得的通用目的LLMs是否自然显示出探索行为，引出了一个基本问题：当代LLMs是否能在上下文中表现出探索能力？

#### 核心贡献
- **探讨LLMs作为代理在简单合成强化学习问题中的表现**
    - **挑战1：探索是否自然发生在LLMs中**
        挑战1是确定在不进行训练干预的情况下，LLMs是否具备探索的本能能力。论文通过在多臂老虎机问题中测试LLMs来回答这个问题，多臂老虎机问题能够分离探索与利用之间的权衡。LLMs在没有适当提示设计的情况下通常不会显示出稳健的探索行为，但是当基于GPT-4且提示设计包含探索提示、外部总结的交互历史并采用零次射击思路链推理时，能够显示出满意的探索行为。

    - **挑战2：统计学上的微妙性与计算约束**
        挑战2是在面临财务和计算约束的同时，搜索大量的提示设计空间并获得统计意义上的结果。论文的方法是识别代理统计量作为长期探索失败的诊断标准，结合少量复制和短学习周期来特征化长期探索失败，尽管奖励标准可能太过嘈杂无用。

#### 实现与部署
实验结果表明，多数配置下的LLMs未能表现出稳健的探索行为，在大多数情況下，即使在初始轮后，LLMs未能选择最佳动作（arm）。这一现象反映在多个实验配置中，尤其是在基础提示设计下的GPT-4中，超过60%的实验未能聚焦于更好的选择。唯一成功的配置结合了GPT-4和一个增强提示，这个提示提供了探索的暗示，并外部总结交互历史，要求LLM使用零次射击思路链推理。研究人员认为，尽管当前代LLMs有可能在简单的RL环境中探索——在合适地指导提示下，但在更复杂的设置中或许需要进行训练干预，以赋予LLMs更复杂的探索能力。

#### 总结
这篇论文调查了当代大型语言模型（LLMs）能否在上下文中从事探索的问题，特别是在没有训练干预的情况下。经过一系列实验，作者发现只有在特定的配置下LLMs才能稳健地进行探索。研究表明，没有适当的提示设计，即使是最先进的LLMs也可能无法在更复杂的环境中进行探索，而在这些环境中外部总结历史可能是一个非平凡的算法设计问题。这项工作提示了LLMs可能需要有针对性的算法干预才能在复杂环境中有效地工作。