#### 背景
- **背景**       
    文章讨论了从人类反馈中学习的强化学习（RLHF）在对齐大型语言模型（LLMs）与人类偏好中的有效性。然而，在该过程中存在一个重要问题：由于人类偏好数据集的有限性，建立的奖励模型往往无法准确代表潜在的人类偏好。这个近似误差会导致在RL阶段得到不可靠的奖励，从而出现“奖励过优化”现象，即LLM错误地利用高奖励状态，人为地提高估计的代理奖励，而实际奖励下降。这种错误导向使LLM优先考虑奖励最大化而非内容质量和用户对齐。

- **已有的工作**
    为了应对奖励过优化，当前的缓解策略主要集中在对RL波利斯训练中不确定性较高的样本进行惩罚。这些方法使用多个不同种子训练的奖励模型集合，通过集合中的奖励估计方差来量化不确定性。尽管理论上这些不确定性估计可以识别不可靠的奖励，但是训练和维护策略优化过程中的多个奖励模型在内存中的计算成本使其在实际场景中不切实际。

#### 核心贡献
- **提出了一个Adversarial Policy Optimization (AdvPO)**
    - **挑战1：计算成本高昂**
        文献提供了一种轻量级方法来量化奖励不确定性，该方法仅依赖奖励模型的最后一层嵌入，无需计算昂贵的奖励集合。AdvPO解决了以奖励模型预测的置信区间为中心的分布式鲁棒优化问题，从而对策略进行改进。

    - **挑战2：奖励过优化问题**
        AdvPO通过对奖励模型的预测置信区间附近对抗性地搜索，用于策略优化。与先前解决过优化问题的工作相比，该方法使用不确定性的方式更为宽松。此外，通过实验验证，AdvPO在Anthropic HH 和 TL;DR摘要数据集上有效解决了奖励过优化问题，并通过人类辅助评估证明了所学习LLM的性能。

#### 实现与部署
AdvPO在减轻奖励过优化的同时，避免了与基于集合的方法相比的计算负担。它不仅能够减少过优化，并且还能够提升策略的整体性能，这通过人类辅助的评估得到了验证。实验结果表明AdvPO与现有考虑不确定性的方法以及标准PPO相比，在实际场景中显示出其有效性。

#### 总结
这篇论文介绍了Adversarial Policy Optimization (AdvPO)，它是解决基于人类反馈的强化学习过程中出现的奖励过优化问题的新方法，特别是在与人类偏好对齐的大型语言模型中。AdvPO有效地在没有带来高额计算成本的情况下缓解了奖励过优化。