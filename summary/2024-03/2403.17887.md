#### 背景
- **背景**       
    论文探讨了如今的大型语言模型（LLMs）在被训练完成后的有效性问题，特别是深层网络参数如何在推理阶段中发挥作用。

- **已有的工作**
    当前模型的预训练方法可能没有有效利用深层网络的参数，或者浅层起到了储存知识的关键作用。

#### 核心贡献
- **提出了一个简单剪枝策略**
    - **挑战1：维持模型性能**
        使用层剪枝算法识别可以被移除的最优层块，然后通过少量微调进行“修复”，特别是使用高效微调（PEFT）方法进行质量化和低秩适配（QLoRA）。

    - **挑战2：资源消耗优化**
        研究结果表明，层剪枝方法可以与其他PEFT策略结合，进一步减少微调阶段的计算资源，并改善推理阶段的内存和延迟。

#### 实现与部署
在不同的问答基准测试中，通过移除高达一半的网络层，发现模型性能的下降微乎其微。论文还探讨了如何衡量剪枝前后网络层之间的相似度，并且用于指导剪枝的最佳层块选择。此外，论文对采用这种策略后模型的准确性、下一个词预测的损失以及表示之间的角度距离进行了评估。

#### 总结
本论文针对流行的开权重预训练LLMs提出了一种简单的层剪枝策略，并展示了在删除大量层后LLMs对性能影响较小的实证研究。