#### 背景
- **背景**       
    文章介绍了大型语言模型（LLMs）拥有新的能力和应用领域，但是衡量它们与人类偏好的一致性仍然是一个巨大挑战。

- **已有的工作**
    尽管目前社区已经推出了各种评估LLMs性能的基准测试，但现有的基于静态数据集和绝对真理（ground-truth）的方法无法捕捉模型处理现实世界开放式任务时与人类偏好对齐的细腻和多样性。现有工具的局限性包括测试问题的闭环性和测试集的静态性，这些使得测试结果的可靠性变得难以保证。考虑到这些问题，迫切需要一个基于人类偏好、开放且实时的评估平台，以更准确地反映实际使用场景。

#### 核心贡献
- **提出了一个名为 Chatbot Arena 的开放平台**
    - **挑战1：如何收集真实世界场景的多样化问题**
        本文提出的Chatbot Arena使用基于众包的方式搜集多样化的用户问题，这些问题能代表真实世界情境，并且足够具有区分度，从而区别不同的模型。
        
    - **挑战2：如何有效评估和排名大量模型**
        Chatbot Arena利用了一系列统计技术，如Bradley & Terry统计模型和Vovk & Wang的E-values，通过精心设计的效率抽样算法，可靠且节省样本地对模型进行评估和排名。

#### 实现与部署
自2023年4月以来，该平台已经收集了超过240,000张来自约90,000名用户的投票，支持超过100种不同的语言。为了吸引用户参与，论文的作者们提供了50多个最先进的模型供用户免费使用，并且与行业领先的模型开发者如OpenAI、Google等合作，把他们最新的模型整合到平台中。通过不断更新排行榜、发布分析博客、释放数据集以及分享信息，以保持社区的参与度。这些努力使得Chatbot Arena成为LLM领域中最多被引用的领先者。

#### 总结
Chatbot Arena是一个基于用户偏好，用于评估大型语言模型的开放平台。它通过众包方式收集用户问题并进行匿名化的随机化对决，用于评估LLMs的表现，解决了现有静态数据集基准测试的局限性，并通过精心设计的统计方法确保了评估结果的可信度和效率。