#### 背景
- **背景**       
    论文指出，大型语言模型（LLMs）在接受巨量文本语料库训练后，会显示出令人担忧的社会偏见水平。这些未经检测的偏见可能会加剧社会不平等，并导致不公平甚至不道德的结果。随着LLMs能力越来越强，并开始在各个领域（如医疗和教育）作为决策系统的基础组件，解决这些偏见问题变得尤其重要。

- **已有的工作**
    当前已有的去偏见方法包括：模型参数的直接微调、修改解码步骤和基于提示的技术。但由于安全和商业利益等原因，许多高能力的LLMs（例如GPT-4、Gemini、Claude 2）是闭源的，普通大众无法访问模型内部结构或参数。因此，基于提示的技术成为在使用闭源LLMs时缓解偏见的唯一可行选择。

#### 核心贡献
- **提出了一个因果关系引导的去偏见框架**
    - **挑战1：如何在无法访问内部结构和参数的情况下去偏见LLMs**
        因果关系引导的去偏见框架通过了解数据生成过程和LLMs内部推理过程的因果机制，指导设计提示语而不是修改内部机制或参数。该框架统一了现有的去偏见提示方法，并提出了鼓励无偏见推理的新方法，即使只有黑箱访问权限也能有效去偏见。

    - **挑战2：优化去偏见提示的设计**
        通过构建详细的因果模型来理解培训语料的数据生成过程和LLMs的推理过程。发现选择机制在确定LLMs输出如何被不同提示调节方面起着关键作用。此框架提出的提示设计策略在实证研究中表现出色，明显优于现有的基于提示的去偏见方法。

#### 实现与部署
通过应用实证研究分析，论文发现结合两种直觉（即抑制偏见推理和鼓励无偏见推理）的提示显著优于现有的基于提示的去偏见方法。这些强有力的实证结果清楚地表明了即使在只有黑箱访问的情况下，因果关系引导的去偏见框架的有效性。

#### 总结
该论文成功提出了一个新的因果关系引导的去偏见框架，并通过实证研究验证了其有效性，既可以整合现有的基于提示的去偏见方法，也为诱导无偏见推理提出了新的途径。