#### 背景
- **背景**       
    文章介绍了LLMs在解决问题方面展现了显著的能力，但在解决数学问题方面依然存在不足，这可能是由于解决数学问题所需的多步骤复杂推理能力。

- **已有的工作**
    预先训练采用指令调优的方法已经证明能在一定程度上提升LLMs能力，但这种方法受限于现有数学推理数据集的规模有限。目前最受欢迎的数学数据集各只包含大约7.5千个训练样本，这制约了进一步提升模型性能的空间。

#### 核心贡献
- **提出了一个名为MathScale的方法**
    - **挑战1：数据集规模和质量不足**
        MathScale通过使用前沿LLMs（如GPT-3.5）生成高质量的数学推理数据来解决这一挑战。MathScale首先提取种子数学题目中的主题和知识点以及建立概念图，然后用它来生成新的数学题目。

    - **挑战2：缺少全面的评估基准**
        所提出的MWPBENCH是一个数学单词问题的评估基准，包含10个数据集，覆盖了从K-12到大学以及竞赛水平的各种数学问题。此外，MWPBENCH提出了一个统一的评价协议，以保障模型比较的一致性和公平性。

#### 实现与部署
MathScale有效地扩大了其生成的数学数据集的规模，并创建了一个包含200万个数学问题-答案对的数据集（MathScaleQA）。通过将MathScaleQA应用于微调开源大型语言模型（例如LLaMA-2和Mistral），显著提高了它们在数学推理方面的能力。在MWPBENCH上的评估中，MathScale-7B模型在所有数据集上均达到了最佳性能，相较于相同规模的最佳对等模型，在微观平均准确率上提高了42.9%，宏观平均准确率上提高了43.7%。

#### 总结
MathScale提出了一个可扩展的方法来创建高质量的数学推理数据，通过构建新的评估基准MWPBENCH全面地评价LLMs在数学推理上的能力，显著提升了模型解决数学问题的性能。