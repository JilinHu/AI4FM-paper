#### 背景
- **背景**       
    大型语言模型（LLMs）往往因为无法区分超出其知识范围的问题而生成错误的输出，这种现象称为幻觉（hallucination）。过去的研究主要集中在提高正确性上，但忽视了拒绝机制的重要性。

- **已有的工作**
    目前，很多研究通过增强LLMs的知识来避免超出知识范围的问题，比如策划训练数据或在推理过程中使用检索增强的生成（RAG）。然而，模型的知识本质上是有限的，即使是最强大的模型比如GPT-4也会遇到幻觉现象。这表明幻觉问题的根本在于模型与其知识范围之间的错位。

#### 核心贡献
- **提出了一个xxx**
    - **挑战1：评估LLMs的诚实度复杂**
        辨别模型“知道什么”和“不知道什么”的能力被视为一个分类问题，但在不同模型或不同提示下准确率会有很大差异，传统的分类指标由于缺乏真实标签和动态标签分布而不合适。

    - **挑战2：现有对齐框架难以有效提升模型的诚实度**
        现有的对齐框架如RLHF构建的奖励模型主要针对模型不具体的偏好数据训练。而因为不同模型具有不同程度的知识，利用通用的对齐数据提高不同模型的诚实度变得挑战重重。

#### 实现与部署
为了解决这些问题，研究者们提出了Reinforcement Learning from Knowledge Feedback (RLKF)框架。该框架利用知识反馈来动态确定模型的知识边界，并训练一个可靠的奖励模型来鼓励拒绝超出知识范围的问题。实验结果显示RLKF在数学问题上能够显著提升LLMs的可靠性。对于内部领域的算术数据集和外部领域的GSM8K数据集的实验展示了该框架的有效性，并显著提高了基线模型的可靠性。

#### 总结
这项工作通过提出RLKF框架并定义了新的模型可靠性评估指标，有效地解决了LLMs的幻觉问题，并提升了LLMs的诚实度和可靠性，显示出打造更值得信赖的AI系统的潜力。