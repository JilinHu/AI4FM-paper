#### 背景
- **背景**       
    论文分析了当前大型语言模型（LLM）在训练、微调和服务方面的挑战。尤其在数据质量和多样性、以及模型架构和优化方面。

- **已有的工作**
    虽然现有的工作在提高大型语言模型性能和效率方面已经取得了一定进展，但通常要么是在数据处理方面不够细致，要么是在模型架构的修改和优化上不够创新，这限制了模型表现的进一步提升。

#### 核心贡献
- **提出了一个名为Yi-34B的模型**
    - **挑战1：数据质量和多样性**
        论文提出了一个多级数据清洗策略，目标是提高数据集的质量和多样性。通过采用规则过滤、学习过滤和基于簇的过滤等技术，Yi-34B能够从常见的网络爬虫数据中抽取高质量和多样性的数据。

    - **挑战2：模型架构和优化**
        Yi-34B模型采用了改进的解码器只有Transformer架构，并引入了分组查询注意力机制（Grouped-Query Attention, GQA）和SwiGLU激活函数来减少训练和推断成本，而没有观察到性能下降。

#### 实现与部署
Yi-34B在多个标准评测基准上达到了与GPT-3.5相当的性能。通过模型参数和缓存量化，实现了控制推理成本，便于社区在成本效益的设备上部署此模型。详细的性能对比表明，在诸如常识推理、大学考试、数学、编码、阅读理解等评估基准上，Yi-34B与主要LLMs表现相当。模型对社区的贡献包括提供与GPT-3.5质量匹配且成本有效的模型，支持开发者构建AI原生应用，以及为用户提供可本地运行的聊天机器人。

#### 总结
该论文成功地提出了一个在性能和效率上都可与GPT-3.5相媲美的Yi-34B模型，并详细描述了在大型语言模型预训练及其指令微调方面的创新方法。