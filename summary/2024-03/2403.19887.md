#### 背景
- **背景**       
    论文介绍了一种名为Jamba的新型大型语言模型，该模型基于一种新颖的混合Transformer-Mamba体系结构。Jamba将Transformer层和Mamba层以及专家混合（MoE）组件相结合，从而提供了改进的性能和更高的吞吐量，同时保持了可管理的内存足迹。所提出的7B基础的Jamba模型被设计成可以适配单个80GB GPU，且具备灵活的体系结构允许资源和目标特定的配置。

- **已有的工作**
    尽管Transformer作为大型语言模型的主导架构非常受欢迎，但存在两个主要问题：其对内存和计算资源的高需求限制了处理长上下文的能力；其缺乏单一摘要状态导致推理速度慢且吞吐量低。此外，尽管最近的状态空间模型（SSM）如Mamba在处理长距离关系上比递归神经网络（RNN）更有效，但仍然在性能上落后于同等规模的Transformer语言模型。

#### 核心贡献
- **提出了一个新型混合Transformer-Mamba体系结构的语言模型**
    - **挑战1：如何处理长上下文**
        Jamba采用混合Transformer层和Mamba层的方式，以及专家混合（MoE）组件，达到了处理长上下文的能力。其结构允许根据具体的硬件和性能需求调整Transformer与Mamba层的比例，以此来平衡内存使用、高效训练和长上下文处理能力的关系。

    - **挑战2：如何提升模型吞吐量同时保持较小的内存足迹**
        Jamba通过在某些MLP层应用MoE，增加了模型的容量而不增加计算需求（即活跃参数的数量保持可管理）。这种方法使Jamba能够训练极大规模的模型，并且表现出色，对于长上下文的评估有着更优的性能。

#### 实现与部署
Jamba模型在多个标准的语言模型基准测试及长上下文评估中表现出与Mixtral-8x7B相当的性能，该模型拥有类似数量的参数，并且也优于更大的Llama-2 70B模型。特别地，Jamba支持长达256K token的上下文——这是公开可用的生产级模型中支持的最长上下文长度。在长上下文评估中，Jamba在多数数据集上都比Mixtral表现更好，同时Jamba也极其高效，例如对于长上下文，它的吞吐量是Mixtral-8x7B的3倍。此外，即便是在超过128K token的上下文中，Jamba也能适配单个GPU（使用8位权重），这在类似规模的仅限于注意力的模型（如Mixtral-8x7B）中是无法实现的。

#### 总结
Jamba是基于混合Transformer-Mamba体系结构的新型大型语言模型，突破了处理长上下文的限制，并且通过应用专家混合（MoE）组件提高了模型吞吐量，同时保持了较小的内存足迹。此模型标志着在大型语言模型领域的一个新方向，并展示了高效训练与强大性能之间的可能平衡。